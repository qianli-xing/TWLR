{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IJCAI2021_autoencoder+distance().ipynb","provenance":[{"file_id":"143CXIXh7PVh4uev3KYYiDAgF_oHzuwmP","timestamp":1609389062429},{"file_id":"1htjO1FWxLEKDWqLxkLYkd4gdHGMdMqF2","timestamp":1603669767934}],"collapsed_sections":["Cn9AOyvF61ne","966ir-RMIiMT","JzktnlMo46-Y","GIxFW3Bze3D1","zLehmDu87Plw","qOEG-PVA6Fe7","Wexe7oP4h7Vk","bRyemKAQm0Yn","eVzROeKR1tmQ"],"toc_visible":true,"mount_file_id":"1iQaBTsfBYz293M0SRh1SNfuhlvtaUog9","authorship_tag":"ABX9TyNjdxzHwAxV4f+5v5rjDFGn"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4276uKYjt0qi"},"source":["### utilities"]},{"cell_type":"code","metadata":{"id":"1dCyoVXJsf_3","executionInfo":{"status":"ok","timestamp":1610679582164,"user_tz":-660,"elapsed":4652,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["import random\n","import math\n","import csv\n","import random\n","import numpy as np\n","import sys\n","import pandas as pd\n","import torch\n","from itertools import combinations\n","\n","import pickle\n","\n","from collections import defaultdict, Counter\n","from sklearn.cluster import OPTICS, cluster_optics_dbscan\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import copy\n","\n","# 20200518 \n","# accuracy 和 label aggregation 分开\n","# 计算一个 cluster 里面 worker 的平均的准确率;\n","\n","def accuracy(workerlist,threshold,t2truth):\n","  # worker回答的 task 先找到, evaluate其他的worker和special worker的方式不一样\n","  # 非special worker 需要他们的准确率 只是在实验时候展示,对于special worker 选择时候是要选择5个task 进行evaluate;\n","  # 他不是 spammer 那么, 如果回答的task 不够覆盖全部task怎么办?  \n","  tasklist = []\n","  worker_acc_dict = {}\n","  for w in workerlist:\n","    #w2tl的结果是 tuple list\n","    total_tuple_list = w2tl[w]\n","    # 用threshold 来确定 evaluate 一个worker 用多少task\n","\n","    if len(total_tuple_list) > threshold:\n","      short_task_list = random.sample(total_tuple_list, 5)\n","    # acc 先记录个数, 再计算准确率\n","    acc =0\n","    for t_id,w_answer in total_tuple_list:\n","      # 还需要一个truth[t_id], 可能会有不存在的情况, 如何处理\n","      truth = t2truth[t_id]\n","      if w_answer == truth:\n","        acc = acc+1\n","      \n","    acc = acc/len(total_tuple_list)\n","    worker_acc_dict[w] = acc\n","  # with open(path+str(threshold)+'_'+'worker_acc_dict.pickle','wb') as file:\n","  #   pickle.dump(worker_acc_dict,file)\n","  return worker_acc_dict\n","\n","def accuracy_all(workerlist,t2truth):\n","  # worker回答的 task 先找到, evaluate其他的worker和special worker的方式不一样\n","  # 非special worker 需要他们的准确率 只是在实验时候展示,对于special worker 选择时候是要选择5个task 进行evaluate;\n","  # 他不是 spammer 那么, 如果回答的task 不够覆盖全部task怎么办?  \n","  # tasklist = []\n","  worker_acc_dict = {}\n","  for w in workerlist:\n","    #w2tl的结果是 tuple list\n","    total_tuple_list = w2tl[w]\n","    # 用threshold 来确定 evaluate 一个worker 用多少task\n","    #多余5个元素选择5个选择5个\n","    # if len(total_tuple_list) > threshold:\n","    #   short_task_list = random.sample(total_list, 5)\n","    # acc 先记录个数, 再计算准确率\n","    acc =0\n","    for t_id,w_answer in total_tuple_list:\n","      # 还需要一个truth[t_id], 可能会有不存在的情况, 如何处理\n","      truth = t2truth[t_id]\n","      if w_answer == truth:\n","        acc = acc+1\n","      \n","    acc = acc/len(total_tuple_list)\n","    worker_acc_dict[w] = acc\n","  # with open(path+'worker_acc_all_dict.pickle','wb') as file:\n","  #   pickle.dump(worker_acc_dict,file)\n","  return worker_acc_dict\n","def worker_cluster_acc(ep_label_woker_dict,worker_acc_dict):\n","  cluster_acc_dict={}\n","  for ep in ep_label_woker_dict:\n","    label_wid_dict = ep_label_woker_dict[ep]\n","  #special worker 的分类是-1\n","  for label in label_list:\n","    worker_list = label_wid_dict[label]\n","    #测试一下每一类worker的准确率 \n","    #acc 累加,结果是一个cluster中所有worker的 acc 平均值\n","    acc = 0\n","    for wid in worker_list:\n","      #worker_acc 是一个dict\n","      acc_w = worker_acc_dict[wid]\n","      acc = acc + acc_w\n","    acc = acc/len(worker_list)\n","    cluster_acc_dict[label] = acc\n","  \n","  return cluster_acc_dict\n","\n","# 做一个函数 numpy，每一行的id就是worker的num_id ( 是w2input要numI_id 和str_id 对应上)\n","# 就是一个ppmi\n","def input2numpy(input_dict,wstrid2wnumid,wnumid2wstrid,feat_size,path,name):\n","  x_train = np.zeros((feat_size,),dtype = np.float)\n","  for i in range(len(input_dict)):\n","    #确认number 对应的string_id\n","    str_id  = wnumid2wstrid[i]\n","    #对应worker的 input array\n","    worker_input_array = np.asarray(input_dict[str_id])\n","    x_train = np.vstack((x_train,worker_input_array))\n","\n","  x_train_all = np.delete(x_train,0,axis=0)\n","  np.save(path+name,x_train_all)\n","  return x_train_all\n","\n","def extend_nodes(nodes_batch, training_cps, wstrid2wnumid):\n","  #深复制之后就是两个对象，需要import copy\n","  original_nodes_batch = copy.deepcopy(nodes_batch)\n","  node_pairs = {}\n","  extended_nodes_batch = set(nodes_batch)\n","  #wnumid2wstrid = wnumid2wstrid\n","  for node in nodes_batch:\n","      strid = wnumid2wstrid[node]\n","      cps = training_cps[strid]\n","      node_pairs[node] = cps\n","      for cp in cps:\n","        extended_nodes_batch.add(cp[1])\n","  extended_nodes_batch = list(extended_nodes_batch)\n","  return extended_nodes_batch\n","\n","def w2wsim2numpy(input_dict,wstrid2wnumid,wnumid2wstrid,feat_size,path,name):\n","  x_train = np.zeros((feat_size,),dtype = np.float)\n","  for i in range(len(input_dict)):\n","    #确认number 对应的string_id\n","    str_id  = wnumid2wstrid[i]\n","    #对应worker的 input array\n","    worker_input_array = np.asarray(input_dict[str_id])\n","\n","    #设定一个threshold, 小于threshold(0.3), 设置成零, 大于threshold 设置成1\n","    worker_input_array[worker_input_array<0.3] = 0\n","    worker_input_array[worker_input_array>=0.3] = 1\n","\n","    x_train = np.vstack((x_train,worker_input_array))\n","\n","  x_train_all = np.delete(x_train,0,axis=0)\n","  np.save(path+name,x_train_all)\n","  return x_train_all"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"NIVq2ut5tc_3","executionInfo":{"status":"ok","timestamp":1610679582168,"user_tz":-660,"elapsed":4651,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":[""],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ngYMZusux635"},"source":["### DeepFD"]},{"cell_type":"code","metadata":{"id":"nsAWTBrlyGJ9","executionInfo":{"status":"ok","timestamp":1610679582168,"user_tz":-660,"elapsed":4648,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["class DeepFD(nn.Module):\n","    def __init__(self, w_pb_input, feat_size, emb_size):\n","    #def __init__(self, features, feat_size, hidden_size, emb_size):\n","        super(DeepFD, self).__init__()\n","        self.w_pb_input = w_pb_input\n","        # feat_size 就是输入的size, hidden_size就是第一次变换的size,emb_size就是encoder输出的size;\n","        self.fc1 = nn.Linear(feat_size, 64)\n","        self.fc2 = nn.Linear(64, emb_size)\n","        self.fc3 = nn.Linear(emb_size, 64)\n","        self.fc4 = nn.Linear(64, feat_size)\n","        \n","    def init_params(self):\n","        for param in self.parameters():\n","            if len(param.size()) == 2:\n","                nn.init.xavier_uniform_(param)\n","            else:\n","                # initialize all bias as zeros\n","                nn.init.constant_(param, 0.0)\n","    # forward 的参数, nodes_batch 就是 forward 函数的 input; \n","    # nodes_batch 就是一组数据,nodes_batch 是编号, features.nodes_batch 才是数据\n","    # def forward(self, nodes_batch):\n","    #     feats = self.w_pb_input[nodes_batch]\n","    #     x_en = F.relu_(self.fc1(feats))\n","    #     embs = F.relu_(self.fc2(x_en))\n","    #     x_de = F.relu_(self.fc3(embs))\n","    #     recon = F.relu_(self.fc4(x_de))\n","    #     #20200716 换成sigmoid 激活函数\n","    #     # recon = F.sigmoid(self.fc4(x_de))\n","    #     return embs, recon\n","    def forward(self, nodes_batch):\n","        #20201231 加上噪声\n","        feats = self.w_pb_input[nodes_batch]\n","        \n","        #均值是0, 方差是 1 的 高斯噪声\n","        #feats = feats + torch.rand(feats.shape)\n","        \n","        x_en = F.relu_(self.fc1(feats))\n","        embs = F.relu_(self.fc2(x_en))\n","        x_de = F.relu_(self.fc3(embs))\n","        recon = F.relu_(self.fc4(x_de))\n","        #20200716 换成sigmoid 激活函数\n","        # recon = F.sigmoid(self.fc4(x_de))\n","        return embs, recon"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"GhdGMlSVTKQq","executionInfo":{"status":"ok","timestamp":1610679582169,"user_tz":-660,"elapsed":4646,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["# 20200824 加了一个layersize\n","class DeepFD(nn.Module):\n","    def __init__(self, w_pb_input, feat_size, layersize, emb_size):\n","    #def __init__(self, features, feat_size, hidden_size, emb_size):\n","        super(DeepFD, self).__init__()\n","        self.w_pb_input = w_pb_input\n","        # feat_size 就是输入的size, hidden_size就是第一次变换的size,emb_size就是encoder输出的size;\n","        self.fc1 = nn.Linear(feat_size, layersize)\n","        self.fc2 = nn.Linear(layersize, emb_size)\n","        self.fc3 = nn.Linear(emb_size, layersize)\n","        self.fc4 = nn.Linear(layersize, feat_size)\n","        \n","    def init_params(self):\n","        for param in self.parameters():\n","            if len(param.size()) == 2:\n","                nn.init.xavier_uniform_(param)\n","            else:\n","                # initialize all bias as zeros\n","                nn.init.constant_(param, 0.0)\n","    # forward 的参数, nodes_batch 就是 forward 函数的 input; \n","    # nodes_batch 就是一组数据,nodes_batch 是编号, features.nodes_batch 才是数据\n","    def forward(self, nodes_batch):\n","        feats = self.w_pb_input[nodes_batch]\n","        x_en = F.relu_(self.fc1(feats))\n","        embs = F.relu_(self.fc2(x_en))\n","        x_de = F.relu_(self.fc3(embs))\n","        recon = F.relu_(self.fc4(x_de))\n","        #20200716 换成sigmoid 激活函数\n","        # recon = F.sigmoid(self.fc4(x_de))\n","        return embs, recon"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dy1oNi4kxbEv"},"source":["###DeepFD loss"]},{"cell_type":"code","metadata":{"id":"PRNhOYa7xRj_","executionInfo":{"status":"ok","timestamp":1610679582169,"user_tz":-660,"elapsed":4644,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["# class Loss_DeepFD():\n","#     def __init__(self, features, worker_similarity_df, wnumid2wstrid, training_cps, device, alpha, beta, gamma):\n","#         self.worker_similarity_df = worker_similarity_df\n","#         self.wnumid2wstrid = wnumid2wstrid\n","        \n","\n","#         self.training_cps = training_cps\n","\n","#         self.features = features\n","        \n","#         self.device = device\n","#         #三部分的权重\n","#         self.alpha = alpha\n","#         self.beta = beta\n","#         self.gamma = gamma\n","#         #下面和simi 相关的重点关注\n","#         #这个graph_simi是要改一下的\n","#         #self.graph_simi = graph_simi\n","        \n","        \n","#         self.node_pairs = {}\n","#         self.original_nodes_batch = None\n","#         self.extended_nodes_batch = None\n","#     # def extend_nodes(self, nodes_batch, training_cps):\n","#     #     #深复制之后就是两个对象\n","#     #     self.original_nodes_batch = copy.deepcopy(nodes_batch)\n","#     #     self.node_pairs = {}\n","#     #     self.extended_nodes_batch = set(nodes_batch)\n","\n","#     #     for node in nodes_batch:\n","#     #         cps = training_cps[node]\n","#     #         self.node_pairs[node] = cps\n","#     #         for cp in cps:\n","#     #             self.extended_nodes_batch.add(cp[1])\n","#     #     self.extended_nodes_batch = list(self.extended_nodes_batch)\n","    \n","#     # def extend_nodes(self, nodes_batch, training_cps, wnumid2wstrid):\n","#     #     #深复制之后就是两个对象，需要import copy\n","#     #     self.original_nodes_batch = copy.deepcopy(nodes_batch)\n","#     #     self.node_pairs = {}\n","#     #     self.extended_nodes_batch = set(nodes_batch)\n","#     #     self.wnumid2wstrid = wnumid2wstrid\n","#     #     for node in nodes_batch:\n","#     #         strid = self.wnumid2wstrid[node]\n","#     #         cps = training_cps[strid]\n","#     #         self.node_pairs[node] = cps\n","#     #         for cp in cps:\n","#     #           extended_nodes_batch.add(cp[1])\n","#     #     self.extended_nodes_batch = list(extended_nodes_batch)\n","#     #     return self.extended_nodes_batch\n","\n","#     def extend_nodes(self, nodes_batch):\n","#         #深复制之后就是两个对象，需要import copy\n","#         self.original_nodes_batch = copy.deepcopy(nodes_batch)\n","#         self.node_pairs = {}\n","#         self.extended_nodes_batch = set(nodes_batch)\n","#         #self.wnumid2wstrid = wnumid2wstrid\n","#         for node in nodes_batch:\n","#             strid = self.wnumid2wstrid[node]\n","#             cps = self.training_cps[strid]\n","#             self.node_pairs[node] = cps\n","#             for cp in cps:\n","#               self.extended_nodes_batch.add(cp[1])\n","#         self.extended_nodes_batch = list(self.extended_nodes_batch)\n","#         return self.extended_nodes_batch\n","    \n","#     def get_loss(self, nodes_batch, embs_batch, recon_batch):\n","#         # calculate loss_simi and loss+recon,\n","#         # loss_reg is included in SGD optimizer as weight_decay\n","#         loss_recon = self.get_loss_recon(nodes_batch, recon_batch)\n","#         loss_simi = self.get_loss_simi(embs_batch)\n","#         loss = loss_recon + self.alpha * loss_simi\n","#         return loss\n","    \n","#     #def get_loss_simi(self, embs_batch, nodes_batch):\n","#     def get_loss_simi(self, embs_batch):\n","#         #node2index n 是extended_nodes 就是worker num_id; i 是 整个batch 数量的索引\n","#         node2index = {n:i for i,n in enumerate(self.extended_nodes_batch)}\n","        \n","#         simi_feat = []\n","#         simi_embs = []\n","#         #用nodes_batch 把nodes_pair找出来. node_pairs在extended_nodes中赋值过了\n","#         #self.node_pairs[node]\n","        \n","        \n","#         for node, cps in self.node_pairs.items():\n","#             for i, j in cps:\n","#               #graph_simi 这里需要dataframe了\n","#               wistrid=wnumid2wstrid[i]\n","#               wjstrid=wnumid2wstrid[j]\n","#               simi = self.worker_similarity_df.loc[wistrid,wjstrid]\n","\n","#               #simi_feat.append(torch.FloatTensor([self.graph_simi[i, j]]))\n","#               simi_feat.append(torch.FloatTensor([simi]))\n","#               dis_ij = (embs_batch[node2index[i]] - embs_batch[node2index[j]]) ** 2\n","#               dis_ij = torch.exp(-dis_ij.sum())\n","#               #torch.view 好比numpy.shape/numpy.reshape\n","#               simi_embs.append(dis_ij.view(1))\n","#         simi_feat = torch.cat(simi_feat, 0).to(self.device)\n","#         simi_embs = torch.cat(simi_embs, 0)\n","#         L = simi_feat * ((simi_embs - simi_feat) ** 2)\n","#         return L.mean()\n","\n","#     def get_loss_recon(self, nodes_batch, recon_batch):\n","#         feats_batch = self.features[nodes_batch]\n","#         # H_batch 应该就是哈德马积\n","#         H_batch = (feats_batch * (self.beta - 1)) + 1\n","#         assert feats_batch.size() == recon_batch.size() == H_batch.size()\n","#         #**可以用来表示幂次运算 2 ** 7  # 2 to the power of 7\n","#         L = ((recon_batch - feats_batch) * H_batch) ** 2\n","#         return L.mean()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"mzRWL7J80V2l","executionInfo":{"status":"ok","timestamp":1610679582169,"user_tz":-660,"elapsed":4642,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["#20200618\n","class Loss_DeepFD():\n","    def __init__(self, features, worker_similarity_df, wnumid2wstrid, training_cps, device, alpha, beta, gamma):\n","        self.worker_similarity_df = worker_similarity_df\n","        self.wnumid2wstrid = wnumid2wstrid\n","        \n","\n","        self.training_cps = training_cps\n","\n","        self.features = features\n","        \n","        self.device = device\n","        #三部分的权重\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.gamma = gamma\n","        #下面和simi 相关的重点关注\n","        #这个graph_simi是要改一下的\n","        #self.graph_simi = graph_simi\n","        \n","        \n","        self.node_pairs = {}\n","        self.original_nodes_batch = None\n","        self.extended_nodes_batch = None\n","    # def extend_nodes(self, nodes_batch, training_cps):\n","    #     #深复制之后就是两个对象\n","    #     self.original_nodes_batch = copy.deepcopy(nodes_batch)\n","    #     self.node_pairs = {}\n","    #     self.extended_nodes_batch = set(nodes_batch)\n","\n","    #     for node in nodes_batch:\n","    #         cps = training_cps[node]\n","    #         self.node_pairs[node] = cps\n","    #         for cp in cps:\n","    #             self.extended_nodes_batch.add(cp[1])\n","    #     self.extended_nodes_batch = list(self.extended_nodes_batch)\n","    \n","    # def extend_nodes(self, nodes_batch, training_cps, wnumid2wstrid):\n","    #     #深复制之后就是两个对象，需要import copy\n","    #     self.original_nodes_batch = copy.deepcopy(nodes_batch)\n","    #     self.node_pairs = {}\n","    #     self.extended_nodes_batch = set(nodes_batch)\n","    #     self.wnumid2wstrid = wnumid2wstrid\n","    #     for node in nodes_batch:\n","    #         strid = self.wnumid2wstrid[node]\n","    #         cps = training_cps[strid]\n","    #         self.node_pairs[node] = cps\n","    #         for cp in cps:\n","    #           extended_nodes_batch.add(cp[1])\n","    #     self.extended_nodes_batch = list(extended_nodes_batch)\n","    #     return self.extended_nodes_batch\n","\n","    def extend_nodes(self, nodes_batch):\n","        #深复制之后就是两个对象，需要import copy\n","        self.original_nodes_batch = copy.deepcopy(nodes_batch)\n","        self.node_pairs = {}\n","        self.extended_nodes_batch = set(nodes_batch)\n","        #self.wnumid2wstrid = wnumid2wstrid\n","        for node in nodes_batch:\n","            strid = self.wnumid2wstrid[node]\n","            cps = self.training_cps[strid]\n","            self.node_pairs[node] = cps\n","            for cp in cps:\n","              self.extended_nodes_batch.add(cp[1])\n","        self.extended_nodes_batch = list(self.extended_nodes_batch)\n","        return self.extended_nodes_batch\n","    \n","    def get_loss(self, nodes_batch, embs_batch, recon_batch):\n","        # calculate loss_simi and loss+recon,\n","        # loss_reg is included in SGD optimizer as weight_decay\n","        loss_recon = self.get_loss_recon(nodes_batch, recon_batch)\n","        loss_simi = self.get_loss_simi(embs_batch)\n","        loss = loss_recon + self.alpha * loss_simi\n","        return loss\n","    \n","    #def get_loss_simi(self, embs_batch, nodes_batch):\n","    def get_loss_simi(self, embs_batch):\n","        #node2index n 是extended_nodes 就是worker num_id; i 是 整个batch 数量的索引\n","        node2index = {n:i for i,n in enumerate(self.extended_nodes_batch)}\n","        \n","        simi_feat = []\n","        simi_embs = []\n","        #用nodes_batch 把nodes_pair找出来. node_pairs在extended_nodes中赋值过了\n","        #self.node_pairs[node]\n","        \n","        \n","        for node, cps in self.node_pairs.items():\n","            for i, j in cps:\n","              #graph_simi 这里需要dataframe了\n","              wistrid=wnumid2wstrid[i]\n","              wjstrid=wnumid2wstrid[j]\n","              #20200709 针对worker id 是 int的改造\n","              simi = self.worker_similarity_df.loc[wistrid,wjstrid]\n","\n","              #simi_feat.append(torch.FloatTensor([self.graph_simi[i, j]]))\n","              simi_feat.append(torch.FloatTensor([simi]))\n","              dis_ij = (embs_batch[node2index[i]] - embs_batch[node2index[j]]) ** 2\n","              dis_ij = torch.exp(-dis_ij.sum())\n","              #torch.view 好比numpy.shape/numpy.reshape\n","              simi_embs.append(dis_ij.view(1))\n","        simi_feat = torch.cat(simi_feat, 0).to(self.device)\n","        simi_embs = torch.cat(simi_embs, 0)\n","        L = simi_feat * ((simi_embs - simi_feat) ** 2)\n","        return L.mean()\n","\n","    def get_loss_recon(self, nodes_batch, recon_batch):\n","        feats_batch = self.features[nodes_batch]\n","        # H_batch 应该就是哈德马积\n","        H_batch = (feats_batch * (self.beta - 1)) + 1\n","        assert feats_batch.size() == recon_batch.size() == H_batch.size()\n","        #**可以用来表示幂次运算 2 ** 7  # 2 to the power of 7\n","        L = ((recon_batch - feats_batch) * H_batch) ** 2\n","        return L.mean()"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"7u5QwekI8RhA","executionInfo":{"status":"ok","timestamp":1610679582170,"user_tz":-660,"elapsed":4638,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["#20200709 \n","# 针对wid是整数的操作;\n","class Loss_DeepFD():\n","    def __init__(self, features, worker_similarity_df, wnumid2wstrid, training_cps, device, alpha, beta, gamma):\n","        self.worker_similarity_df = worker_similarity_df\n","        self.wnumid2wstrid = wnumid2wstrid\n","        \n","\n","        self.training_cps = training_cps\n","\n","        self.features = features\n","        \n","        self.device = device\n","        #三部分的权重\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.gamma = gamma\n","        #下面和simi 相关的重点关注\n","        #这个graph_simi是要改一下的\n","        #self.graph_simi = graph_simi\n","        \n","        \n","        self.node_pairs = {}\n","        self.original_nodes_batch = None\n","        self.extended_nodes_batch = None\n","    # def extend_nodes(self, nodes_batch, training_cps):\n","    #     #深复制之后就是两个对象\n","    #     self.original_nodes_batch = copy.deepcopy(nodes_batch)\n","    #     self.node_pairs = {}\n","    #     self.extended_nodes_batch = set(nodes_batch)\n","\n","    #     for node in nodes_batch:\n","    #         cps = training_cps[node]\n","    #         self.node_pairs[node] = cps\n","    #         for cp in cps:\n","    #             self.extended_nodes_batch.add(cp[1])\n","    #     self.extended_nodes_batch = list(self.extended_nodes_batch)\n","    \n","    # def extend_nodes(self, nodes_batch, training_cps, wnumid2wstrid):\n","    #     #深复制之后就是两个对象，需要import copy\n","    #     self.original_nodes_batch = copy.deepcopy(nodes_batch)\n","    #     self.node_pairs = {}\n","    #     self.extended_nodes_batch = set(nodes_batch)\n","    #     self.wnumid2wstrid = wnumid2wstrid\n","    #     for node in nodes_batch:\n","    #         strid = self.wnumid2wstrid[node]\n","    #         cps = training_cps[strid]\n","    #         self.node_pairs[node] = cps\n","    #         for cp in cps:\n","    #           extended_nodes_batch.add(cp[1])\n","    #     self.extended_nodes_batch = list(extended_nodes_batch)\n","    #     return self.extended_nodes_batch\n","\n","    def extend_nodes(self, nodes_batch):\n","        #深复制之后就是两个对象，需要import copy\n","        self.original_nodes_batch = copy.deepcopy(nodes_batch)\n","        self.node_pairs = {}\n","        self.extended_nodes_batch = set(nodes_batch)\n","        #self.wnumid2wstrid = wnumid2wstrid\n","        for node in nodes_batch:\n","            strid = self.wnumid2wstrid[node]\n","            cps = self.training_cps[strid]\n","            self.node_pairs[node] = cps\n","            for cp in cps:\n","              self.extended_nodes_batch.add(cp[1])\n","        self.extended_nodes_batch = list(self.extended_nodes_batch)\n","        return self.extended_nodes_batch\n","    \n","    def get_loss(self, nodes_batch, embs_batch, recon_batch):\n","        # calculate loss_simi and loss+recon,\n","        # loss_reg is included in SGD optimizer as weight_decay\n","        loss_recon = self.get_loss_recon(nodes_batch, recon_batch)\n","        loss_simi = self.get_loss_simi(embs_batch)\n","        loss = loss_recon + self.alpha * loss_simi\n","        return loss\n","    \n","    #def get_loss_simi(self, embs_batch, nodes_batch):\n","    def get_loss_simi(self, embs_batch):\n","        #node2index n 是extended_nodes 就是worker num_id; i 是 整个batch 数量的索引\n","        node2index = {n:i for i,n in enumerate(self.extended_nodes_batch)}\n","        \n","        simi_feat = []\n","        simi_embs = []\n","        #用nodes_batch 把nodes_pair找出来. node_pairs在extended_nodes中赋值过了\n","        #self.node_pairs[node]\n","        \n","        \n","        for node, cps in self.node_pairs.items():\n","            for i, j in cps:\n","              #graph_simi 这里需要dataframe了\n","              wi=wnumid2wstrid[i]\n","              wj=wnumid2wstrid[j]\n","              #20200709 针对worker id 是 int的改造\n","              wistrid = 'w' + str(wi)\n","              wjstrid = 'w' + str(wj)\n","              simi = self.worker_similarity_df.loc[wistrid,wjstrid]\n","\n","              #simi_feat.append(torch.FloatTensor([self.graph_simi[i, j]]))\n","              simi_feat.append(torch.FloatTensor([simi]))\n","              dis_ij = (embs_batch[node2index[i]] - embs_batch[node2index[j]]) ** 2\n","              dis_ij = torch.exp(-dis_ij.sum())\n","              #torch.view 好比numpy.shape/numpy.reshape\n","              simi_embs.append(dis_ij.view(1))\n","        simi_feat = torch.cat(simi_feat, 0).to(self.device)\n","        simi_embs = torch.cat(simi_embs, 0)\n","        L = simi_feat * ((simi_embs - simi_feat) ** 2)\n","        return L.mean()\n","\n","    def get_loss_recon(self, nodes_batch, recon_batch):\n","        feats_batch = self.features[nodes_batch]\n","        # H_batch 应该就是哈德马积\n","        H_batch = (feats_batch * (self.beta - 1)) + 1\n","        assert feats_batch.size() == recon_batch.size() == H_batch.size()\n","        #**可以用来表示幂次运算 2 ** 7  # 2 to the power of 7\n","        L = ((recon_batch - feats_batch) * H_batch) ** 2\n","        return L.mean()"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8N-2chd9ZbCP"},"source":["### train 函数\n","\n"]},{"cell_type":"code","metadata":{"id":"YbWQ4Uzd9b26","executionInfo":{"status":"ok","timestamp":1610679582694,"user_tz":-660,"elapsed":5160,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["# 返回 deepFD\n","\n","def train_model(wnumid2wstrid,training_cps, deepFD, model_loss, device, epoch):\n","  #train_model(Dl, args, logger, deepFD, model_loss, device, epoch):\n","    #读一下 wnumid2wstrid 字典\n","    #读一下 training_cps\n","\n","    # D1.ds+'_train' =  np.arange(np.shape(graph_u2p)[0])\n","    #train_nodes = getattr(Dl, Dl.ds+'_train')\n","    train_nodes = np.arange(len(wnumid2wstrid))\n","\n","    # 这一步把 user id 打乱\n","    np.random.shuffle(train_nodes)\n","\n","    #存的是所有参数, 干啥呢\n","    params = []\n","    #learning rate, gama 是一个权重吗？按照原来的代码设定一下\n","    learning_rate = 0.025\n","    \n","    gama = 0.001\n","    # 参数为什么要手动更新呢?\n","    for param in deepFD.parameters():\n","        if param.requires_grad:\n","            params.append(param)\n","    \n","    #对参数的更新是自己写的,具体的还得再研究,其他方法的参数是怎么优化的,为什么要把参数一些参数拿出来?\n","    # gama 不知道写的是啥,\n","    optimizer = torch.optim.SGD(params, lr=lr, weight_decay=gamma)\n","    #下面两个都有什么区别optimizer 和deepFD 有什么区别呢?\n","    optimizer.zero_grad()\n","    deepFD.zero_grad()\n","    \n","    # b_sz 默认是 100, batch size的缩写 batches 就是一个整数, 176 = 2*2*2*11\n","    b_sz = 22\n","    batches = math.ceil(len(train_nodes) / b_sz)\n","\n","    #不知道是干啥,谁访问的点? 这里没用上\n","    visited_nodes = set()\n","    \n","    ### 弄这个地方\n","    #training_cps是一个字典,存的是一个点和他的邻居, 用的是defaultdict ,key:node_id ; value : [(id,pos_id/neg_id),]\n","    # training_cps = Dl.get_train()\n","    training_cps = training_cps\n","    \n","    #logger.info('sampled pos and neg nodes for each node in this epoch.')\n","    for index in range(batches):\n","        #弄出来一个batch,还是id,不是表示\n","        # 这两个nodes batch 有什么区别\n","        # 根据数量得到的一组,比如(0,100)\n","        nodes_batch = train_nodes[index*b_sz:(index+1)*b_sz]\n","        \n","        \n","        # nodes_batch = np.asarray(model_loss.extend_nodes(nodes_batch, training_cps))\n","        nodes_batch = np.asarray(model_loss.extend_nodes(nodes_batch))\n","        \n","        \n","        # 用asarray把list 转换成array了，nodes_batch 就是entended nodes batch；\n","        #nodes_batch = np.asarray(Loss_DeepFD.extend_nodes(nodes_batch,training_cps))\n","        #nodes_batch = np.asarray(model_loss.extend_nodes(nodes_batch))\n","        \n","        # |= 取了一个并集\n","        visited_nodes |= set(nodes_batch)\n","\n","        #deepFD(nodes_batch) 会把参数读到 forward 里面去执行,对feats 进行赋值,才是真正的 representation\n","        # 需要两个 model 实例,只需要一个\n","        # \n","        # PB_embs_batch, PB_recon_batch = PB_model(nodes_batch)\n","        # AB_embs_batch, AB_recon_batch = AB_model(nodes_batch)\n","\n","        # pb_embedding, ab_embedding, embedding, pb_recons, ab_recons = deepSW(nodes_batch)\n","        \n","        embs_batch, recon_batch = deepFD(nodes_batch)\n","        loss = model_loss.get_loss(nodes_batch, embs_batch, recon_batch)\n","\n","        # embeddings, pb_embedding, ab_embedding, pb_recons, ab_recons = deepSW(nodes_batch)\n","        \n","        # loss = model_loss.get_loss(nodes_batch, pb_recons, ab_recons)\n","\n","        # logger.info(f'EP[{epoch}], Batch [{index+1}/{batches}], Loss: {loss.item():.4f}, Dealed Nodes [{len(visited_nodes)}/{len(train_nodes)}]')\n","        loss.backward()\n","\n","        nn.utils.clip_grad_norm_(deepFD.parameters(), 5)\n","        optimizer.step()\n","\n","        optimizer.zero_grad()\n","        deepFD.zero_grad()\n","\n","        #输入一下loss\n","        print('epoch', epoch,'batch',index, 'loss =',loss.data.numpy())\n","        # stop when all nodes are trained\n","        if len(visited_nodes) == len(train_nodes):\n","            break\n","\n","    return deepFD "],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UoNOEJQc9Z8N"},"source":["#### get_embeddings"]},{"cell_type":"code","metadata":{"id":"tPGrjPOpOL1V","executionInfo":{"status":"ok","timestamp":1610679582694,"user_tz":-660,"elapsed":5158,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["#返回一个worker_emb_numpy\n","def get_embeddings(deepFD, wnumid2wstrid, path):\n","  train_nodes = np.arange(len(wnumid2wstrid))\n","  with torch.no_grad():\n","    embs_batch, _ = deepFD(train_nodes)\n","  assert len(embs_batch) == len(train_nodes)\n","  embs_temp = embs_batch.detach()\n","  worker_emb_numpy = embs_temp.numpy()\n","  np.save(path+'worker_emb_numpy.npy',worker_emb_numpy)\n","  \n","  return worker_emb_numpy\n","    \n","    \n","\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WHLmj8Yd8TB7"},"source":["### 选expert的方法\n"]},{"cell_type":"markdown","metadata":{"id":"06d258wSnWLn"},"source":["#### distance_experts\n","20200701 通过算距离的方式, 选出experts\n","距离越大, ability 越低;"]},{"cell_type":"code","metadata":{"id":"SaHCg4q5LG0e","executionInfo":{"status":"ok","timestamp":1610679582695,"user_tz":-660,"elapsed":5157,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["# from sklearn.metrics import pairwise_distances\n","# pairwise_distances(a,metric=\"cosine\")\n","# dists = pairwise_distances(worker_emb_numpy[i].reshape(1,-1), worker_emb_numpy[j].reshape(1,-1))"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8_7CJUwmaIH","executionInfo":{"status":"ok","timestamp":1610679582695,"user_tz":-660,"elapsed":5155,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["from sklearn.metrics import pairwise_distances\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","## scipy 也有一些算距离的公式\n","# from scipy.spatial import distance_matrix\n","\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as colors\n","from itertools import cycle\n","from sklearn.cluster import OPTICS, cluster_optics_dbscan\n","def distance_experts(wstrid2wnumid, wnumid2wstrid, worker_acc_dict, t2w, t2wl, w2t, path, deepSW):\n","  \n","  #读入需要分类的数据\n","  worker_emb_numpy = get_embeddings(deepSW, wnumid2wstrid, path)\n","  w2sumdistance_dict = {}\n","  # worker_emb_numpy = worker_emb_numpy[worker_numid_list]\n","\n","  ### 20200701 计算每一个worker的degree\n","  # 最后用一个字典把结果存上, key 用str_id 还是 num id ? 先用num_id\n","  # 可以不用pandas\n","  # 20200701 可以和optics 最远的距离, 和我们求和的方式比较一下 experts\n","  for i in range(worker_emb_numpy.shape[0]):\n","    temp = 0\n","    for j in range(worker_emb_numpy.shape[0]):\n","      # 20200702 加上reshape 操作拉成1维\n","      # dists = pairwise_distances(worker_emb_numpy[i].reshape(1,-1), worker_emb_numpy[j].reshape(1,-1))\n","      # 20200716 cosinsimilarity 试一次\n","      dists = 1 - cosine_similarity(worker_emb_numpy[i].reshape(1,-1), worker_emb_numpy[j].reshape(1,-1))\n","      temp = temp + dists\n","    # 20200702 temp[0][0] 是数值 temp[0] 是 shape (1,) 的数组, temp 是shape[1,1] 的数组\n","    w2sumdistance_dict[i] = temp[0][0]\n","  \n","  # text = path + 'LLA-Input_worker_distance_acc_num.txt'\n","  # with open(text, 'wt') as f:\n","  #   print('-----------------------','worker_dbscan_cluster_acc', file=f)\n","\n","  \n","  return w2sumdistance_dict\n","\n","  \n","  "],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cn9AOyvF61ne"},"source":["### Top1_experts\n","aggregation 方法"]},{"cell_type":"code","metadata":{"id":"EjqHA8wj1iBZ","executionInfo":{"status":"ok","timestamp":1610679582695,"user_tz":-660,"elapsed":5154,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["# 20200702 选出top1的worker 作为结果;\n","def take3rd(elem):\n","    return elem[2]\n","def Top1_experts(w2sumdistance_dict,wstrid2wnumid, wnumid2wstrid, t2wl):\n","\n","  #t2wl = self.t2wl\n","  # worker_list = []\n","  # for i in idnumpy:\n","  #   strid = wnumid2wstrid[i]\n","  #   worker_list.append(strid)\n","  \n","  # key : task_id, value: wnumid,label,distance\n","  t2pwld = {}\n","  for t in t2wl:\n","    wl_list = t2wl[t]\n","    w1d_list = []\n","    predict = 0\n","    \n","    for wl_tuple in wl_list:\n","      worker = wl_tuple[0]\n","      label = wl_tuple[1]\n","      numid = wstrid2wnumid[worker]\n","      w_dis = w2sumdistance_dict[numid]\n","      w1d_list.append([worker,label,w_dis])\n","    # 根据 sum_distance  升序排列;\n","    w1d_list.sort(key=take3rd,reverse=False)\n","    # 取排在第一位的 wld 信息\n","    # 20200716 取几个直接进行切片就行了\n","    t2pwld[t] = w1d_list[0:1]\n","\n","  return t2pwld\n","  "],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"7mlAnS9d79ka","executionInfo":{"status":"ok","timestamp":1610679582696,"user_tz":-660,"elapsed":5153,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["# 2020随机一个结果看看;"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"EajYSpL860s6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610679582696,"user_tz":-660,"elapsed":5140,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}},"outputId":"e89e652c-b4e0-49b7-b1c3-32bd4d568995"},"source":["aaa = [5,2,3,1,4]\n","aaa.sort()\n","print(aaa)\n","bbb = aaa[:3]\n","print(bbb)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["[1, 2, 3, 4, 5]\n","[1, 2, 3]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c45N4gefz8Kc","executionInfo":{"status":"ok","timestamp":1610679582697,"user_tz":-660,"elapsed":5139,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["# 把wnum_id 转成str_id\n","def wstr_2_distance(w2sumdistance_dict,wstrid2wnumid):\n","\n","  # key : wstr_id, value: distance\n","  wstr2distance = {}\n","  for wnumid in w2sumdistance_dict:\n","    wstrid = wnumid2wstrid[wnumid]\n","    wstr2distance[wstrid] = w2sumdistance_dict[wnumid]\n","\n","  return wstr2distance"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"KGT90R768DKm","executionInfo":{"status":"ok","timestamp":1610679582697,"user_tz":-660,"elapsed":5138,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["##20200702\n","# 为 top_1 设计的方法;\n","def getaccuracy_Top1(e2truth, t2pwld, t2w):\n","  #label 应该是字符串\n","  count = 0\n","  right_taskid_list = []\n","  wrong_taskid_list = []\n","  # truth = ''\n","  \n","  for e in t2w:\n","\n","    ## 20200627 加上这一句就可以用全部数据集了;\n","    if e not in e2truth:\n","      continue\n","    # print(e)\n","    temp = t2pwld[e]\n","    #temp 的第二位表示 label\n","    predict_answer = temp[0][1]\n","  \n","    count += 1 \n","    \n","    if predict_answer == e2truth[e]:\n","      \n","      right_taskid_list.append(e)\n","    else:\n","      wrong_taskid_list.append(e)  \n","\n","    # 准确率, 回答正确的list, 回答错误的list\n","  return len(right_taskid_list)/count, right_taskid_list, wrong_taskid_list"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"966ir-RMIiMT"},"source":["### Similar3 方法\n","20200708 在回答 task i 的 worker 中找到 3 个 最相似的worker 做MV"]},{"cell_type":"code","metadata":{"id":"rfsjCGX5IhfV","executionInfo":{"status":"ok","timestamp":1610679582698,"user_tz":-660,"elapsed":5137,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["from sklearn.metrics import pairwise_distances\n","import numpy as np\n","from itertools import combinations\n","\n","## scipy 也有一些算距离的公式\n","# from scipy.spatial import distance_matrix\n","\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as colors\n","from itertools import cycle\n","from sklearn.cluster import OPTICS, cluster_optics_dbscan\n","\n","\n","#20200709 \n","def S3_experts(wstrid2wnumid, wnumid2wstrid,  t2w, t2wl, w2t, path, deepSW):\n","  \n","  #读入需要分类的数据\n","  worker_emb_numpy = get_embeddings(deepSW, wnumid2wstrid, path)\n","  # 类似t2wl的, 直接用mv 就行了;\n","  t2s3w = {}\n","  t2s3wl = {}\n","  \n","  for t in t2w:\n","    w_list = t2w[t]\n","    # 把距离都算一遍\n","    # temp = 0\n","    temp = 10000\n","    for w_set in combinations(w_list,3):\n","      dists = 0\n","      #计算一组3worker 之间的距离\n","      for cal_distance in combinations(w_set,2):\n","        wi = cal_distance[0]\n","        wj = cal_distance[1]      \n","        numid_i = wstrid2wnumid[wi]\n","        numid_j = wstrid2wnumid[wj]\n","\n","        dist = pairwise_distances(worker_emb_numpy[numid_i].reshape(1,-1), worker_emb_numpy[numid_j].reshape(1,-1))\n","        dists = dists + dist\n","      if dists < temp:\n","        temp = dist\n","        s3workers = w_set\n","    t2s3w[t] = s3workers\n","  # 弄一个t2wl 类似的东西;\n","  for t in t2wl:\n","    wl_list = t2wl[t]\n","    s3wl_list = []\n","    for w,l in wl_list:\n","      if w in t2s3w[t]:\n","        s3wl_list.append([w,l])\n","    t2s3wl[t] = s3wl_list\n","\n","      \n","\n","\n","  \n","  return t2s3wl"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"zNrLfbn37q_D","executionInfo":{"status":"ok","timestamp":1610679582698,"user_tz":-660,"elapsed":5135,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["# 20200720 有一些问题 \n","#from sklearn.metrics import pairwise_distances\n","# import numpy as np\n","# from itertools import combinations\n","\n","# ## scipy 也有一些算距离的公式\n","# # from scipy.spatial import distance_matrix\n","\n","# from sklearn.manifold import TSNE\n","# import matplotlib.pyplot as plt\n","# import matplotlib.colors as colors\n","# from itertools import cycle\n","# from sklearn.cluster import OPTICS, cluster_optics_dbscan\n","\n","\n","# # 20200702 选出top1的worker 作为结果;\n","# def take3rd(elem):\n","#     return elem[2]\n","# def Top1_experts(w2sumdistance_dict,wstrid2wnumid, wnumid2wstrid, t2wl):\n","\n","#   #t2wl = self.t2wl\n","#   # worker_list = []\n","#   # for i in idnumpy:\n","#   #   strid = wnumid2wstrid[i]\n","#   #   worker_list.append(strid)\n","  \n","#   # key : task_id, value: wnumid,label,distance\n","#   t2pwld = {}\n","#   for t in t2wl:\n","#     wl_list = t2wl[t]\n","#     w1d_list = []\n","#     predict = 0\n","    \n","#     for wl_tuple in wl_list:\n","#       worker = wl_tuple[0]\n","#       label = wl_tuple[1]\n","#       numid = wstrid2wnumid[worker]\n","#       w_dis = w2sumdistance_dict[numid]\n","#       w1d_list.append([worker,label,w_dis])\n","#     # 根据 sum_distance  升序排列;\n","#     w1d_list.sort(key=take3rd,reverse=False)\n","#     # 取排在第一位的 wld 信息\n","#     # 20200716 取几个直接进行切片就行了\n","#     t2pwld[t] = w1d_list[0:3]\n","\n","#   return t2pwld\n","\n","# # 20200612 为甚要用类写, 不这么写了;\n","\n","# def W_AggMethod(t2wl,label_set):\n","\n","#   #t2wl = self.t2wl\n","#   t2lpd={}\n","#   for t in t2wl:\n","#     t2lpd[t]={}\n","\n","#     # multi label 初始化;\n","#     for label in label_set:\n","#       t2lpd[t][label] = 0\n","#     # e2lpd[e]['0']=0\n","#     # e2lpd[e]['1']=0\n","    \n","#     alls = len(t2wl[t])\n","#     for i in range(alls):\n","#       # 按顺序*乘一个权重;  第一个的一票顶2票, 或者一票顶3票?\n","\n","\n","#     for item in t2wl[t]:\n","#       label=item[1]\n","#       t2lpd[t][label]+= 1\n","\n","    \n","#     if alls!=0:\n","#         # e2lpd[e]['0']=1.0*e2lpd[e]['0']/alls\n","#         # e2lpd[e]['1']=1.0*e2lpd[e]['1']/alls\n","#       for label in label_set:\n","#           t2lpd[t][label] = 1.0 * t2lpd[t][label] / alls\n","#     else:\n","#         # e2lpd[e]['0']=0.5\n","#         # e2lpd[e]['1']=0.5\n","#       for label in label_set:\n","#           t2lpd[t][label] = 1.0 / len(label_set)\n","\n","#   # return self.expand(e2lpd)\n","#   return t2lpd\n","\n"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JzktnlMo46-Y"},"source":["#### S3 MV 方法"]},{"cell_type":"code","metadata":{"id":"krOcYfAStpD-","executionInfo":{"status":"ok","timestamp":1610679582699,"user_tz":-660,"elapsed":5135,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["# 20200612 为甚要用类写, 不这么写了;\n","\n","def AggMethod(t2wl,label_set):\n","\n","  #t2wl = self.t2wl\n","  t2lpd={}\n","  for t in t2wl:\n","    t2lpd[t]={}\n","\n","    # multi label 初始化;\n","    for label in label_set:\n","      t2lpd[t][label] = 0\n","    # e2lpd[e]['0']=0\n","    # e2lpd[e]['1']=0\n","    \n","    alls = len(t2wl[t])\n","    for item in t2wl[t]:\n","      label=item[1]\n","      t2lpd[t][label]+= 1\n","\n","    \n","    if alls!=0:\n","        # e2lpd[e]['0']=1.0*e2lpd[e]['0']/alls\n","        # e2lpd[e]['1']=1.0*e2lpd[e]['1']/alls\n","      for label in label_set:\n","          t2lpd[t][label] = 1.0 * t2lpd[t][label] / alls\n","    else:\n","        # e2lpd[e]['0']=0.5\n","        # e2lpd[e]['1']=0.5\n","      for label in label_set:\n","          t2lpd[t][label] = 1.0 / len(label_set)\n","\n","  # return self.expand(e2lpd)\n","  return t2lpd"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"DpSgrHlE5YGc","executionInfo":{"status":"ok","timestamp":1610679582699,"user_tz":-660,"elapsed":5133,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["##20200702\n","\n","def getaccuracy(e2truth, e2lpd, t2w):\n","  # 需要确保e2lpd 已经是一个完整结果了; \n","  #label 应该是字符串\n","  count = 0\n","  right_taskid_list = []\n","  wrong_taskid_list = []\n","  # truth = ''\n","  \n","  for e in t2w:\n","\n","    ## 20200627 加上这一句就可以用全部数据集了;\n","    if e not in e2truth:\n","      continue\n","    # print(e)\n","    temp = 0\n","    predict_answer = 0\n","    #找到worker的mv-answer e2lpd 就是各种方法的结果; temp 存了一个值\n","    for label in e2lpd[e]:\n","      if temp < e2lpd[e][label]:\n","        temp = e2lpd[e][label]\n","        #print(temp)\n","      \n","    count += 1 \n","    for label in e2lpd[e]:\n","      if temp == e2lpd[e][label]:\n","        predict_answer = label\n","      # print('temp',temp)\n","      # print('predict',predict_answer)\n","    \n","    if predict_answer == e2truth[e]:\n","      \n","      right_taskid_list.append(e)\n","    else:\n","      wrong_taskid_list.append(e)  \n","\n","    # 准确率, 回答正确的list, 回答错误的list\n","  return len(right_taskid_list)/count, right_taskid_list, wrong_taskid_list"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"LYvK4IlbnJGQ","executionInfo":{"status":"ok","timestamp":1610679582700,"user_tz":-660,"elapsed":5133,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":[""],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eua_KzQ7H9FS"},"source":["## weighted_mv\n","20200720 1_weight = sum(w_ti);"]},{"cell_type":"code","metadata":{"id":"gY4Mxa7IH9FU","executionInfo":{"status":"ok","timestamp":1610679582700,"user_tz":-660,"elapsed":5131,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["# 20201026 返回根据距离排序的list；\n","# 20200702 选出top1的worker 作为结果;\n","\n","# 取出第三个元素, 在下面事一个排序的key\n","def take3rd(elem):\n","    return elem[2]\n","# 20200720 对worker 按照distance 排序了 worker 作为结果;\n","def Sort_experts(w2sumdistance_dict,wstrid2wnumid, wnumid2wstrid, t2wl):\n","\n","  #t2wl = self.t2wl\n","  # worker_list = []\n","  # for i in idnumpy:\n","  #   strid = wnumid2wstrid[i]\n","  #   worker_list.append(strid)\n","  \n","  # key : task_id, value: wnumid,label,distance\n","  t2pwld = {}\n","  for t in t2wl:\n","    wl_list = t2wl[t]\n","    w1d_list = []\n","    predict = 0\n","    \n","    for wl_tuple in wl_list:\n","      worker = wl_tuple[0]\n","      label = wl_tuple[1]\n","      numid = wstrid2wnumid[worker]\n","      w_dis = w2sumdistance_dict[numid]\n","      w1d_list.append([worker,label,w_dis])\n","    # 根据 sum_distance  升序排列;\n","    w1d_list.sort(key=take3rd,reverse=False)\n","    # 取排在第一位的 wld 信息\n","    # 20200716 取几个直接进行切片就行了, 不需要进行切片了;\n","    t2pwld[t] = w1d_list\n","\n","  return t2pwld"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"KjT2p0PWH9FX","executionInfo":{"status":"ok","timestamp":1610679582700,"user_tz":-660,"elapsed":5130,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["def my_sum(i):\n","  if i < 0:\n","      raise ValueError\n","  elif i <= 1:\n","      return i\n","  else:\n","      return i + my_sum(i-1)\n","\n","# 20200612 为甚要用类写, 不这么写了;\n","\n","def W_MV_AggMethod(t2wl,label_set):\n","\n","  #t2wl = self.t2wl\n","  t2lpd={}\n","  for t in t2wl:\n","    t2lpd[t]={}\n","\n","    # multi label 初始化;\n","    for label in label_set:\n","      t2lpd[t][label] = 0\n","    # e2lpd[e]['0']=0\n","    # e2lpd[e]['1']=0\n","    \n","    # alls = len(t2wl[t])\n","    # for item in t2wl[t]:\n","    #   label=item[1]\n","    #   t2lpd[t][label]+= 1\n","\n","    #20200720 加上一个weight;\n","    wld_list = t2wl[t]\n","    alls = len(wld_list)\n","    weight_sum = my_sum(alls)\n","    \n","    for i in range(alls):\n","      label = wld_list[i][1]\n","      weight = 1.0*(alls-i)\n","      t2lpd[t][label] += weight\n","\n","\n","    # for item in t2wl[t]:\n","    #   label=item[1]\n","    #   t2lpd[t][label]+= 1\n","    \n","    if alls!=0:\n","        # e2lpd[e]['0']=1.0*e2lpd[e]['0']/alls\n","        # e2lpd[e]['1']=1.0*e2lpd[e]['1']/alls\n","      for label in label_set:\n","          #20200720 weight_sum\n","          t2lpd[t][label] = 1.0 * t2lpd[t][label] / weight_sum\n","    else:\n","        # e2lpd[e]['0']=0.5\n","        # e2lpd[e]['1']=0.5\n","      for label in label_set:\n","          t2lpd[t][label] = 1.0 / len(label_set)\n","\n","  # return self.expand(e2lpd)\n","  return t2lpd"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"yDH04RNfH9FZ","executionInfo":{"status":"ok","timestamp":1610679582701,"user_tz":-660,"elapsed":5129,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["\n","##20200702\n","\n","def W_MV_getaccuracy(e2truth, e2lpd, t2w):\n","  # 需要确保e2lpd 已经是一个完整结果了; \n","  #label 应该是字符串\n","  count = 0\n","  right_taskid_list = []\n","  wrong_taskid_list = []\n","  # truth = ''\n","  \n","  for e in t2w:\n","\n","    ## 20200627 加上这一句就可以用全部数据集了;\n","    if e not in e2truth:\n","      continue\n","    # print(e)\n","    temp = 0\n","    predict_answer = 0\n","    #找到worker的mv-answer e2lpd 就是各种方法的结果; temp 存了一个值\n","    for label in e2lpd[e]:\n","      if temp < e2lpd[e][label]:\n","        temp = e2lpd[e][label]\n","        #print(temp)\n","      \n","    count += 1 \n","    for label in e2lpd[e]:\n","      if temp == e2lpd[e][label]:\n","        predict_answer = label\n","      # print('temp',temp)\n","      # print('predict',predict_answer)\n","    \n","    if predict_answer == e2truth[e]:\n","      \n","      right_taskid_list.append(e)\n","    else:\n","      wrong_taskid_list.append(e)  \n","\n","    # 准确率, 回答正确的list, 回答错误的list\n","  return len(right_taskid_list)/count, right_taskid_list, wrong_taskid_list"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"gnoXwMMMg1zx"},"source":["## 20201026 运行之前所有代码;"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lz0eBYNgNqXO"},"source":["import random\r\n","import math\r\n","import csv\r\n","import random\r\n","import numpy as np\r\n","import sys\r\n","import pandas as pd\r\n","import torch\r\n","from itertools import combinations\r\n","\r\n","import pickle\r\n","\r\n","from collections import defaultdict, Counter\r\n","from sklearn.cluster import OPTICS, cluster_optics_dbscan\r\n","\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","import copy\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rTYhZnm-xxsp"},"source":["## main 函数\n","Top1 和 WMV"]},{"cell_type":"code","metadata":{"id":"dSRGKrhsEeST","executionInfo":{"status":"ok","timestamp":1610679800979,"user_tz":-660,"elapsed":1042,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["#20201231 输出文件\r\n","save_path = '/content/drive/MyDrive/Colab Notebooks/Deep worker cluster/files_for_IJCAI2021/data/'\r\n","pathlist = [#save_path + 'active-crowd-toolkit/CF/',\r\n","        # save_path + 'active-crowd-toolkit/CF_amt/',\r\n","        save_path +'active-crowd-toolkit/MS/',\r\n","        #  save_path +'active-crowd-toolkit/SP/',\r\n","        #  save_path +'active-crowd-toolkit/SP_amt/',\r\n","        #  save_path +'active-crowd-toolkit/ZenCrowd_all/',\r\n","        #  save_path +'active-crowd-toolkit/ZenCrowd_in/',\r\n","         save_path +'active-crowd-toolkit/ZenCrowd_us/',\r\n","         save_path +'SpectralMethodsMeetEM/bluebird/',\r\n","        #  save_path +'SpectralMethodsMeetEM/dog/',\r\n","        #  save_path +'SpectralMethodsMeetEM/rte/',\r\n","         save_path +'SpectralMethodsMeetEM/trec/',\r\n","        #  save_path +'SpectralMethodsMeetEM/web/',\r\n","         save_path +'crowdscale2013/fact_eval/',\r\n","        #  save_path +'crowdscale2013/sentiment/',\r\n","         save_path +'crowd_truth_inference/d_Duck Identification/',\r\n","        #  save_path +'crowd_truth_inference/d_jn-product/',\r\n","        #  save_path +'crowd_truth_inference/d_sentiment/',\r\n","        #  save_path +'crowd_truth_inference/s4_Dog data/',\r\n","         save_path +'crowd_truth_inference/s4_Face Sentiment Identification/',\r\n","        #  save_path +'crowd_truth_inference/s4_Relevance/',\r\n","        #  save_path +'crowd_truth_inference/s5_AdultContent/'\r\n","          ]"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"iLNCWYCM4yyz","executionInfo":{"status":"ok","timestamp":1610679808406,"user_tz":-660,"elapsed":1007,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}}},"source":["#20201026 输出文件\n","# pathlist = ['/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/CF/',\n","#         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/CF_amt/',\n","#         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/MS/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/SP/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/SP_amt/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/ZenCrowd_all/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/ZenCrowd_in/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/ZenCrowd_us/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/SpectralMethodsMeetEM/bluebird/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/SpectralMethodsMeetEM/dog/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/SpectralMethodsMeetEM/rte/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/SpectralMethodsMeetEM/trec/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/SpectralMethodsMeetEM/web/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowdscale2013/fact_eval/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowdscale2013/sentiment/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/d_Duck Identification/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/d_jn-product/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/d_sentiment/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/s4_Dog data/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/s4_Face Sentiment Identification/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/s4_Relevance/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/s5_AdultContent/'\n","#           ]\n","# pathlist = ['Notebooks/Deep worker cluster/datasets/data_at_scale/SpectralMethodsMeetEM/trec/'\n","\n","\n","headers = ['alpha','layer','emb',\n","           'epoch_10_Top1_ACC','epoch_10_Top1_ACC_Right','epoch_10_Top1_ACC_Wrong','epoch_10_MV_ACC','epoch_10_MV_ACC_Right','epoch_10_MV_ACC_Wrong',\n","           'epoch_20_Top1_ACC','epoch_20_Top1_ACC_Right','epoch_20_Top1_ACC_Wrong','epoch_20_MV_ACC','epoch_20_MV_ACC_Right','epoch_20_MV_ACC_Wrong',\n","           'epoch_30_Top1_ACC','epoch_30_Top1_ACC_Right','epoch_30_Top1_ACC_Wrong','epoch_30_MV_ACC','epoch_30_MV_ACC_Right','epoch_30_MV_ACC_Wrong',\n","           'epoch_40_Top1_ACC','epoch_40_Top1_ACC_Right','epoch_40_Top1_ACC_Wrong','epoch_40_MV_ACC','epoch_40_MV_ACC_Right','epoch_40_MV_ACC_Wrong',\n","           'epoch_50_Top1_ACC','epoch_50_Top1_ACC_Right','epoch_50_Top1_ACC_Wrong','epoch_50_MV_ACC','epoch_50_MV_ACC_Right','epoch_50_MV_ACC_Wrong',\n","           'Top1_ACC_AV','Top1_ACC_MAX','MV_ACC_AV','MV_ACC_MAX']\n","\n","output_path = '/content/drive/MyDrive/Colab Notebooks/Deep worker cluster/files_for_IJCAI2021/'\n","for path in pathlist:\n","\n","  datasetname = path.split('/')[-2]\n","  # result_file = output_path + datasetname+ '_noise_WORT_TOP1_MV_answer_3.csv'\n","  result_file = output_path + datasetname+ '_noise_a_0_WORT_TOP1_MV_answer_3.csv'\n","  with open(result_file,'w')as f:\n","    f_csv = csv.writer(f)\n","    f_csv.writerow(headers)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"UFBGvbd6NRN_"},"source":["# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vYanEksG0r1o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610702475553,"user_tz":-660,"elapsed":1960314,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}},"outputId":"5bb9d5ca-a70f-4b73-9886-1361ad236d6f"},"source":["#20200824 对alpha 值的改变的测试\n","# 下面的值已经不全了, 因为有的数据集出了一些问题\n","# alpha_list = [0]\n","# emb_size_list= [10]\n","# layer_size_list = [64]\n","# alpha_list = [10,20,30,40,50]\n","alpha_list = [0]\n","emb_size_list= [2,3,4,5,6,7,8,9,10]\n","layer_size_list = [16,32,64]\n","output_path = '/content/drive/MyDrive/Colab Notebooks/Deep worker cluster/files_for_IJCAI2021/'\n","\n","\n","for path in pathlist:\n","  seed = 1234\n","  device = torch.device('cpu')\n","  # 这几个要调整一下的;\n","  b_sz = 10\n","  hidden_size = 128\n","  emb_size = 4\n","  epochs = 50\n","  # Hyper parameters\n","  # 20200824 改一下alpha 参数试一试;\n","  # alpha = apa\n","  beta = 20\n","  gamma = 0.01\n","  lr = 0.025\n","\n","  # datafile = path+'answer_3.csv'\n","  # datafile = path + 'answer_mv_failed.csv'\n","  # datafile =  path + 'select_for_new_2.csv'\n","  ########## pb + ab 模式的input\n","  with open(path+'w2task_input.pickle','rb') as file:\n","    w2task_input = pickle.load(file)\n","\n","  # with open(path+'w2w_input.pickle','rb') as file:\n","  #   w2w_input = pickle.load(file) \n","  #######################################################################\n","  with open(path+'wnumid2wstrid.pickle','rb') as file:\n","    wnumid2wstrid = pickle.load(file)\n","\n","  with open(path+'wstrid2wnumid.pickle','rb') as file:\n","    wstrid2wnumid = pickle.load(file)\n","\n","  with open(path+'t2w.pickle','rb') as file:\n","    t2w = pickle.load(file)\n","  with open(path+'t2wl.pickle','rb') as file:\n","    t2wl = pickle.load(file)\n","\n","\n","  with open(path+'t2truth.pickle','rb') as file:\n","    t2truth = pickle.load(file)\n","\n","\n","  with open(path+'w2t.pickle','rb') as file:\n","    w2t = pickle.load(file)\n","\n","  with open(path+'w2tl.pickle','rb') as file:\n","    w2tl = pickle.load(file)\n","\n","  with open(path+'worker_acc_dict.pickle','rb') as file:\n","    worker_acc_dict = pickle.load(file)\n","\n","\n","\n","  with open(path+'w2w.pickle','rb') as file:\n","    w2w = pickle.load(file) \n","\n","  with open(path+'training_cps.pickle','rb') as file:\n","    training_cps = pickle.load(file) \n","\n","\n","  with open(path+'label_set.pickle','rb') as file:\n","    label_set = pickle.load(file)\n","\n","  # dataset = path[78:]\n","  # rows = []\n","  csv_data = pd.read_csv(path+'worker_simlarity.csv',low_memory = False,index_col=0)\n","  worker_similarity_df = pd.DataFrame(csv_data)\n","\n","  datasetname = path.split('/')[-2]\n","  output_file = output_path + datasetname+ '_noise_WORT_TOP1_MV_answer_3.csv'\n","  \n","\n","  for apa in alpha_list:\n","    \n","    for layer in layer_size_list:\n","      for emb_size in emb_size_list:\n","    \n","        #20201026\n","        ##\n","        \n","        output_list =[]\n","        output_list.append(apa)\n","        output_list.append(layer)\n","        output_list.append(emb_size)\n","\n","          \n","        # 20200824 改一下alpha 参数试一试;从最外层循环读进来的\n","        alpha = apa  \n","\n","        # model 构建和训练\n","        pb_feat_size = len(t2w)\n","        w_pb_input = input2numpy(w2task_input,wstrid2wnumid,wnumid2wstrid,pb_feat_size,path,'w_pb_input.npy')\n","        w_pb_input_tensor = torch.FloatTensor(w_pb_input).to(device)\n","        # 20200824 DeepFD 函数也改了;\n","        deepFD = DeepFD(w_pb_input_tensor, w_pb_input_tensor.size(1),layer, emb_size)\n","        deepFD.to(device)\n","        #应该是在这才循环apha\n","        model_loss = Loss_DeepFD(w_pb_input_tensor, worker_similarity_df, wnumid2wstrid, training_cps, device, alpha, beta, gamma)\n","\n","        # 需要写下来的东西, 保存起来的;\n","        \n","        ####20200811多一个max指标\n","        max_temp_Top1 = 0\n","        max_temp_MV = 0\n","\n","        acc_Top1_all = 0\n","        acc_WM_all = 0\n","\n","        for epoch in range(epochs):\n","          #logger.info(f'----------------------EPOCH {epoch}-----------------------')\n","          \n","          deepFD = train_model(wnumid2wstrid, training_cps, deepFD, model_loss, device, epoch)\n","          #deepSW = train_model_DeepSW(wnumid2wstrid, deepSW, model_loss, device, epoch)\n","          \n","          \n","          #每训练10次出一个结果;\n","          if (epoch+1) % 10 == 0:    \n","            w2sumdistance_dict = distance_experts(wstrid2wnumid, wnumid2wstrid, worker_acc_dict, t2w, t2wl, w2t, path, deepFD)\n","            \n","            ##Top1 方法\n","            # Top1_epoch_list = []\n","            t2pwld  = Top1_experts(w2sumdistance_dict, wstrid2wnumid, wnumid2wstrid, t2wl)\n","            acc_Top1, rightlist_Top1, wronglist_Top1  = getaccuracy_Top1(t2truth, t2pwld, t2w)\n","            \n","                    \n","            \n","            output_list.append(acc_Top1)\n","            output_list.append(len(rightlist_Top1))\n","            output_list.append(len(wronglist_Top1))\n","            \n","            # Top1_value_list.extend(Top1_epoch_list)\n","            acc_Top1_all = acc_Top1_all + acc_Top1\n","            # 20200811 多加的MAX的计算\n","            if acc_Top1 > max_temp_Top1:\n","              max_temp_Top1 = acc_Top1\n","\n","            ## weighted mv 方法\n","            # WM_epoch_list = []\n","            mv_t2pwld  = Sort_experts(w2sumdistance_dict, wstrid2wnumid, wnumid2wstrid, t2wl)\n","            mv_t2lpd = W_MV_AggMethod(mv_t2pwld,label_set)\n","            acc_mv, rightlist_mv, wronglist_mv  = W_MV_getaccuracy(t2truth, mv_t2lpd, t2w)\n","            \n","            output_list.append(acc_mv)\n","            output_list.append(len(rightlist_mv))\n","            output_list.append(len(wronglist_mv))\n","            \n","            # WM_value_list.extend(WM_epoch_list)\n","            acc_WM_all = acc_WM_all + acc_mv\n","            # 20200811 多加的MAX的计算\n","            if acc_mv > max_temp_MV:\n","              max_temp_MV = acc_mv\n","            \n","            ## 训练完毕,计算最后结果写入文件\n","            if (epoch+1) % 50 == 0:\n","              acc_Top1_all = acc_Top1_all / 5\n","              output_list.append(acc_Top1_all)\n","              output_list.append(max_temp_Top1)\n","\n","              acc_WM_all = acc_WM_all / 5\n","              output_list.append(acc_WM_all)\n","              output_list.append(max_temp_MV)\n","\n","              with open(output_file,'a+')as f:\n","                f_csv = csv.writer(f)\n","                f_csv.writerow(output_list)\n","              "],"execution_count":28,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n","epoch 4 batch 14 loss = 2.4539053\n","epoch 4 batch 15 loss = 2.3867378\n","epoch 4 batch 16 loss = 2.4544718\n","epoch 4 batch 17 loss = 2.487422\n","epoch 4 batch 18 loss = 2.3173964\n","epoch 5 batch 0 loss = 2.3350198\n","epoch 5 batch 1 loss = 2.5833275\n","epoch 5 batch 2 loss = 2.3196468\n","epoch 5 batch 3 loss = 2.458021\n","epoch 5 batch 4 loss = 2.2637415\n","epoch 5 batch 5 loss = 2.5290709\n","epoch 5 batch 6 loss = 2.342804\n","epoch 5 batch 7 loss = 2.4007227\n","epoch 5 batch 8 loss = 2.1097195\n","epoch 5 batch 9 loss = 2.524586\n","epoch 5 batch 10 loss = 2.2316613\n","epoch 5 batch 11 loss = 2.575252\n","epoch 5 batch 12 loss = 2.4171696\n","epoch 5 batch 13 loss = 2.3164313\n","epoch 5 batch 14 loss = 2.2050302\n","epoch 5 batch 15 loss = 2.6282492\n","epoch 6 batch 0 loss = 2.374275\n","epoch 6 batch 1 loss = 2.4312558\n","epoch 6 batch 2 loss = 2.5043182\n","epoch 6 batch 3 loss = 2.6227229\n","epoch 6 batch 4 loss = 2.3664658\n","epoch 6 batch 5 loss = 2.4877694\n","epoch 6 batch 6 loss = 2.371396\n","epoch 6 batch 7 loss = 2.6546268\n","epoch 6 batch 8 loss = 2.2412133\n","epoch 6 batch 9 loss = 2.399282\n","epoch 6 batch 10 loss = 2.304588\n","epoch 6 batch 11 loss = 2.3167894\n","epoch 6 batch 12 loss = 2.4110482\n","epoch 6 batch 13 loss = 2.3488517\n","epoch 6 batch 14 loss = 2.2592108\n","epoch 6 batch 15 loss = 2.4953759\n","epoch 6 batch 16 loss = 2.2723746\n","epoch 6 batch 17 loss = 2.499049\n","epoch 6 batch 18 loss = 2.2214193\n","epoch 6 batch 19 loss = 2.305861\n","epoch 6 batch 20 loss = 2.3623917\n","epoch 6 batch 21 loss = 2.2988598\n","epoch 6 batch 22 loss = 2.2418747\n","epoch 6 batch 23 loss = 2.3100383\n","epoch 6 batch 24 loss = 2.3188806\n","epoch 6 batch 25 loss = 2.2451136\n","epoch 6 batch 26 loss = 2.4279034\n","epoch 6 batch 27 loss = 2.2910874\n","epoch 6 batch 28 loss = 2.415451\n","epoch 6 batch 29 loss = 2.4250271\n","epoch 6 batch 30 loss = 2.2991736\n","epoch 6 batch 31 loss = 1.980788\n","epoch 7 batch 0 loss = 2.2543995\n","epoch 7 batch 1 loss = 2.3401515\n","epoch 7 batch 2 loss = 2.2167318\n","epoch 7 batch 3 loss = 2.4664416\n","epoch 7 batch 4 loss = 2.2910595\n","epoch 7 batch 5 loss = 2.0085073\n","epoch 7 batch 6 loss = 2.256516\n","epoch 7 batch 7 loss = 2.2891955\n","epoch 7 batch 8 loss = 2.346738\n","epoch 7 batch 9 loss = 2.2708173\n","epoch 7 batch 10 loss = 2.3435297\n","epoch 7 batch 11 loss = 2.2755656\n","epoch 7 batch 12 loss = 2.2849925\n","epoch 7 batch 13 loss = 2.2632585\n","epoch 7 batch 14 loss = 2.4935358\n","epoch 7 batch 15 loss = 2.231079\n","epoch 7 batch 16 loss = 2.4304051\n","epoch 7 batch 17 loss = 2.2610621\n","epoch 7 batch 18 loss = 2.2948203\n","epoch 8 batch 0 loss = 2.2308438\n","epoch 8 batch 1 loss = 2.3103702\n","epoch 8 batch 2 loss = 2.223891\n","epoch 8 batch 3 loss = 2.2793438\n","epoch 8 batch 4 loss = 2.2791667\n","epoch 8 batch 5 loss = 2.3412564\n","epoch 8 batch 6 loss = 2.1453543\n","epoch 8 batch 7 loss = 2.3543477\n","epoch 8 batch 8 loss = 2.1843948\n","epoch 8 batch 9 loss = 2.3518536\n","epoch 8 batch 10 loss = 2.3323894\n","epoch 8 batch 11 loss = 2.1993663\n","epoch 8 batch 12 loss = 2.3752396\n","epoch 8 batch 13 loss = 2.3042996\n","epoch 8 batch 14 loss = 2.2367167\n","epoch 8 batch 15 loss = 2.1795986\n","epoch 8 batch 16 loss = 2.1880646\n","epoch 8 batch 17 loss = 2.1211362\n","epoch 8 batch 18 loss = 2.1490536\n","epoch 8 batch 19 loss = 2.256149\n","epoch 8 batch 20 loss = 2.2111173\n","epoch 9 batch 0 loss = 2.3925161\n","epoch 9 batch 1 loss = 2.066106\n","epoch 9 batch 2 loss = 2.092707\n","epoch 9 batch 3 loss = 2.184445\n","epoch 9 batch 4 loss = 2.2403152\n","epoch 9 batch 5 loss = 2.3248954\n","epoch 9 batch 6 loss = 2.2580245\n","epoch 9 batch 7 loss = 2.21245\n","epoch 9 batch 8 loss = 2.2248924\n","epoch 9 batch 9 loss = 2.3075318\n","epoch 9 batch 10 loss = 2.229583\n","epoch 9 batch 11 loss = 2.4211228\n","epoch 9 batch 12 loss = 2.307878\n","epoch 9 batch 13 loss = 2.2019114\n","epoch 9 batch 14 loss = 2.174764\n","epoch 9 batch 15 loss = 2.1297348\n","epoch 9 batch 16 loss = 2.2435253\n","epoch 9 batch 17 loss = 2.266827\n","epoch 9 batch 18 loss = 2.1770813\n","epoch 9 batch 19 loss = 2.081776\n","epoch 9 batch 20 loss = 2.0997465\n","epoch 9 batch 21 loss = 2.0928621\n","epoch 9 batch 22 loss = 2.1765814\n","epoch 9 batch 23 loss = 2.2541325\n","epoch 9 batch 24 loss = 2.2425656\n","epoch 9 batch 25 loss = 2.1388948\n","epoch 9 batch 26 loss = 2.3659892\n","epoch 10 batch 0 loss = 2.0594034\n","epoch 10 batch 1 loss = 2.2653399\n","epoch 10 batch 2 loss = 2.3286898\n","epoch 10 batch 3 loss = 2.2605987\n","epoch 10 batch 4 loss = 2.1326518\n","epoch 10 batch 5 loss = 2.0692208\n","epoch 10 batch 6 loss = 2.027974\n","epoch 10 batch 7 loss = 2.1588707\n","epoch 10 batch 8 loss = 2.2096176\n","epoch 10 batch 9 loss = 2.1725488\n","epoch 10 batch 10 loss = 2.0363946\n","epoch 10 batch 11 loss = 2.2454612\n","epoch 10 batch 12 loss = 2.2008867\n","epoch 10 batch 13 loss = 2.2178652\n","epoch 10 batch 14 loss = 2.2728286\n","epoch 10 batch 15 loss = 2.0156648\n","epoch 10 batch 16 loss = 2.2216175\n","epoch 10 batch 17 loss = 2.196819\n","epoch 10 batch 18 loss = 2.2816167\n","epoch 10 batch 19 loss = 2.2145555\n","epoch 10 batch 20 loss = 2.1382453\n","epoch 11 batch 0 loss = 2.1045194\n","epoch 11 batch 1 loss = 2.1463308\n","epoch 11 batch 2 loss = 2.2417326\n","epoch 11 batch 3 loss = 2.2487063\n","epoch 11 batch 4 loss = 2.011713\n","epoch 11 batch 5 loss = 2.2136762\n","epoch 11 batch 6 loss = 2.0921621\n","epoch 11 batch 7 loss = 2.0375998\n","epoch 11 batch 8 loss = 2.2322772\n","epoch 11 batch 9 loss = 2.2236776\n","epoch 11 batch 10 loss = 2.0905566\n","epoch 11 batch 11 loss = 2.1615288\n","epoch 11 batch 12 loss = 2.2073643\n","epoch 11 batch 13 loss = 2.2805142\n","epoch 11 batch 14 loss = 2.2111235\n","epoch 11 batch 15 loss = 2.079055\n","epoch 11 batch 16 loss = 2.2475471\n","epoch 11 batch 17 loss = 2.1387198\n","epoch 11 batch 18 loss = 2.168013\n","epoch 11 batch 19 loss = 2.184828\n","epoch 11 batch 20 loss = 2.1972415\n","epoch 11 batch 21 loss = 2.03899\n","epoch 11 batch 22 loss = 1.9619273\n","epoch 12 batch 0 loss = 2.0966907\n","epoch 12 batch 1 loss = 2.1589243\n","epoch 12 batch 2 loss = 2.1861947\n","epoch 12 batch 3 loss = 2.1301\n","epoch 12 batch 4 loss = 2.0869694\n","epoch 12 batch 5 loss = 2.0453196\n","epoch 12 batch 6 loss = 2.1126008\n","epoch 12 batch 7 loss = 2.0406024\n","epoch 12 batch 8 loss = 2.172212\n","epoch 12 batch 9 loss = 2.111363\n","epoch 12 batch 10 loss = 1.9432718\n","epoch 12 batch 11 loss = 2.1477711\n","epoch 12 batch 12 loss = 2.1160161\n","epoch 12 batch 13 loss = 2.2435358\n","epoch 12 batch 14 loss = 2.2017848\n","epoch 12 batch 15 loss = 2.1654468\n","epoch 12 batch 16 loss = 2.0366116\n","epoch 12 batch 17 loss = 1.8293512\n","epoch 12 batch 18 loss = 2.199075\n","epoch 13 batch 0 loss = 2.222469\n","epoch 13 batch 1 loss = 2.0989408\n","epoch 13 batch 2 loss = 2.14023\n","epoch 13 batch 3 loss = 2.1745365\n","epoch 13 batch 4 loss = 2.063908\n","epoch 13 batch 5 loss = 2.0427313\n","epoch 13 batch 6 loss = 2.1489973\n","epoch 13 batch 7 loss = 1.9741797\n","epoch 13 batch 8 loss = 2.16365\n","epoch 13 batch 9 loss = 2.1315153\n","epoch 13 batch 10 loss = 2.1650152\n","epoch 13 batch 11 loss = 2.1280158\n","epoch 13 batch 12 loss = 2.118552\n","epoch 13 batch 13 loss = 2.144855\n","epoch 13 batch 14 loss = 2.1113336\n","epoch 13 batch 15 loss = 2.0832946\n","epoch 13 batch 16 loss = 2.0944974\n","epoch 13 batch 17 loss = 2.2049217\n","epoch 13 batch 18 loss = 2.1048353\n","epoch 14 batch 0 loss = 2.0572624\n","epoch 14 batch 1 loss = 2.0572448\n","epoch 14 batch 2 loss = 2.0047386\n","epoch 14 batch 3 loss = 2.1120853\n","epoch 14 batch 4 loss = 2.1118538\n","epoch 14 batch 5 loss = 2.0482678\n","epoch 14 batch 6 loss = 1.9818046\n","epoch 14 batch 7 loss = 2.1017191\n","epoch 14 batch 8 loss = 2.1550581\n","epoch 14 batch 9 loss = 2.2346392\n","epoch 14 batch 10 loss = 2.0704808\n","epoch 14 batch 11 loss = 2.1337223\n","epoch 14 batch 12 loss = 2.1928558\n","epoch 14 batch 13 loss = 2.0308363\n","epoch 14 batch 14 loss = 2.054141\n","epoch 14 batch 15 loss = 1.9825191\n","epoch 14 batch 16 loss = 2.0596275\n","epoch 14 batch 17 loss = 2.1092134\n","epoch 15 batch 0 loss = 1.994334\n","epoch 15 batch 1 loss = 2.1670992\n","epoch 15 batch 2 loss = 1.9440721\n","epoch 15 batch 3 loss = 1.9928052\n","epoch 15 batch 4 loss = 2.1317203\n","epoch 15 batch 5 loss = 2.0081682\n","epoch 15 batch 6 loss = 2.103318\n","epoch 15 batch 7 loss = 2.029073\n","epoch 15 batch 8 loss = 2.123516\n","epoch 15 batch 9 loss = 1.9469303\n","epoch 15 batch 10 loss = 1.9763763\n","epoch 15 batch 11 loss = 2.1033392\n","epoch 15 batch 12 loss = 2.0371785\n","epoch 15 batch 13 loss = 2.0154285\n","epoch 15 batch 14 loss = 2.0810473\n","epoch 15 batch 15 loss = 1.9521319\n","epoch 15 batch 16 loss = 2.1476943\n","epoch 15 batch 17 loss = 2.0956478\n","epoch 15 batch 18 loss = 2.05192\n","epoch 15 batch 19 loss = 2.057483\n","epoch 15 batch 20 loss = 1.9150319\n","epoch 15 batch 21 loss = 2.0351336\n","epoch 15 batch 22 loss = 1.9823797\n","epoch 15 batch 23 loss = 2.1147184\n","epoch 15 batch 24 loss = 2.056958\n","epoch 15 batch 25 loss = 1.9755092\n","epoch 15 batch 26 loss = 1.9480605\n","epoch 16 batch 0 loss = 2.0194576\n","epoch 16 batch 1 loss = 2.0645752\n","epoch 16 batch 2 loss = 2.1047063\n","epoch 16 batch 3 loss = 2.0421534\n","epoch 16 batch 4 loss = 1.9713923\n","epoch 16 batch 5 loss = 1.8700918\n","epoch 16 batch 6 loss = 2.0377986\n","epoch 16 batch 7 loss = 2.0623293\n","epoch 16 batch 8 loss = 2.087497\n","epoch 16 batch 9 loss = 1.9973624\n","epoch 16 batch 10 loss = 1.906508\n","epoch 16 batch 11 loss = 2.0823443\n","epoch 17 batch 0 loss = 2.13512\n","epoch 17 batch 1 loss = 1.9080331\n","epoch 17 batch 2 loss = 2.0328643\n","epoch 17 batch 3 loss = 1.9745324\n","epoch 17 batch 4 loss = 2.1789005\n","epoch 17 batch 5 loss = 1.9485797\n","epoch 17 batch 6 loss = 2.0708451\n","epoch 17 batch 7 loss = 2.1159818\n","epoch 17 batch 8 loss = 2.0475721\n","epoch 17 batch 9 loss = 1.9542851\n","epoch 17 batch 10 loss = 2.1548429\n","epoch 17 batch 11 loss = 1.9866302\n","epoch 17 batch 12 loss = 1.9545835\n","epoch 17 batch 13 loss = 2.1290772\n","epoch 17 batch 14 loss = 2.1012158\n","epoch 17 batch 15 loss = 2.065513\n","epoch 17 batch 16 loss = 2.1172001\n","epoch 17 batch 17 loss = 1.9373268\n","epoch 17 batch 18 loss = 2.106052\n","epoch 17 batch 19 loss = 1.9310437\n","epoch 17 batch 20 loss = 2.1074007\n","epoch 18 batch 0 loss = 2.0084772\n","epoch 18 batch 1 loss = 2.0398147\n","epoch 18 batch 2 loss = 1.8504807\n","epoch 18 batch 3 loss = 2.0541596\n","epoch 18 batch 4 loss = 1.8608127\n","epoch 18 batch 5 loss = 1.9629046\n","epoch 18 batch 6 loss = 1.9430203\n","epoch 18 batch 7 loss = 1.9681832\n","epoch 18 batch 8 loss = 1.9628642\n","epoch 18 batch 9 loss = 1.9273345\n","epoch 18 batch 10 loss = 2.1226315\n","epoch 18 batch 11 loss = 2.036508\n","epoch 18 batch 12 loss = 1.9912659\n","epoch 18 batch 13 loss = 1.8622035\n","epoch 18 batch 14 loss = 2.0884461\n","epoch 18 batch 15 loss = 2.098974\n","epoch 18 batch 16 loss = 1.9001222\n","epoch 19 batch 0 loss = 1.9934367\n","epoch 19 batch 1 loss = 1.965785\n","epoch 19 batch 2 loss = 1.9987029\n","epoch 19 batch 3 loss = 1.8656052\n","epoch 19 batch 4 loss = 2.0639136\n","epoch 19 batch 5 loss = 2.0977998\n","epoch 19 batch 6 loss = 2.2053556\n","epoch 19 batch 7 loss = 1.882762\n","epoch 19 batch 8 loss = 2.0369494\n","epoch 19 batch 9 loss = 2.1034582\n","epoch 19 batch 10 loss = 2.0472476\n","epoch 19 batch 11 loss = 1.9985751\n","epoch 19 batch 12 loss = 2.1257212\n","epoch 19 batch 13 loss = 1.9450132\n","epoch 19 batch 14 loss = 1.9835911\n","epoch 19 batch 15 loss = 1.9437599\n","epoch 19 batch 16 loss = 1.8682756\n","epoch 20 batch 0 loss = 2.036951\n","epoch 20 batch 1 loss = 2.0212264\n","epoch 20 batch 2 loss = 1.7922161\n","epoch 20 batch 3 loss = 1.964835\n","epoch 20 batch 4 loss = 1.9802656\n","epoch 20 batch 5 loss = 2.036562\n","epoch 20 batch 6 loss = 1.9843861\n","epoch 20 batch 7 loss = 1.9670284\n","epoch 20 batch 8 loss = 1.9434514\n","epoch 20 batch 9 loss = 2.0216494\n","epoch 20 batch 10 loss = 1.8597248\n","epoch 20 batch 11 loss = 2.0841973\n","epoch 20 batch 12 loss = 1.962172\n","epoch 20 batch 13 loss = 2.0711296\n","epoch 20 batch 14 loss = 1.9488511\n","epoch 20 batch 15 loss = 1.8386184\n","epoch 20 batch 16 loss = 1.8312665\n","epoch 20 batch 17 loss = 1.9445891\n","epoch 20 batch 18 loss = 2.0146024\n","epoch 20 batch 19 loss = 1.954388\n","epoch 20 batch 20 loss = 1.9740862\n","epoch 20 batch 21 loss = 2.0179422\n","epoch 20 batch 22 loss = 1.7867141\n","epoch 20 batch 23 loss = 1.9076189\n","epoch 21 batch 0 loss = 1.7647874\n","epoch 21 batch 1 loss = 1.9052975\n","epoch 21 batch 2 loss = 1.9926348\n","epoch 21 batch 3 loss = 1.946523\n","epoch 21 batch 4 loss = 1.9783691\n","epoch 21 batch 5 loss = 1.8974315\n","epoch 21 batch 6 loss = 1.9138606\n","epoch 21 batch 7 loss = 1.9889371\n","epoch 21 batch 8 loss = 1.8831245\n","epoch 21 batch 9 loss = 1.8236417\n","epoch 21 batch 10 loss = 1.9618138\n","epoch 21 batch 11 loss = 1.9850872\n","epoch 21 batch 12 loss = 1.9636767\n","epoch 21 batch 13 loss = 1.8304743\n","epoch 21 batch 14 loss = 1.9414041\n","epoch 21 batch 15 loss = 1.925583\n","epoch 21 batch 16 loss = 1.8365655\n","epoch 22 batch 0 loss = 1.9978607\n","epoch 22 batch 1 loss = 1.8670535\n","epoch 22 batch 2 loss = 2.001867\n","epoch 22 batch 3 loss = 2.0809689\n","epoch 22 batch 4 loss = 1.8153052\n","epoch 22 batch 5 loss = 2.0395029\n","epoch 22 batch 6 loss = 1.86484\n","epoch 22 batch 7 loss = 1.816245\n","epoch 22 batch 8 loss = 1.8715339\n","epoch 22 batch 9 loss = 1.8924636\n","epoch 22 batch 10 loss = 1.9383444\n","epoch 22 batch 11 loss = 1.8506889\n","epoch 22 batch 12 loss = 1.986277\n","epoch 22 batch 13 loss = 1.9481683\n","epoch 22 batch 14 loss = 1.8087618\n","epoch 22 batch 15 loss = 1.9218684\n","epoch 22 batch 16 loss = 1.8574704\n","epoch 22 batch 17 loss = 1.9734422\n","epoch 22 batch 18 loss = 1.9527591\n","epoch 22 batch 19 loss = 1.874018\n","epoch 22 batch 20 loss = 1.9431612\n","epoch 22 batch 21 loss = 1.9216988\n","epoch 22 batch 22 loss = 1.9746122\n","epoch 22 batch 23 loss = 1.9255091\n","epoch 22 batch 24 loss = 2.002635\n","epoch 23 batch 0 loss = 1.6962333\n","epoch 23 batch 1 loss = 1.9693731\n","epoch 23 batch 2 loss = 2.0301185\n","epoch 23 batch 3 loss = 1.9205024\n","epoch 23 batch 4 loss = 1.9120471\n","epoch 23 batch 5 loss = 1.9217936\n","epoch 23 batch 6 loss = 1.9199369\n","epoch 23 batch 7 loss = 1.9355737\n","epoch 23 batch 8 loss = 1.9214625\n","epoch 23 batch 9 loss = 1.9431852\n","epoch 23 batch 10 loss = 1.8512516\n","epoch 23 batch 11 loss = 1.9518306\n","epoch 23 batch 12 loss = 1.7161096\n","epoch 23 batch 13 loss = 1.770756\n","epoch 23 batch 14 loss = 2.0372934\n","epoch 23 batch 15 loss = 1.9151392\n","epoch 23 batch 16 loss = 1.9488844\n","epoch 23 batch 17 loss = 1.9488692\n","epoch 23 batch 18 loss = 1.8631942\n","epoch 23 batch 19 loss = 1.9066069\n","epoch 23 batch 20 loss = 1.9417602\n","epoch 23 batch 21 loss = 1.758885\n","epoch 23 batch 22 loss = 1.793311\n","epoch 23 batch 23 loss = 1.9137857\n","epoch 23 batch 24 loss = 1.9013377\n","epoch 23 batch 25 loss = 1.9094882\n","epoch 23 batch 26 loss = 2.0336666\n","epoch 23 batch 27 loss = 1.8859607\n","epoch 23 batch 28 loss = 1.956103\n","epoch 23 batch 29 loss = 1.9187343\n","epoch 24 batch 0 loss = 1.7584285\n","epoch 24 batch 1 loss = 1.951428\n","epoch 24 batch 2 loss = 1.8764021\n","epoch 24 batch 3 loss = 1.9249126\n","epoch 24 batch 4 loss = 1.9186109\n","epoch 24 batch 5 loss = 1.8417931\n","epoch 24 batch 6 loss = 1.8556994\n","epoch 24 batch 7 loss = 1.970548\n","epoch 24 batch 8 loss = 1.9353529\n","epoch 24 batch 9 loss = 1.9046993\n","epoch 24 batch 10 loss = 2.002876\n","epoch 24 batch 11 loss = 1.9096706\n","epoch 24 batch 12 loss = 1.9632941\n","epoch 24 batch 13 loss = 1.8882859\n","epoch 24 batch 14 loss = 1.9780027\n","epoch 24 batch 15 loss = 1.7630744\n","epoch 24 batch 16 loss = 1.8483762\n","epoch 24 batch 17 loss = 1.8072121\n","epoch 24 batch 18 loss = 1.9744358\n","epoch 25 batch 0 loss = 1.7481613\n","epoch 25 batch 1 loss = 1.8094008\n","epoch 25 batch 2 loss = 1.8675303\n","epoch 25 batch 3 loss = 1.8856127\n","epoch 25 batch 4 loss = 1.7941751\n","epoch 25 batch 5 loss = 1.7014413\n","epoch 25 batch 6 loss = 1.836724\n","epoch 25 batch 7 loss = 1.9159987\n","epoch 25 batch 8 loss = 1.9592291\n","epoch 25 batch 9 loss = 1.7993244\n","epoch 25 batch 10 loss = 1.933962\n","epoch 25 batch 11 loss = 1.8394393\n","epoch 25 batch 12 loss = 1.9232647\n","epoch 25 batch 13 loss = 1.7941235\n","epoch 25 batch 14 loss = 1.7977874\n","epoch 25 batch 15 loss = 1.9278865\n","epoch 26 batch 0 loss = 1.8766103\n","epoch 26 batch 1 loss = 1.9292458\n","epoch 26 batch 2 loss = 1.8427047\n","epoch 26 batch 3 loss = 1.7727602\n","epoch 26 batch 4 loss = 2.037051\n","epoch 26 batch 5 loss = 1.892144\n","epoch 26 batch 6 loss = 1.9057903\n","epoch 26 batch 7 loss = 1.8704015\n","epoch 26 batch 8 loss = 1.9874785\n","epoch 26 batch 9 loss = 1.9033753\n","epoch 26 batch 10 loss = 1.9763199\n","epoch 26 batch 11 loss = 1.8698674\n","epoch 26 batch 12 loss = 1.9942404\n","epoch 26 batch 13 loss = 1.8405606\n","epoch 26 batch 14 loss = 1.8660036\n","epoch 26 batch 15 loss = 1.8266702\n","epoch 26 batch 16 loss = 1.7798158\n","epoch 27 batch 0 loss = 1.7732129\n","epoch 27 batch 1 loss = 1.8526063\n","epoch 27 batch 2 loss = 1.9051989\n","epoch 27 batch 3 loss = 1.7818848\n","epoch 27 batch 4 loss = 1.7989136\n","epoch 27 batch 5 loss = 1.7873931\n","epoch 27 batch 6 loss = 1.7235758\n","epoch 27 batch 7 loss = 1.8711702\n","epoch 27 batch 8 loss = 1.9310994\n","epoch 27 batch 9 loss = 1.7369139\n","epoch 27 batch 10 loss = 1.9269643\n","epoch 27 batch 11 loss = 1.8231375\n","epoch 27 batch 12 loss = 1.8359451\n","epoch 27 batch 13 loss = 1.8856552\n","epoch 27 batch 14 loss = 1.9357014\n","epoch 27 batch 15 loss = 1.8613285\n","epoch 27 batch 16 loss = 1.9398713\n","epoch 27 batch 17 loss = 1.8004364\n","epoch 27 batch 18 loss = 1.8851657\n","epoch 28 batch 0 loss = 1.9263\n","epoch 28 batch 1 loss = 1.8698866\n","epoch 28 batch 2 loss = 1.7970916\n","epoch 28 batch 3 loss = 1.9305403\n","epoch 28 batch 4 loss = 1.8380922\n","epoch 28 batch 5 loss = 1.8412656\n","epoch 28 batch 6 loss = 1.7993504\n","epoch 28 batch 7 loss = 1.7971183\n","epoch 28 batch 8 loss = 1.7796875\n","epoch 28 batch 9 loss = 1.8062786\n","epoch 28 batch 10 loss = 1.6737424\n","epoch 28 batch 11 loss = 1.8486592\n","epoch 28 batch 12 loss = 1.6783404\n","epoch 28 batch 13 loss = 1.8589653\n","epoch 28 batch 14 loss = 1.9476519\n","epoch 28 batch 15 loss = 1.7280294\n","epoch 28 batch 16 loss = 1.7437443\n","epoch 28 batch 17 loss = 1.8366013\n","epoch 28 batch 18 loss = 1.8366439\n","epoch 29 batch 0 loss = 1.8200179\n","epoch 29 batch 1 loss = 1.7768584\n","epoch 29 batch 2 loss = 1.8231157\n","epoch 29 batch 3 loss = 1.7308304\n","epoch 29 batch 4 loss = 1.8133987\n","epoch 29 batch 5 loss = 1.7998322\n","epoch 29 batch 6 loss = 1.9351599\n","epoch 29 batch 7 loss = 1.9385524\n","epoch 29 batch 8 loss = 1.9348881\n","epoch 29 batch 9 loss = 1.822191\n","epoch 29 batch 10 loss = 1.7774886\n","epoch 29 batch 11 loss = 1.8741946\n","epoch 29 batch 12 loss = 1.8387975\n","epoch 29 batch 13 loss = 1.897369\n","epoch 29 batch 14 loss = 1.8375237\n","epoch 29 batch 15 loss = 2.0060844\n","epoch 29 batch 16 loss = 1.7912749\n","epoch 29 batch 17 loss = 1.8395013\n","epoch 29 batch 18 loss = 1.6934564\n","epoch 30 batch 0 loss = 1.8707019\n","epoch 30 batch 1 loss = 1.8030998\n","epoch 30 batch 2 loss = 1.8379797\n","epoch 30 batch 3 loss = 1.8953446\n","epoch 30 batch 4 loss = 1.9800258\n","epoch 30 batch 5 loss = 1.7697672\n","epoch 30 batch 6 loss = 1.8356935\n","epoch 30 batch 7 loss = 1.7022032\n","epoch 30 batch 8 loss = 1.8160818\n","epoch 30 batch 9 loss = 1.9758667\n","epoch 30 batch 10 loss = 1.7931948\n","epoch 30 batch 11 loss = 1.836544\n","epoch 30 batch 12 loss = 1.8550744\n","epoch 30 batch 13 loss = 1.8407487\n","epoch 30 batch 14 loss = 1.8624809\n","epoch 30 batch 15 loss = 1.838561\n","epoch 30 batch 16 loss = 1.8982089\n","epoch 31 batch 0 loss = 1.7957416\n","epoch 31 batch 1 loss = 1.8609173\n","epoch 31 batch 2 loss = 1.694536\n","epoch 31 batch 3 loss = 1.9374455\n","epoch 31 batch 4 loss = 1.9532329\n","epoch 31 batch 5 loss = 1.7688216\n","epoch 31 batch 6 loss = 1.7784963\n","epoch 31 batch 7 loss = 1.747169\n","epoch 31 batch 8 loss = 1.7978638\n","epoch 31 batch 9 loss = 1.9191077\n","epoch 31 batch 10 loss = 1.7933228\n","epoch 31 batch 11 loss = 1.7192814\n","epoch 31 batch 12 loss = 1.8608071\n","epoch 31 batch 13 loss = 1.7046238\n","epoch 31 batch 14 loss = 1.8068457\n","epoch 31 batch 15 loss = 1.6024626\n","epoch 31 batch 16 loss = 1.7309839\n","epoch 32 batch 0 loss = 1.8272991\n","epoch 32 batch 1 loss = 1.8559687\n","epoch 32 batch 2 loss = 1.6778682\n","epoch 32 batch 3 loss = 1.8132004\n","epoch 32 batch 4 loss = 1.8259773\n","epoch 32 batch 5 loss = 1.7597841\n","epoch 32 batch 6 loss = 1.8146176\n","epoch 32 batch 7 loss = 1.84654\n","epoch 32 batch 8 loss = 1.7755913\n","epoch 32 batch 9 loss = 1.8384616\n","epoch 32 batch 10 loss = 1.7521731\n","epoch 32 batch 11 loss = 1.7948822\n","epoch 32 batch 12 loss = 1.7410144\n","epoch 32 batch 13 loss = 1.9213287\n","epoch 32 batch 14 loss = 1.8858668\n","epoch 32 batch 15 loss = 1.8724388\n","epoch 33 batch 0 loss = 1.7698487\n","epoch 33 batch 1 loss = 1.7619526\n","epoch 33 batch 2 loss = 1.8857931\n","epoch 33 batch 3 loss = 1.8682055\n","epoch 33 batch 4 loss = 1.7494613\n","epoch 33 batch 5 loss = 1.767452\n","epoch 33 batch 6 loss = 1.7737882\n","epoch 33 batch 7 loss = 1.6815724\n","epoch 33 batch 8 loss = 1.8847197\n","epoch 33 batch 9 loss = 1.8434105\n","epoch 33 batch 10 loss = 1.7622712\n","epoch 33 batch 11 loss = 1.7837989\n","epoch 33 batch 12 loss = 1.7139498\n","epoch 33 batch 13 loss = 1.6791946\n","epoch 33 batch 14 loss = 1.6249318\n","epoch 33 batch 15 loss = 1.8224276\n","epoch 33 batch 16 loss = 1.823383\n","epoch 33 batch 17 loss = 1.8748397\n","epoch 33 batch 18 loss = 1.7918391\n","epoch 33 batch 19 loss = 1.5468054\n","epoch 34 batch 0 loss = 1.754375\n","epoch 34 batch 1 loss = 1.7752358\n","epoch 34 batch 2 loss = 1.8380051\n","epoch 34 batch 3 loss = 1.7584058\n","epoch 34 batch 4 loss = 1.8085213\n","epoch 34 batch 5 loss = 1.6231804\n","epoch 34 batch 6 loss = 1.8617312\n","epoch 34 batch 7 loss = 1.8665714\n","epoch 34 batch 8 loss = 1.7819159\n","epoch 34 batch 9 loss = 1.7588314\n","epoch 34 batch 10 loss = 1.7118663\n","epoch 34 batch 11 loss = 1.7514006\n","epoch 34 batch 12 loss = 1.8568025\n","epoch 34 batch 13 loss = 1.5975937\n","epoch 34 batch 14 loss = 1.8460383\n","epoch 34 batch 15 loss = 1.6443357\n","epoch 34 batch 16 loss = 1.7809159\n","epoch 34 batch 17 loss = 1.8194958\n","epoch 34 batch 18 loss = 1.7636416\n","epoch 34 batch 19 loss = 1.7823974\n","epoch 35 batch 0 loss = 1.80848\n","epoch 35 batch 1 loss = 1.830401\n","epoch 35 batch 2 loss = 1.7610549\n","epoch 35 batch 3 loss = 1.6855888\n","epoch 35 batch 4 loss = 1.8236824\n","epoch 35 batch 5 loss = 1.7426523\n","epoch 35 batch 6 loss = 1.7878786\n","epoch 35 batch 7 loss = 1.6885601\n","epoch 35 batch 8 loss = 1.6992857\n","epoch 35 batch 9 loss = 1.8029444\n","epoch 35 batch 10 loss = 1.7721367\n","epoch 35 batch 11 loss = 1.8806062\n","epoch 35 batch 12 loss = 1.8588724\n","epoch 35 batch 13 loss = 1.8349683\n","epoch 35 batch 14 loss = 1.7449776\n","epoch 35 batch 15 loss = 1.6919616\n","epoch 35 batch 16 loss = 1.7797945\n","epoch 35 batch 17 loss = 1.6726046\n","epoch 36 batch 0 loss = 1.8507792\n","epoch 36 batch 1 loss = 1.8267286\n","epoch 36 batch 2 loss = 1.7204964\n","epoch 36 batch 3 loss = 1.800079\n","epoch 36 batch 4 loss = 1.7466806\n","epoch 36 batch 5 loss = 1.7855232\n","epoch 36 batch 6 loss = 1.7158722\n","epoch 36 batch 7 loss = 1.7844353\n","epoch 36 batch 8 loss = 1.7277097\n","epoch 36 batch 9 loss = 1.7344035\n","epoch 36 batch 10 loss = 1.8034369\n","epoch 36 batch 11 loss = 1.7744689\n","epoch 36 batch 12 loss = 1.796497\n","epoch 36 batch 13 loss = 1.6755831\n","epoch 36 batch 14 loss = 1.8274249\n","epoch 36 batch 15 loss = 1.7813311\n","epoch 36 batch 16 loss = 1.7776641\n","epoch 36 batch 17 loss = 1.7065672\n","epoch 36 batch 18 loss = 1.7752553\n","epoch 36 batch 19 loss = 1.7965388\n","epoch 36 batch 20 loss = 1.7948123\n","epoch 37 batch 0 loss = 1.6244069\n","epoch 37 batch 1 loss = 1.6946571\n","epoch 37 batch 2 loss = 1.7791026\n","epoch 37 batch 3 loss = 1.736743\n","epoch 37 batch 4 loss = 1.6695669\n","epoch 37 batch 5 loss = 1.7811179\n","epoch 37 batch 6 loss = 1.7811285\n","epoch 37 batch 7 loss = 1.723976\n","epoch 37 batch 8 loss = 1.7603271\n","epoch 37 batch 9 loss = 1.811185\n","epoch 37 batch 10 loss = 1.7525887\n","epoch 37 batch 11 loss = 1.8233546\n","epoch 37 batch 12 loss = 1.7734584\n","epoch 37 batch 13 loss = 1.8177743\n","epoch 37 batch 14 loss = 1.8090411\n","epoch 37 batch 15 loss = 1.8352306\n","epoch 37 batch 16 loss = 1.6869638\n","epoch 37 batch 17 loss = 1.7838042\n","epoch 37 batch 18 loss = 1.7212731\n","epoch 37 batch 19 loss = 1.7447547\n","epoch 38 batch 0 loss = 1.7194991\n","epoch 38 batch 1 loss = 1.8354656\n","epoch 38 batch 2 loss = 1.7851176\n","epoch 38 batch 3 loss = 1.830034\n","epoch 38 batch 4 loss = 1.8355368\n","epoch 38 batch 5 loss = 1.6909802\n","epoch 38 batch 6 loss = 1.6360558\n","epoch 38 batch 7 loss = 1.7131183\n","epoch 38 batch 8 loss = 1.698756\n","epoch 38 batch 9 loss = 1.8037541\n","epoch 38 batch 10 loss = 1.7399076\n","epoch 38 batch 11 loss = 1.6898134\n","epoch 38 batch 12 loss = 1.7597142\n","epoch 38 batch 13 loss = 1.6058495\n","epoch 39 batch 0 loss = 1.6555012\n","epoch 39 batch 1 loss = 1.6709275\n","epoch 39 batch 2 loss = 1.6640755\n","epoch 39 batch 3 loss = 1.6802173\n","epoch 39 batch 4 loss = 1.7786868\n","epoch 39 batch 5 loss = 1.6334665\n","epoch 39 batch 6 loss = 1.7401818\n","epoch 39 batch 7 loss = 1.5866413\n","epoch 39 batch 8 loss = 1.6601124\n","epoch 39 batch 9 loss = 1.7343075\n","epoch 39 batch 10 loss = 1.8278908\n","epoch 39 batch 11 loss = 1.8517818\n","epoch 39 batch 12 loss = 1.8322513\n","epoch 39 batch 13 loss = 1.7134025\n","epoch 39 batch 14 loss = 1.8075566\n","epoch 39 batch 15 loss = 1.6981776\n","epoch 39 batch 16 loss = 1.9004462\n","epoch 39 batch 17 loss = 1.7412993\n","epoch 39 batch 18 loss = 1.8217307\n","epoch 39 batch 19 loss = 1.7648776\n","epoch 39 batch 20 loss = 1.640857\n","epoch 39 batch 21 loss = 1.627958\n","epoch 39 batch 22 loss = 1.7423978\n","epoch 39 batch 23 loss = 1.7469763\n","epoch 40 batch 0 loss = 1.6332945\n","epoch 40 batch 1 loss = 1.7502395\n","epoch 40 batch 2 loss = 1.9071443\n","epoch 40 batch 3 loss = 1.6146295\n","epoch 40 batch 4 loss = 1.7150283\n","epoch 40 batch 5 loss = 1.6235822\n","epoch 40 batch 6 loss = 1.5941392\n","epoch 40 batch 7 loss = 1.5386542\n","epoch 40 batch 8 loss = 1.7407495\n","epoch 40 batch 9 loss = 1.8483752\n","epoch 40 batch 10 loss = 1.6192472\n","epoch 40 batch 11 loss = 1.8380488\n","epoch 40 batch 12 loss = 1.7155582\n","epoch 40 batch 13 loss = 1.8706481\n","epoch 40 batch 14 loss = 1.7136887\n","epoch 40 batch 15 loss = 1.6888336\n","epoch 40 batch 16 loss = 1.8761532\n","epoch 40 batch 17 loss = 1.659888\n","epoch 40 batch 18 loss = 1.7759298\n","epoch 40 batch 19 loss = 1.7279956\n","epoch 40 batch 20 loss = 1.8170483\n","epoch 40 batch 21 loss = 1.7886593\n","epoch 40 batch 22 loss = 1.7940603\n","epoch 40 batch 23 loss = 1.8126142\n","epoch 40 batch 24 loss = 1.6420207\n","epoch 40 batch 25 loss = 1.7566264\n","epoch 40 batch 26 loss = 1.7901524\n","epoch 40 batch 27 loss = 1.6179531\n","epoch 40 batch 28 loss = 1.5546455\n","epoch 41 batch 0 loss = 1.721879\n","epoch 41 batch 1 loss = 1.6454724\n","epoch 41 batch 2 loss = 1.7032701\n","epoch 41 batch 3 loss = 1.8139198\n","epoch 41 batch 4 loss = 1.7068911\n","epoch 41 batch 5 loss = 1.6260564\n","epoch 41 batch 6 loss = 1.8077022\n","epoch 41 batch 7 loss = 1.6859063\n","epoch 41 batch 8 loss = 1.5966208\n","epoch 41 batch 9 loss = 1.6521919\n","epoch 41 batch 10 loss = 1.7187289\n","epoch 41 batch 11 loss = 1.7548596\n","epoch 41 batch 12 loss = 1.4618322\n","epoch 41 batch 13 loss = 1.773136\n","epoch 41 batch 14 loss = 1.6918789\n","epoch 41 batch 15 loss = 1.6854724\n","epoch 42 batch 0 loss = 1.7052729\n","epoch 42 batch 1 loss = 1.627845\n","epoch 42 batch 2 loss = 1.7209717\n","epoch 42 batch 3 loss = 1.7684915\n","epoch 42 batch 4 loss = 1.5822835\n","epoch 42 batch 5 loss = 1.6746726\n","epoch 42 batch 6 loss = 1.6074916\n","epoch 42 batch 7 loss = 1.7950648\n","epoch 42 batch 8 loss = 1.8010021\n","epoch 42 batch 9 loss = 1.7430308\n","epoch 42 batch 10 loss = 1.6685933\n","epoch 42 batch 11 loss = 1.6191818\n","epoch 42 batch 12 loss = 1.7432606\n","epoch 42 batch 13 loss = 1.561867\n","epoch 42 batch 14 loss = 1.766472\n","epoch 42 batch 15 loss = 1.6489241\n","epoch 43 batch 0 loss = 1.6999707\n","epoch 43 batch 1 loss = 1.6737708\n","epoch 43 batch 2 loss = 1.7615452\n","epoch 43 batch 3 loss = 1.7925726\n","epoch 43 batch 4 loss = 1.6298603\n","epoch 43 batch 5 loss = 1.6924326\n","epoch 43 batch 6 loss = 1.6590906\n","epoch 43 batch 7 loss = 1.7684492\n","epoch 43 batch 8 loss = 1.7444865\n","epoch 43 batch 9 loss = 1.707259\n","epoch 43 batch 10 loss = 1.7848375\n","epoch 43 batch 11 loss = 1.6634158\n","epoch 43 batch 12 loss = 1.6928102\n","epoch 43 batch 13 loss = 1.685445\n","epoch 43 batch 14 loss = 1.6738547\n","epoch 43 batch 15 loss = 1.7031841\n","epoch 43 batch 16 loss = 1.6948092\n","epoch 44 batch 0 loss = 1.7886498\n","epoch 44 batch 1 loss = 1.7183944\n","epoch 44 batch 2 loss = 1.7005638\n","epoch 44 batch 3 loss = 1.6491393\n","epoch 44 batch 4 loss = 1.8261542\n","epoch 44 batch 5 loss = 1.6971153\n","epoch 44 batch 6 loss = 1.7728436\n","epoch 44 batch 7 loss = 1.6167109\n","epoch 44 batch 8 loss = 1.7620562\n","epoch 44 batch 9 loss = 1.7280241\n","epoch 44 batch 10 loss = 1.580821\n","epoch 44 batch 11 loss = 1.8114591\n","epoch 44 batch 12 loss = 1.7229004\n","epoch 44 batch 13 loss = 1.7350141\n","epoch 44 batch 14 loss = 1.7196131\n","epoch 44 batch 15 loss = 1.6813093\n","epoch 44 batch 16 loss = 1.6613921\n","epoch 44 batch 17 loss = 1.6916903\n","epoch 44 batch 18 loss = 1.639679\n","epoch 44 batch 19 loss = 1.6407983\n","epoch 44 batch 20 loss = 1.7411635\n","epoch 44 batch 21 loss = 1.6603994\n","epoch 44 batch 22 loss = 1.5966358\n","epoch 44 batch 23 loss = 1.7332913\n","epoch 44 batch 24 loss = 1.674089\n","epoch 44 batch 25 loss = 1.6884987\n","epoch 44 batch 26 loss = 1.7116518\n","epoch 44 batch 27 loss = 1.7109513\n","epoch 44 batch 28 loss = 1.8011345\n","epoch 44 batch 29 loss = 1.6958207\n","epoch 45 batch 0 loss = 1.6467985\n","epoch 45 batch 1 loss = 1.7203987\n","epoch 45 batch 2 loss = 1.6220493\n","epoch 45 batch 3 loss = 1.5837901\n","epoch 45 batch 4 loss = 1.7416205\n","epoch 45 batch 5 loss = 1.6583546\n","epoch 45 batch 6 loss = 1.7528793\n","epoch 45 batch 7 loss = 1.7935616\n","epoch 45 batch 8 loss = 1.6795173\n","epoch 45 batch 9 loss = 1.5825843\n","epoch 45 batch 10 loss = 1.6895801\n","epoch 45 batch 11 loss = 1.549283\n","epoch 45 batch 12 loss = 1.6913553\n","epoch 45 batch 13 loss = 1.7064635\n","epoch 45 batch 14 loss = 1.5215291\n","epoch 45 batch 15 loss = 1.6460218\n","epoch 45 batch 16 loss = 1.6802472\n","epoch 45 batch 17 loss = 1.6251839\n","epoch 45 batch 18 loss = 1.7144072\n","epoch 45 batch 19 loss = 1.7881609\n","epoch 45 batch 20 loss = 1.8119556\n","epoch 45 batch 21 loss = 1.7008154\n","epoch 45 batch 22 loss = 1.704508\n","epoch 45 batch 23 loss = 1.5879786\n","epoch 45 batch 24 loss = 1.6508921\n","epoch 45 batch 25 loss = 1.7379416\n","epoch 45 batch 26 loss = 1.6568947\n","epoch 46 batch 0 loss = 1.8131396\n","epoch 46 batch 1 loss = 1.6172779\n","epoch 46 batch 2 loss = 1.6599118\n","epoch 46 batch 3 loss = 1.724367\n","epoch 46 batch 4 loss = 1.8036252\n","epoch 46 batch 5 loss = 1.688513\n","epoch 46 batch 6 loss = 1.67486\n","epoch 46 batch 7 loss = 1.6076735\n","epoch 46 batch 8 loss = 1.749286\n","epoch 46 batch 9 loss = 1.6534315\n","epoch 46 batch 10 loss = 1.5050923\n","epoch 46 batch 11 loss = 1.6657537\n","epoch 46 batch 12 loss = 1.7973418\n","epoch 46 batch 13 loss = 1.6802305\n","epoch 46 batch 14 loss = 1.5347583\n","epoch 46 batch 15 loss = 1.741712\n","epoch 46 batch 16 loss = 1.578563\n","epoch 46 batch 17 loss = 1.6499807\n","epoch 46 batch 18 loss = 1.7043712\n","epoch 46 batch 19 loss = 1.5660098\n","epoch 46 batch 20 loss = 1.6349148\n","epoch 46 batch 21 loss = 1.7066813\n","epoch 46 batch 22 loss = 1.5664383\n","epoch 46 batch 23 loss = 1.6195412\n","epoch 46 batch 24 loss = 1.7308544\n","epoch 46 batch 25 loss = 1.7722112\n","epoch 46 batch 26 loss = 1.7227992\n","epoch 46 batch 27 loss = 1.6753412\n","epoch 46 batch 28 loss = 1.6520352\n","epoch 46 batch 29 loss = 1.6058065\n","epoch 46 batch 30 loss = 1.597278\n","epoch 47 batch 0 loss = 1.7044988\n","epoch 47 batch 1 loss = 1.6800739\n","epoch 47 batch 2 loss = 1.7724537\n","epoch 47 batch 3 loss = 1.753889\n","epoch 47 batch 4 loss = 1.7243727\n","epoch 47 batch 5 loss = 1.7489405\n","epoch 47 batch 6 loss = 1.7202822\n","epoch 47 batch 7 loss = 1.5914474\n","epoch 47 batch 8 loss = 1.6120164\n","epoch 47 batch 9 loss = 1.546944\n","epoch 47 batch 10 loss = 1.7380003\n","epoch 47 batch 11 loss = 1.5898556\n","epoch 47 batch 12 loss = 1.6055491\n","epoch 47 batch 13 loss = 1.5599649\n","epoch 47 batch 14 loss = 1.5501326\n","epoch 47 batch 15 loss = 1.7161696\n","epoch 47 batch 16 loss = 1.5139521\n","epoch 48 batch 0 loss = 1.5946918\n","epoch 48 batch 1 loss = 1.6978027\n","epoch 48 batch 2 loss = 1.6909318\n","epoch 48 batch 3 loss = 1.6240218\n","epoch 48 batch 4 loss = 1.6389241\n","epoch 48 batch 5 loss = 1.665274\n","epoch 48 batch 6 loss = 1.6417603\n","epoch 48 batch 7 loss = 1.7360249\n","epoch 48 batch 8 loss = 1.7708975\n","epoch 48 batch 9 loss = 1.704488\n","epoch 48 batch 10 loss = 1.7129112\n","epoch 48 batch 11 loss = 1.498376\n","epoch 48 batch 12 loss = 1.586555\n","epoch 48 batch 13 loss = 1.8103738\n","epoch 48 batch 14 loss = 1.7382337\n","epoch 48 batch 15 loss = 1.6395946\n","epoch 48 batch 16 loss = 1.666736\n","epoch 48 batch 17 loss = 1.6724306\n","epoch 48 batch 18 loss = 1.7280725\n","epoch 48 batch 19 loss = 1.6171916\n","epoch 48 batch 20 loss = 1.5401514\n","epoch 49 batch 0 loss = 1.6826059\n","epoch 49 batch 1 loss = 1.6890312\n","epoch 49 batch 2 loss = 1.7151381\n","epoch 49 batch 3 loss = 1.7230245\n","epoch 49 batch 4 loss = 1.600059\n","epoch 49 batch 5 loss = 1.6158526\n","epoch 49 batch 6 loss = 1.7176746\n","epoch 49 batch 7 loss = 1.7950063\n","epoch 49 batch 8 loss = 1.5449501\n","epoch 49 batch 9 loss = 1.5898311\n","epoch 49 batch 10 loss = 1.5847802\n","epoch 49 batch 11 loss = 1.6623642\n","epoch 49 batch 12 loss = 1.7419026\n","epoch 49 batch 13 loss = 1.7398477\n","epoch 49 batch 14 loss = 1.6720628\n","epoch 49 batch 15 loss = 1.5474397\n","epoch 49 batch 16 loss = 1.5929011\n","epoch 49 batch 17 loss = 1.5997471\n","epoch 0 batch 0 loss = 17.475073\n","epoch 1 batch 0 loss = 16.665895\n","epoch 2 batch 0 loss = 15.59456\n","epoch 3 batch 0 loss = 15.128332\n","epoch 4 batch 0 loss = 15.189757\n","epoch 5 batch 0 loss = 15.003686\n","epoch 6 batch 0 loss = 15.101829\n","epoch 7 batch 0 loss = 14.920872\n","epoch 8 batch 0 loss = 15.046915\n","epoch 9 batch 0 loss = 14.863189\n","epoch 10 batch 0 loss = 15.00381\n","epoch 11 batch 0 loss = 14.821204\n","epoch 12 batch 0 loss = 14.972519\n","epoch 13 batch 0 loss = 14.78397\n","epoch 14 batch 0 loss = 14.946587\n","epoch 15 batch 0 loss = 14.752305\n","epoch 16 batch 0 loss = 14.924442\n","epoch 17 batch 0 loss = 14.72424\n","epoch 18 batch 0 loss = 14.9048395\n","epoch 19 batch 0 loss = 14.698929\n","epoch 20 batch 0 loss = 14.887303\n","epoch 21 batch 0 loss = 14.675651\n","epoch 22 batch 0 loss = 14.871188\n","epoch 23 batch 0 loss = 14.654142\n","epoch 24 batch 0 loss = 14.856422\n","epoch 25 batch 0 loss = 14.634009\n","epoch 26 batch 0 loss = 14.84226\n","epoch 27 batch 0 loss = 14.615272\n","epoch 28 batch 0 loss = 14.829142\n","epoch 29 batch 0 loss = 14.597191\n","epoch 30 batch 0 loss = 14.816405\n","epoch 31 batch 0 loss = 14.580012\n","epoch 32 batch 0 loss = 14.804227\n","epoch 33 batch 0 loss = 14.563549\n","epoch 34 batch 0 loss = 14.792492\n","epoch 35 batch 0 loss = 14.547653\n","epoch 36 batch 0 loss = 14.780815\n","epoch 37 batch 0 loss = 14.53256\n","epoch 38 batch 0 loss = 14.769731\n","epoch 39 batch 0 loss = 14.517559\n","epoch 40 batch 0 loss = 14.758838\n","epoch 41 batch 0 loss = 14.502951\n","epoch 42 batch 0 loss = 14.748014\n","epoch 43 batch 0 loss = 14.488808\n","epoch 44 batch 0 loss = 14.736872\n","epoch 45 batch 0 loss = 14.475278\n","epoch 46 batch 0 loss = 14.726343\n","epoch 47 batch 0 loss = 14.461515\n","epoch 48 batch 0 loss = 14.7159\n","epoch 49 batch 0 loss = 14.44799\n","epoch 0 batch 0 loss = 18.309858\n","epoch 1 batch 0 loss = 18.14614\n","epoch 2 batch 0 loss = 17.407291\n","epoch 3 batch 0 loss = 16.44537\n","epoch 4 batch 0 loss = 15.74956\n","epoch 5 batch 0 loss = 15.269208\n","epoch 6 batch 0 loss = 15.046332\n","epoch 7 batch 0 loss = 15.023541\n","epoch 8 batch 0 loss = 15.006457\n","epoch 9 batch 0 loss = 14.925965\n","epoch 10 batch 0 loss = 14.902565\n","epoch 11 batch 0 loss = 14.862973\n","epoch 12 batch 0 loss = 14.825052\n","epoch 13 batch 0 loss = 14.815562\n","epoch 14 batch 0 loss = 14.76597\n","epoch 15 batch 0 loss = 14.778206\n","epoch 16 batch 0 loss = 14.716939\n","epoch 17 batch 0 loss = 14.633908\n","epoch 17 batch 1 loss = 14.674319\n","epoch 18 batch 0 loss = 14.718791\n","epoch 19 batch 0 loss = 14.636592\n","epoch 20 batch 0 loss = 14.6926155\n","epoch 21 batch 0 loss = 14.601518\n","epoch 22 batch 0 loss = 14.667654\n","epoch 23 batch 0 loss = 14.567545\n","epoch 24 batch 0 loss = 14.642915\n","epoch 25 batch 0 loss = 14.534335\n","epoch 26 batch 0 loss = 14.613227\n","epoch 27 batch 0 loss = 14.504143\n","epoch 28 batch 0 loss = 14.585105\n","epoch 29 batch 0 loss = 14.466641\n","epoch 30 batch 0 loss = 14.552239\n","epoch 31 batch 0 loss = 14.4269085\n","epoch 32 batch 0 loss = 14.515545\n","epoch 33 batch 0 loss = 14.3794155\n","epoch 34 batch 0 loss = 14.470205\n","epoch 35 batch 0 loss = 14.324041\n","epoch 36 batch 0 loss = 14.41562\n","epoch 37 batch 0 loss = 14.259745\n","epoch 38 batch 0 loss = 14.351783\n","epoch 39 batch 0 loss = 14.188315\n","epoch 40 batch 0 loss = 14.279979\n","epoch 41 batch 0 loss = 14.112459\n","epoch 42 batch 0 loss = 14.205944\n","epoch 43 batch 0 loss = 14.035407\n","epoch 44 batch 0 loss = 14.131518\n","epoch 45 batch 0 loss = 13.959833\n","epoch 46 batch 0 loss = 14.060034\n","epoch 47 batch 0 loss = 13.89018\n","epoch 48 batch 0 loss = 13.997838\n","epoch 49 batch 0 loss = 13.829085\n","epoch 0 batch 0 loss = 18.133678\n","epoch 1 batch 0 loss = 18.092133\n","epoch 2 batch 0 loss = 17.984674\n","epoch 2 batch 1 loss = 18.02592\n","epoch 3 batch 0 loss = 17.993614\n","epoch 4 batch 0 loss = 17.960249\n","epoch 5 batch 0 loss = 17.92585\n","epoch 6 batch 0 loss = 17.8906\n","epoch 7 batch 0 loss = 17.85445\n","epoch 8 batch 0 loss = 17.81717\n","epoch 9 batch 0 loss = 17.779066\n","epoch 10 batch 0 loss = 17.74025\n","epoch 11 batch 0 loss = 17.700893\n","epoch 12 batch 0 loss = 17.660936\n","epoch 13 batch 0 loss = 17.620415\n","epoch 14 batch 0 loss = 17.579296\n","epoch 15 batch 0 loss = 17.733622\n","epoch 15 batch 1 loss = 17.495113\n","epoch 16 batch 0 loss = 17.452703\n","epoch 17 batch 0 loss = 17.409843\n","epoch 18 batch 0 loss = 17.366823\n","epoch 19 batch 0 loss = 17.323706\n","epoch 20 batch 0 loss = 17.280573\n","epoch 21 batch 0 loss = 17.18059\n","epoch 21 batch 1 loss = 17.194225\n","epoch 22 batch 0 loss = 17.150955\n","epoch 23 batch 0 loss = 17.107674\n","epoch 24 batch 0 loss = 17.06451\n","epoch 25 batch 0 loss = 17.021446\n","epoch 26 batch 0 loss = 16.978521\n","epoch 27 batch 0 loss = 16.935862\n","epoch 28 batch 0 loss = 16.893454\n","epoch 29 batch 0 loss = 16.851337\n","epoch 30 batch 0 loss = 16.809473\n","epoch 31 batch 0 loss = 16.76793\n","epoch 32 batch 0 loss = 16.726759\n","epoch 33 batch 0 loss = 16.685898\n","epoch 34 batch 0 loss = 16.6454\n","epoch 35 batch 0 loss = 16.605326\n","epoch 36 batch 0 loss = 16.565672\n","epoch 37 batch 0 loss = 16.52644\n","epoch 38 batch 0 loss = 16.487656\n","epoch 39 batch 0 loss = 16.44928\n","epoch 40 batch 0 loss = 16.411276\n","epoch 41 batch 0 loss = 16.373753\n","epoch 42 batch 0 loss = 16.336666\n","epoch 43 batch 0 loss = 16.30005\n","epoch 44 batch 0 loss = 16.263882\n","epoch 45 batch 0 loss = 16.228165\n","epoch 46 batch 0 loss = 16.192934\n","epoch 47 batch 0 loss = 16.15814\n","epoch 48 batch 0 loss = 16.123821\n","epoch 49 batch 0 loss = 16.090006\n","epoch 0 batch 0 loss = 18.28496\n","epoch 1 batch 0 loss = 18.105698\n","epoch 2 batch 0 loss = 16.905807\n","epoch 3 batch 0 loss = 15.836406\n","epoch 4 batch 0 loss = 15.201931\n","epoch 5 batch 0 loss = 14.899532\n","epoch 6 batch 0 loss = 14.773625\n","epoch 7 batch 0 loss = 14.756752\n","epoch 8 batch 0 loss = 15.129542\n","epoch 9 batch 0 loss = 14.703495\n","epoch 10 batch 0 loss = 15.023598\n","epoch 11 batch 0 loss = 14.677003\n","epoch 12 batch 0 loss = 14.972741\n","epoch 13 batch 0 loss = 14.642812\n","epoch 14 batch 0 loss = 14.931509\n","epoch 15 batch 0 loss = 14.613712\n","epoch 16 batch 0 loss = 14.897424\n","epoch 17 batch 0 loss = 14.58693\n","epoch 18 batch 0 loss = 14.867133\n","epoch 19 batch 0 loss = 14.562795\n","epoch 20 batch 0 loss = 14.836936\n","epoch 21 batch 0 loss = 14.542867\n","epoch 22 batch 0 loss = 14.81196\n","epoch 23 batch 0 loss = 14.520821\n","epoch 24 batch 0 loss = 14.787805\n","epoch 25 batch 0 loss = 14.499859\n","epoch 26 batch 0 loss = 14.756559\n","epoch 27 batch 0 loss = 14.485249\n","epoch 28 batch 0 loss = 14.73501\n","epoch 29 batch 0 loss = 14.464565\n","epoch 30 batch 0 loss = 14.713777\n","epoch 31 batch 0 loss = 14.443998\n","epoch 32 batch 0 loss = 14.69262\n","epoch 33 batch 0 loss = 14.424172\n","epoch 34 batch 0 loss = 14.660411\n","epoch 35 batch 0 loss = 14.410351\n","epoch 36 batch 0 loss = 14.638846\n","epoch 37 batch 0 loss = 14.388228\n","epoch 38 batch 0 loss = 14.615609\n","epoch 39 batch 0 loss = 14.365979\n","epoch 40 batch 0 loss = 14.587168\n","epoch 41 batch 0 loss = 14.344842\n","epoch 42 batch 0 loss = 14.561097\n","epoch 43 batch 0 loss = 14.318878\n","epoch 44 batch 0 loss = 14.5190735\n","epoch 45 batch 0 loss = 14.301926\n","epoch 46 batch 0 loss = 14.486427\n","epoch 47 batch 0 loss = 14.269256\n","epoch 48 batch 0 loss = 14.4493685\n","epoch 49 batch 0 loss = 14.231956\n","epoch 0 batch 0 loss = 18.478287\n","epoch 1 batch 0 loss = 18.426672\n","epoch 2 batch 0 loss = 18.207384\n","epoch 3 batch 0 loss = 17.14935\n","epoch 4 batch 0 loss = 16.050198\n","epoch 5 batch 0 loss = 15.433053\n","epoch 6 batch 0 loss = 15.163191\n","epoch 7 batch 0 loss = 15.333974\n","epoch 8 batch 0 loss = 15.041397\n","epoch 9 batch 0 loss = 15.111631\n","epoch 10 batch 0 loss = 14.982821\n","epoch 11 batch 0 loss = 15.012279\n","epoch 12 batch 0 loss = 14.933835\n","epoch 13 batch 0 loss = 14.941968\n","epoch 14 batch 0 loss = 14.898593\n","epoch 15 batch 0 loss = 14.890598\n","epoch 16 batch 0 loss = 14.866924\n","epoch 17 batch 0 loss = 14.848925\n","epoch 18 batch 0 loss = 14.843469\n","epoch 19 batch 0 loss = 14.8124485\n","epoch 20 batch 0 loss = 14.822612\n","epoch 21 batch 0 loss = 14.780196\n","epoch 22 batch 0 loss = 14.805188\n","epoch 23 batch 0 loss = 14.746448\n","epoch 24 batch 0 loss = 14.793132\n","epoch 25 batch 0 loss = 14.720307\n","epoch 26 batch 0 loss = 14.776576\n","epoch 27 batch 0 loss = 14.697227\n","epoch 28 batch 0 loss = 14.760827\n","epoch 29 batch 0 loss = 14.67302\n","epoch 30 batch 0 loss = 14.7481\n","epoch 31 batch 0 loss = 14.651187\n","epoch 32 batch 0 loss = 14.735117\n","epoch 33 batch 0 loss = 14.630075\n","epoch 34 batch 0 loss = 14.724715\n","epoch 35 batch 0 loss = 14.606974\n","epoch 36 batch 0 loss = 14.7097225\n","epoch 37 batch 0 loss = 14.582472\n","epoch 38 batch 0 loss = 14.706281\n","epoch 39 batch 0 loss = 14.561396\n","epoch 40 batch 0 loss = 14.6919155\n","epoch 41 batch 0 loss = 14.531411\n","epoch 42 batch 0 loss = 14.676101\n","epoch 43 batch 0 loss = 14.509251\n","epoch 44 batch 0 loss = 14.667136\n","epoch 45 batch 0 loss = 14.491904\n","epoch 46 batch 0 loss = 14.655135\n","epoch 47 batch 0 loss = 14.476022\n","epoch 48 batch 0 loss = 14.643384\n","epoch 49 batch 0 loss = 14.457772\n","epoch 0 batch 0 loss = 18.121456\n","epoch 1 batch 0 loss = 17.461681\n","epoch 2 batch 0 loss = 16.297224\n","epoch 3 batch 0 loss = 15.594433\n","epoch 4 batch 0 loss = 15.167067\n","epoch 5 batch 0 loss = 15.083847\n","epoch 6 batch 0 loss = 15.169727\n","epoch 7 batch 0 loss = 14.974155\n","epoch 8 batch 0 loss = 15.079117\n","epoch 9 batch 0 loss = 14.898448\n","epoch 10 batch 0 loss = 15.012074\n","epoch 11 batch 0 loss = 14.815192\n","epoch 12 batch 0 loss = 14.941107\n","epoch 13 batch 0 loss = 14.760504\n","epoch 14 batch 0 loss = 14.893125\n","epoch 15 batch 0 loss = 14.664213\n","epoch 15 batch 1 loss = 14.852757\n","epoch 16 batch 0 loss = 14.676491\n","epoch 17 batch 0 loss = 14.813015\n","epoch 18 batch 0 loss = 14.64008\n","epoch 19 batch 0 loss = 14.777572\n","epoch 20 batch 0 loss = 14.607341\n","epoch 21 batch 0 loss = 14.744082\n","epoch 22 batch 0 loss = 14.572732\n","epoch 23 batch 0 loss = 14.699754\n","epoch 24 batch 0 loss = 14.548623\n","epoch 25 batch 0 loss = 14.664455\n","epoch 26 batch 0 loss = 14.513122\n","epoch 27 batch 0 loss = 14.62828\n","epoch 28 batch 0 loss = 14.47134\n","epoch 29 batch 0 loss = 14.5855465\n","epoch 30 batch 0 loss = 14.4253845\n","epoch 31 batch 0 loss = 14.535792\n","epoch 32 batch 0 loss = 14.374239\n","epoch 33 batch 0 loss = 14.479426\n","epoch 34 batch 0 loss = 14.313565\n","epoch 35 batch 0 loss = 14.413779\n","epoch 36 batch 0 loss = 14.244348\n","epoch 37 batch 0 loss = 14.338054\n","epoch 38 batch 0 loss = 14.1648445\n","epoch 39 batch 0 loss = 14.252294\n","epoch 40 batch 0 loss = 14.079797\n","epoch 41 batch 0 loss = 14.164984\n","epoch 42 batch 0 loss = 13.9950905\n","epoch 43 batch 0 loss = 14.081198\n","epoch 44 batch 0 loss = 13.916207\n","epoch 45 batch 0 loss = 14.006603\n","epoch 46 batch 0 loss = 13.845409\n","epoch 47 batch 0 loss = 13.942437\n","epoch 48 batch 0 loss = 13.784457\n","epoch 49 batch 0 loss = 13.893425\n","epoch 0 batch 0 loss = 18.344034\n","epoch 1 batch 0 loss = 18.277681\n","epoch 2 batch 0 loss = 18.176405\n","epoch 3 batch 0 loss = 17.938793\n","epoch 4 batch 0 loss = 17.296825\n","epoch 5 batch 0 loss = 16.378859\n","epoch 6 batch 0 loss = 15.653097\n","epoch 7 batch 0 loss = 15.242062\n","epoch 8 batch 0 loss = 15.024366\n","epoch 9 batch 0 loss = 14.908178\n","epoch 10 batch 0 loss = 14.878935\n","epoch 11 batch 0 loss = 14.890502\n","epoch 12 batch 0 loss = 14.884024\n","epoch 13 batch 0 loss = 14.818926\n","epoch 14 batch 0 loss = 14.832423\n","epoch 15 batch 0 loss = 14.758824\n","epoch 16 batch 0 loss = 14.791675\n","epoch 17 batch 0 loss = 14.710484\n","epoch 18 batch 0 loss = 14.756188\n","epoch 19 batch 0 loss = 14.668183\n","epoch 20 batch 0 loss = 14.724217\n","epoch 21 batch 0 loss = 14.628024\n","epoch 22 batch 0 loss = 14.692689\n","epoch 23 batch 0 loss = 14.590174\n","epoch 24 batch 0 loss = 14.662254\n","epoch 25 batch 0 loss = 14.552248\n","epoch 26 batch 0 loss = 14.631417\n","epoch 27 batch 0 loss = 14.514169\n","epoch 28 batch 0 loss = 14.599246\n","epoch 29 batch 0 loss = 14.47521\n","epoch 30 batch 0 loss = 14.564981\n","epoch 31 batch 0 loss = 14.433524\n","epoch 32 batch 0 loss = 14.5245905\n","epoch 33 batch 0 loss = 14.387517\n","epoch 34 batch 0 loss = 14.4775715\n","epoch 35 batch 0 loss = 14.331592\n","epoch 36 batch 0 loss = 14.418349\n","epoch 37 batch 0 loss = 14.264146\n","epoch 38 batch 0 loss = 14.343138\n","epoch 39 batch 0 loss = 14.178119\n","epoch 40 batch 0 loss = 14.247201\n","epoch 41 batch 0 loss = 14.0774555\n","epoch 42 batch 0 loss = 14.137063\n","epoch 43 batch 0 loss = 13.964173\n","epoch 44 batch 0 loss = 14.018865\n","epoch 45 batch 0 loss = 13.848254\n","epoch 46 batch 0 loss = 13.909154\n","epoch 47 batch 0 loss = 13.74738\n","epoch 48 batch 0 loss = 13.824242\n","epoch 49 batch 0 loss = 13.673009\n","epoch 0 batch 0 loss = 18.508814\n","epoch 1 batch 0 loss = 18.434793\n","epoch 2 batch 0 loss = 18.258646\n","epoch 3 batch 0 loss = 17.667355\n","epoch 4 batch 0 loss = 16.749634\n","epoch 5 batch 0 loss = 15.9196825\n","epoch 6 batch 0 loss = 15.415063\n","epoch 7 batch 0 loss = 15.122579\n","epoch 8 batch 0 loss = 14.984979\n","epoch 9 batch 0 loss = 14.98593\n","epoch 10 batch 0 loss = 14.804295\n","epoch 11 batch 0 loss = 14.827054\n","epoch 12 batch 0 loss = 14.660706\n","epoch 13 batch 0 loss = 14.686539\n","epoch 14 batch 0 loss = 14.520651\n","epoch 15 batch 0 loss = 14.547438\n","epoch 16 batch 0 loss = 14.364384\n","epoch 17 batch 0 loss = 14.384747\n","epoch 18 batch 0 loss = 14.203936\n","epoch 19 batch 0 loss = 14.227619\n","epoch 20 batch 0 loss = 14.0614\n","epoch 21 batch 0 loss = 14.107329\n","epoch 22 batch 0 loss = 13.953999\n","epoch 23 batch 0 loss = 14.023125\n","epoch 24 batch 0 loss = 13.87782\n","epoch 25 batch 0 loss = 13.959328\n","epoch 26 batch 0 loss = 13.8320675\n","epoch 27 batch 0 loss = 13.920878\n","epoch 28 batch 0 loss = 13.792212\n","epoch 29 batch 0 loss = 13.89353\n","epoch 30 batch 0 loss = 13.76067\n","epoch 31 batch 0 loss = 13.872453\n","epoch 32 batch 0 loss = 13.73439\n","epoch 33 batch 0 loss = 13.855566\n","epoch 34 batch 0 loss = 13.711737\n","epoch 35 batch 0 loss = 13.841501\n","epoch 36 batch 0 loss = 13.691429\n","epoch 37 batch 0 loss = 13.829248\n","epoch 38 batch 0 loss = 13.672754\n","epoch 39 batch 0 loss = 13.818121\n","epoch 40 batch 0 loss = 13.655445\n","epoch 41 batch 0 loss = 13.804371\n","epoch 42 batch 0 loss = 13.642322\n","epoch 43 batch 0 loss = 13.795015\n","epoch 44 batch 0 loss = 13.626447\n","epoch 45 batch 0 loss = 13.786143\n","epoch 46 batch 0 loss = 13.611094\n","epoch 47 batch 0 loss = 13.777601\n","epoch 48 batch 0 loss = 13.596277\n","epoch 49 batch 0 loss = 13.769222\n","epoch 0 batch 0 loss = 18.21726\n","epoch 1 batch 0 loss = 18.086452\n","epoch 2 batch 0 loss = 17.548693\n","epoch 3 batch 0 loss = 16.67175\n","epoch 4 batch 0 loss = 15.766636\n","epoch 4 batch 1 loss = 15.306883\n","epoch 5 batch 0 loss = 14.980539\n","epoch 6 batch 0 loss = 14.786711\n","epoch 7 batch 0 loss = 14.672094\n","epoch 8 batch 0 loss = 14.617994\n","epoch 9 batch 0 loss = 14.581628\n","epoch 10 batch 0 loss = 14.410996\n","epoch 11 batch 0 loss = 14.392322\n","epoch 12 batch 0 loss = 14.225701\n","epoch 13 batch 0 loss = 14.232284\n","epoch 14 batch 0 loss = 14.076188\n","epoch 15 batch 0 loss = 14.109904\n","epoch 16 batch 0 loss = 13.96805\n","epoch 17 batch 0 loss = 14.026897\n","epoch 18 batch 0 loss = 13.891832\n","epoch 19 batch 0 loss = 13.96751\n","epoch 20 batch 0 loss = 13.833885\n","epoch 21 batch 0 loss = 13.923719\n","epoch 22 batch 0 loss = 13.789488\n","epoch 23 batch 0 loss = 13.889355\n","epoch 24 batch 0 loss = 13.753238\n","epoch 25 batch 0 loss = 13.862649\n","epoch 26 batch 0 loss = 13.723408\n","epoch 27 batch 0 loss = 13.841794\n","epoch 28 batch 0 loss = 13.6986\n","epoch 29 batch 0 loss = 13.81517\n","epoch 30 batch 0 loss = 13.684972\n","epoch 31 batch 0 loss = 13.800211\n","epoch 32 batch 0 loss = 13.664065\n","epoch 33 batch 0 loss = 13.786948\n","epoch 34 batch 0 loss = 13.645679\n","epoch 35 batch 0 loss = 13.775378\n","epoch 36 batch 0 loss = 13.627886\n","epoch 37 batch 0 loss = 13.764798\n","epoch 38 batch 0 loss = 13.611365\n","epoch 39 batch 0 loss = 13.7548895\n","epoch 40 batch 0 loss = 13.595344\n","epoch 41 batch 0 loss = 13.745484\n","epoch 42 batch 0 loss = 13.580219\n","epoch 43 batch 0 loss = 13.735954\n","epoch 44 batch 0 loss = 13.566138\n","epoch 45 batch 0 loss = 13.727304\n","epoch 46 batch 0 loss = 13.551904\n","epoch 47 batch 0 loss = 13.718687\n","epoch 48 batch 0 loss = 13.5385065\n","epoch 49 batch 0 loss = 13.710447\n","epoch 0 batch 0 loss = 18.770664\n","epoch 1 batch 0 loss = 18.681135\n","epoch 2 batch 0 loss = 18.656754\n","epoch 3 batch 0 loss = 18.633255\n","epoch 4 batch 0 loss = 18.60921\n","epoch 5 batch 0 loss = 18.584959\n","epoch 6 batch 0 loss = 18.560352\n","epoch 7 batch 0 loss = 18.535248\n","epoch 8 batch 0 loss = 18.509699\n","epoch 9 batch 0 loss = 18.483744\n","epoch 10 batch 0 loss = 18.493366\n","epoch 10 batch 1 loss = 18.43083\n","epoch 11 batch 0 loss = 18.404003\n","epoch 12 batch 0 loss = 18.376919\n","epoch 13 batch 0 loss = 18.34958\n","epoch 14 batch 0 loss = 18.321957\n","epoch 15 batch 0 loss = 18.294033\n","epoch 16 batch 0 loss = 18.265865\n","epoch 17 batch 0 loss = 18.237465\n","epoch 18 batch 0 loss = 18.20878\n","epoch 19 batch 0 loss = 18.179886\n","epoch 20 batch 0 loss = 18.150805\n","epoch 21 batch 0 loss = 18.121588\n","epoch 22 batch 0 loss = 18.092194\n","epoch 23 batch 0 loss = 18.062742\n","epoch 24 batch 0 loss = 18.033228\n","epoch 25 batch 0 loss = 18.003647\n","epoch 26 batch 0 loss = 17.97407\n","epoch 27 batch 0 loss = 17.944477\n","epoch 28 batch 0 loss = 17.914875\n","epoch 29 batch 0 loss = 17.885258\n","epoch 30 batch 0 loss = 17.855623\n","epoch 31 batch 0 loss = 17.825941\n","epoch 32 batch 0 loss = 17.796274\n","epoch 33 batch 0 loss = 17.766636\n","epoch 34 batch 0 loss = 17.737055\n","epoch 35 batch 0 loss = 17.707521\n","epoch 36 batch 0 loss = 17.678019\n","epoch 37 batch 0 loss = 17.64857\n","epoch 38 batch 0 loss = 17.619173\n","epoch 39 batch 0 loss = 17.589832\n","epoch 40 batch 0 loss = 17.491861\n","epoch 40 batch 1 loss = 17.53156\n","epoch 41 batch 0 loss = 17.502415\n","epoch 42 batch 0 loss = 17.473309\n","epoch 43 batch 0 loss = 17.44425\n","epoch 44 batch 0 loss = 17.415264\n","epoch 45 batch 0 loss = 17.386375\n","epoch 46 batch 0 loss = 17.357594\n","epoch 47 batch 0 loss = 17.328949\n","epoch 48 batch 0 loss = 17.300417\n","epoch 49 batch 0 loss = 17.272\n","epoch 0 batch 0 loss = 18.1682\n","epoch 1 batch 0 loss = 17.186556\n","epoch 2 batch 0 loss = 15.96404\n","epoch 3 batch 0 loss = 15.298461\n","epoch 4 batch 0 loss = 15.070329\n","epoch 5 batch 0 loss = 15.1751585\n","epoch 6 batch 0 loss = 14.955536\n","epoch 7 batch 0 loss = 15.035445\n","epoch 8 batch 0 loss = 14.891795\n","epoch 9 batch 0 loss = 14.952138\n","epoch 10 batch 0 loss = 14.843988\n","epoch 11 batch 0 loss = 14.884256\n","epoch 12 batch 0 loss = 14.804706\n","epoch 13 batch 0 loss = 14.832063\n","epoch 14 batch 0 loss = 14.773563\n","epoch 15 batch 0 loss = 14.78489\n","epoch 16 batch 0 loss = 14.746387\n","epoch 17 batch 0 loss = 14.74293\n","epoch 18 batch 0 loss = 14.719476\n","epoch 19 batch 0 loss = 14.707918\n","epoch 20 batch 0 loss = 14.696957\n","epoch 21 batch 0 loss = 14.672834\n","epoch 22 batch 0 loss = 14.675426\n","epoch 23 batch 0 loss = 14.641068\n","epoch 24 batch 0 loss = 14.646979\n","epoch 25 batch 0 loss = 14.621592\n","epoch 26 batch 0 loss = 14.628622\n","epoch 27 batch 0 loss = 14.591585\n","epoch 28 batch 0 loss = 14.610706\n","epoch 29 batch 0 loss = 14.562309\n","epoch 30 batch 0 loss = 14.591286\n","epoch 31 batch 0 loss = 14.535946\n","epoch 32 batch 0 loss = 14.573862\n","epoch 33 batch 0 loss = 14.509549\n","epoch 34 batch 0 loss = 14.556937\n","epoch 35 batch 0 loss = 14.484104\n","epoch 36 batch 0 loss = 14.539908\n","epoch 37 batch 0 loss = 14.459506\n","epoch 38 batch 0 loss = 14.522979\n","epoch 39 batch 0 loss = 14.435643\n","epoch 40 batch 0 loss = 14.506974\n","epoch 41 batch 0 loss = 14.410203\n","epoch 42 batch 0 loss = 14.490293\n","epoch 43 batch 0 loss = 14.385718\n","epoch 44 batch 0 loss = 14.472893\n","epoch 45 batch 0 loss = 14.362116\n","epoch 46 batch 0 loss = 14.453951\n","epoch 47 batch 0 loss = 14.341822\n","epoch 48 batch 0 loss = 14.437745\n","epoch 49 batch 0 loss = 14.319001\n","epoch 0 batch 0 loss = 18.746023\n","epoch 1 batch 0 loss = 18.665365\n","epoch 2 batch 0 loss = 18.385548\n","epoch 3 batch 0 loss = 17.702047\n","epoch 4 batch 0 loss = 16.826826\n","epoch 5 batch 0 loss = 16.060322\n","epoch 6 batch 0 loss = 15.542796\n","epoch 7 batch 0 loss = 15.268003\n","epoch 8 batch 0 loss = 15.129724\n","epoch 9 batch 0 loss = 15.15802\n","epoch 10 batch 0 loss = 15.100693\n","epoch 11 batch 0 loss = 15.068869\n","epoch 12 batch 0 loss = 15.00389\n","epoch 13 batch 0 loss = 15.002466\n","epoch 14 batch 0 loss = 14.931797\n","epoch 15 batch 0 loss = 14.946548\n","epoch 16 batch 0 loss = 14.878894\n","epoch 17 batch 0 loss = 14.903954\n","epoch 18 batch 0 loss = 14.829733\n","epoch 19 batch 0 loss = 14.868246\n","epoch 20 batch 0 loss = 14.785353\n","epoch 21 batch 0 loss = 14.82792\n","epoch 22 batch 0 loss = 14.754656\n","epoch 23 batch 0 loss = 14.799254\n","epoch 24 batch 0 loss = 14.717531\n","epoch 25 batch 0 loss = 14.773081\n","epoch 26 batch 0 loss = 14.682798\n","epoch 27 batch 0 loss = 14.748246\n","epoch 28 batch 0 loss = 14.650761\n","epoch 29 batch 0 loss = 14.724537\n","epoch 30 batch 0 loss = 14.62063\n","epoch 31 batch 0 loss = 14.702471\n","epoch 32 batch 0 loss = 14.591975\n","epoch 33 batch 0 loss = 14.676344\n","epoch 34 batch 0 loss = 14.568613\n","epoch 35 batch 0 loss = 14.654906\n","epoch 36 batch 0 loss = 14.542189\n","epoch 37 batch 0 loss = 14.634581\n","epoch 38 batch 0 loss = 14.515338\n","epoch 39 batch 0 loss = 14.61079\n","epoch 40 batch 0 loss = 14.493156\n","epoch 41 batch 0 loss = 14.591454\n","epoch 42 batch 0 loss = 14.46727\n","epoch 43 batch 0 loss = 14.572393\n","epoch 44 batch 0 loss = 14.442418\n","epoch 45 batch 0 loss = 14.553338\n","epoch 46 batch 0 loss = 14.418712\n","epoch 47 batch 0 loss = 14.530033\n","epoch 48 batch 0 loss = 14.400743\n","epoch 49 batch 0 loss = 14.512462\n","epoch 0 batch 0 loss = 18.743021\n","epoch 1 batch 0 loss = 18.60099\n","epoch 2 batch 0 loss = 17.696169\n","epoch 3 batch 0 loss = 16.613827\n","epoch 4 batch 0 loss = 15.821947\n","epoch 5 batch 0 loss = 15.324217\n","epoch 6 batch 0 loss = 15.069807\n","epoch 7 batch 0 loss = 14.939483\n","epoch 8 batch 0 loss = 14.931251\n","epoch 9 batch 0 loss = 14.958999\n","epoch 10 batch 0 loss = 14.854843\n","epoch 11 batch 0 loss = 14.861906\n","epoch 12 batch 0 loss = 14.7987\n","epoch 13 batch 0 loss = 14.788191\n","epoch 14 batch 0 loss = 14.753046\n","epoch 15 batch 0 loss = 14.729324\n","epoch 16 batch 0 loss = 14.715139\n","epoch 17 batch 0 loss = 14.678828\n","epoch 18 batch 0 loss = 14.64132\n","epoch 18 batch 1 loss = 14.6356535\n","epoch 19 batch 0 loss = 14.650307\n","epoch 20 batch 0 loss = 14.59421\n","epoch 21 batch 0 loss = 14.621783\n","epoch 22 batch 0 loss = 14.556058\n","epoch 23 batch 0 loss = 14.592109\n","epoch 24 batch 0 loss = 14.523912\n","epoch 25 batch 0 loss = 14.56507\n","epoch 26 batch 0 loss = 14.485702\n","epoch 27 batch 0 loss = 14.535278\n","epoch 28 batch 0 loss = 14.452741\n","epoch 29 batch 0 loss = 14.510237\n","epoch 30 batch 0 loss = 14.419897\n","epoch 31 batch 0 loss = 14.485355\n","epoch 32 batch 0 loss = 14.387374\n","epoch 33 batch 0 loss = 14.459892\n","epoch 34 batch 0 loss = 14.355701\n","epoch 35 batch 0 loss = 14.434227\n","epoch 36 batch 0 loss = 14.324001\n","epoch 37 batch 0 loss = 14.407356\n","epoch 38 batch 0 loss = 14.29213\n","epoch 39 batch 0 loss = 14.379893\n","epoch 40 batch 0 loss = 14.259847\n","epoch 41 batch 0 loss = 14.352537\n","epoch 42 batch 0 loss = 14.227275\n","epoch 43 batch 0 loss = 14.324375\n","epoch 44 batch 0 loss = 14.193926\n","epoch 45 batch 0 loss = 14.294145\n","epoch 46 batch 0 loss = 14.159666\n","epoch 47 batch 0 loss = 14.262584\n","epoch 48 batch 0 loss = 14.123535\n","epoch 49 batch 0 loss = 14.227943\n","epoch 0 batch 0 loss = 18.881\n","epoch 1 batch 0 loss = 18.624708\n","epoch 2 batch 0 loss = 17.549883\n","epoch 3 batch 0 loss = 16.42982\n","epoch 4 batch 0 loss = 15.721101\n","epoch 5 batch 0 loss = 15.256579\n","epoch 6 batch 0 loss = 15.08059\n","epoch 7 batch 0 loss = 15.178544\n","epoch 8 batch 0 loss = 14.950295\n","epoch 9 batch 0 loss = 15.054156\n","epoch 10 batch 0 loss = 14.869333\n","epoch 11 batch 0 loss = 14.977399\n","epoch 12 batch 0 loss = 14.805055\n","epoch 13 batch 0 loss = 14.9199705\n","epoch 14 batch 0 loss = 14.74779\n","epoch 15 batch 0 loss = 14.867048\n","epoch 16 batch 0 loss = 14.874891\n","epoch 16 batch 1 loss = 14.827304\n","epoch 17 batch 0 loss = 14.667549\n","epoch 18 batch 0 loss = 14.78946\n","epoch 19 batch 0 loss = 14.634227\n","epoch 20 batch 0 loss = 14.757883\n","epoch 21 batch 0 loss = 14.601292\n","epoch 22 batch 0 loss = 14.728691\n","epoch 23 batch 0 loss = 14.570932\n","epoch 24 batch 0 loss = 14.701346\n","epoch 25 batch 0 loss = 14.542167\n","epoch 26 batch 0 loss = 14.672997\n","epoch 27 batch 0 loss = 14.517181\n","epoch 28 batch 0 loss = 14.64758\n","epoch 29 batch 0 loss = 14.491477\n","epoch 30 batch 0 loss = 14.624526\n","epoch 31 batch 0 loss = 14.465423\n","epoch 32 batch 0 loss = 14.6024475\n","epoch 33 batch 0 loss = 14.440251\n","epoch 34 batch 0 loss = 14.580243\n","epoch 35 batch 0 loss = 14.4161825\n","epoch 36 batch 0 loss = 14.55901\n","epoch 37 batch 0 loss = 14.392442\n","epoch 38 batch 0 loss = 14.53668\n","epoch 39 batch 0 loss = 14.369615\n","epoch 40 batch 0 loss = 14.515041\n","epoch 41 batch 0 loss = 14.346458\n","epoch 42 batch 0 loss = 14.494506\n","epoch 43 batch 0 loss = 14.324638\n","epoch 44 batch 0 loss = 14.468941\n","epoch 45 batch 0 loss = 14.306996\n","epoch 46 batch 0 loss = 14.449905\n","epoch 47 batch 0 loss = 14.284734\n","epoch 48 batch 0 loss = 14.428112\n","epoch 49 batch 0 loss = 14.264869\n","epoch 0 batch 0 loss = 18.744768\n","epoch 1 batch 0 loss = 18.428162\n","epoch 2 batch 0 loss = 17.610573\n","epoch 3 batch 0 loss = 16.654205\n","epoch 4 batch 0 loss = 15.888559\n","epoch 5 batch 0 loss = 15.397977\n","epoch 6 batch 0 loss = 15.1365185\n","epoch 7 batch 0 loss = 14.972816\n","epoch 8 batch 0 loss = 14.8664665\n","epoch 9 batch 0 loss = 14.786905\n","epoch 10 batch 0 loss = 14.747285\n","epoch 11 batch 0 loss = 14.717749\n","epoch 12 batch 0 loss = 14.792595\n","epoch 13 batch 0 loss = 14.628396\n","epoch 14 batch 0 loss = 14.703475\n","epoch 15 batch 0 loss = 14.550099\n","epoch 16 batch 0 loss = 14.619726\n","epoch 17 batch 0 loss = 14.467429\n","epoch 18 batch 0 loss = 14.533587\n","epoch 19 batch 0 loss = 14.381669\n","epoch 20 batch 0 loss = 14.443728\n","epoch 21 batch 0 loss = 14.290527\n","epoch 22 batch 0 loss = 14.34817\n","epoch 23 batch 0 loss = 14.191797\n","epoch 24 batch 0 loss = 14.242794\n","epoch 25 batch 0 loss = 14.087512\n","epoch 26 batch 0 loss = 14.138491\n","epoch 27 batch 0 loss = 13.987445\n","epoch 28 batch 0 loss = 14.0437975\n","epoch 29 batch 0 loss = 13.897523\n","epoch 30 batch 0 loss = 13.963385\n","epoch 31 batch 0 loss = 13.82166\n","epoch 32 batch 0 loss = 13.897542\n","epoch 33 batch 0 loss = 13.757417\n","epoch 34 batch 0 loss = 13.840656\n","epoch 35 batch 0 loss = 13.701631\n","epoch 36 batch 0 loss = 13.793686\n","epoch 37 batch 0 loss = 13.655646\n","epoch 38 batch 0 loss = 13.755899\n","epoch 39 batch 0 loss = 13.617113\n","epoch 40 batch 0 loss = 13.72503\n","epoch 41 batch 0 loss = 13.584313\n","epoch 42 batch 0 loss = 13.699257\n","epoch 43 batch 0 loss = 13.552257\n","epoch 44 batch 0 loss = 13.673181\n","epoch 45 batch 0 loss = 13.52537\n","epoch 46 batch 0 loss = 13.652128\n","epoch 47 batch 0 loss = 13.500672\n","epoch 48 batch 0 loss = 13.632699\n","epoch 49 batch 0 loss = 13.47749\n","epoch 0 batch 0 loss = 18.973541\n","epoch 1 batch 0 loss = 18.814402\n","epoch 2 batch 0 loss = 17.966486\n","epoch 2 batch 1 loss = 16.950272\n","epoch 3 batch 0 loss = 16.080288\n","epoch 4 batch 0 loss = 15.53503\n","epoch 5 batch 0 loss = 15.211699\n","epoch 6 batch 0 loss = 15.045722\n","epoch 7 batch 0 loss = 14.990471\n","epoch 8 batch 0 loss = 14.999005\n","epoch 9 batch 0 loss = 14.94008\n","epoch 10 batch 0 loss = 14.894495\n","epoch 11 batch 0 loss = 14.872608\n","epoch 12 batch 0 loss = 14.821408\n","epoch 13 batch 0 loss = 14.823672\n","epoch 14 batch 0 loss = 14.762779\n","epoch 15 batch 0 loss = 14.781517\n","epoch 16 batch 0 loss = 14.714205\n","epoch 17 batch 0 loss = 14.745936\n","epoch 18 batch 0 loss = 14.6707\n","epoch 19 batch 0 loss = 14.713804\n","epoch 20 batch 0 loss = 14.628928\n","epoch 21 batch 0 loss = 14.682004\n","epoch 22 batch 0 loss = 14.59283\n","epoch 23 batch 0 loss = 14.654648\n","epoch 24 batch 0 loss = 14.03643\n","epoch 24 batch 1 loss = 14.618984\n","epoch 25 batch 0 loss = 14.535325\n","epoch 26 batch 0 loss = 14.591279\n","epoch 27 batch 0 loss = 14.504052\n","epoch 28 batch 0 loss = 14.568608\n","epoch 29 batch 0 loss = 14.474607\n","epoch 30 batch 0 loss = 14.54689\n","epoch 31 batch 0 loss = 14.446442\n","epoch 32 batch 0 loss = 14.525774\n","epoch 33 batch 0 loss = 14.419199\n","epoch 34 batch 0 loss = 14.504129\n","epoch 35 batch 0 loss = 14.393711\n","epoch 36 batch 0 loss = 14.4816675\n","epoch 37 batch 0 loss = 14.370491\n","epoch 38 batch 0 loss = 14.46184\n","epoch 39 batch 0 loss = 14.345389\n","epoch 40 batch 0 loss = 14.442347\n","epoch 41 batch 0 loss = 14.320705\n","epoch 42 batch 0 loss = 14.420452\n","epoch 43 batch 0 loss = 14.299217\n","epoch 44 batch 0 loss = 14.401515\n","epoch 45 batch 0 loss = 14.275293\n","epoch 46 batch 0 loss = 14.38217\n","epoch 47 batch 0 loss = 14.252067\n","epoch 48 batch 0 loss = 14.363291\n","epoch 49 batch 0 loss = 14.22885\n","epoch 0 batch 0 loss = 18.860418\n","epoch 1 batch 0 loss = 18.804281\n","epoch 2 batch 0 loss = 18.588303\n","epoch 3 batch 0 loss = 17.66694\n","epoch 4 batch 0 loss = 16.690197\n","epoch 5 batch 0 loss = 15.984295\n","epoch 6 batch 0 loss = 15.484071\n","epoch 7 batch 0 loss = 15.19515\n","epoch 8 batch 0 loss = 15.017489\n","epoch 9 batch 0 loss = 14.899985\n","epoch 10 batch 0 loss = 14.818552\n","epoch 11 batch 0 loss = 14.769098\n","epoch 12 batch 0 loss = 14.754175\n","epoch 13 batch 0 loss = 14.849015\n","epoch 14 batch 0 loss = 14.793542\n","epoch 15 batch 0 loss = 14.798654\n","epoch 16 batch 0 loss = 14.742845\n","epoch 17 batch 0 loss = 14.759256\n","epoch 18 batch 0 loss = 14.694597\n","epoch 19 batch 0 loss = 14.724768\n","epoch 20 batch 0 loss = 14.651747\n","epoch 21 batch 0 loss = 14.692815\n","epoch 22 batch 0 loss = 14.613179\n","epoch 23 batch 0 loss = 14.663485\n","epoch 24 batch 0 loss = 14.577315\n","epoch 25 batch 0 loss = 14.632482\n","epoch 26 batch 0 loss = 14.54672\n","epoch 27 batch 0 loss = 14.606574\n","epoch 28 batch 0 loss = 14.5144615\n","epoch 29 batch 0 loss = 14.581877\n","epoch 30 batch 0 loss = 14.4828005\n","epoch 31 batch 0 loss = 14.557413\n","epoch 32 batch 0 loss = 14.453652\n","epoch 33 batch 0 loss = 14.532467\n","epoch 34 batch 0 loss = 14.427359\n","epoch 35 batch 0 loss = 14.510087\n","epoch 36 batch 0 loss = 14.400117\n","epoch 37 batch 0 loss = 14.4881525\n","epoch 38 batch 0 loss = 14.371988\n","epoch 39 batch 0 loss = 14.465296\n","epoch 40 batch 0 loss = 14.346262\n","epoch 41 batch 0 loss = 14.444331\n","epoch 42 batch 0 loss = 14.320859\n","epoch 43 batch 0 loss = 14.423055\n","epoch 44 batch 0 loss = 14.296566\n","epoch 45 batch 0 loss = 14.402975\n","epoch 46 batch 0 loss = 14.272047\n","epoch 47 batch 0 loss = 14.38262\n","epoch 48 batch 0 loss = 14.248441\n","epoch 49 batch 0 loss = 14.362216\n","epoch 0 batch 0 loss = 18.88295\n","epoch 1 batch 0 loss = 18.69019\n","epoch 2 batch 0 loss = 17.796976\n","epoch 3 batch 0 loss = 16.755949\n","epoch 4 batch 0 loss = 15.966016\n","epoch 5 batch 0 loss = 15.412799\n","epoch 6 batch 0 loss = 15.128372\n","epoch 7 batch 0 loss = 14.970164\n","epoch 8 batch 0 loss = 14.864621\n","epoch 9 batch 0 loss = 14.78562\n","epoch 10 batch 0 loss = 14.723088\n","epoch 11 batch 0 loss = 14.671391\n","epoch 12 batch 0 loss = 14.627716\n","epoch 13 batch 0 loss = 14.590135\n","epoch 14 batch 0 loss = 14.557533\n","epoch 15 batch 0 loss = 14.5302305\n","epoch 16 batch 0 loss = 14.514905\n","epoch 17 batch 0 loss = 14.540786\n","epoch 18 batch 0 loss = 14.773095\n","epoch 19 batch 0 loss = 14.596937\n","epoch 20 batch 0 loss = 14.741958\n","epoch 21 batch 0 loss = 14.565721\n","epoch 22 batch 0 loss = 14.711173\n","epoch 23 batch 0 loss = 14.523079\n","epoch 24 batch 0 loss = 14.669773\n","epoch 25 batch 0 loss = 14.493955\n","epoch 26 batch 0 loss = 14.63632\n","epoch 27 batch 0 loss = 14.472585\n","epoch 28 batch 0 loss = 14.6113615\n","epoch 29 batch 0 loss = 14.446254\n","epoch 30 batch 0 loss = 14.587617\n","epoch 31 batch 0 loss = 14.420799\n","epoch 32 batch 0 loss = 14.564273\n","epoch 33 batch 0 loss = 14.3958645\n","epoch 34 batch 0 loss = 14.533365\n","epoch 35 batch 0 loss = 14.379425\n","epoch 36 batch 0 loss = 14.511729\n","epoch 37 batch 0 loss = 14.355004\n","epoch 38 batch 0 loss = 14.490202\n","epoch 39 batch 0 loss = 14.331059\n","epoch 40 batch 0 loss = 14.468999\n","epoch 41 batch 0 loss = 14.307335\n","epoch 42 batch 0 loss = 14.44791\n","epoch 43 batch 0 loss = 14.283743\n","epoch 44 batch 0 loss = 14.424505\n","epoch 45 batch 0 loss = 14.262457\n","epoch 46 batch 0 loss = 14.402228\n","epoch 47 batch 0 loss = 14.24055\n","epoch 48 batch 0 loss = 14.380835\n","epoch 49 batch 0 loss = 14.218215\n","epoch 0 batch 0 loss = 18.43168\n","epoch 1 batch 0 loss = 18.160984\n","epoch 2 batch 0 loss = 16.987549\n","epoch 3 batch 0 loss = 15.884876\n","epoch 4 batch 0 loss = 15.344804\n","epoch 5 batch 0 loss = 15.244949\n","epoch 6 batch 0 loss = 15.140048\n","epoch 7 batch 0 loss = 15.104038\n","epoch 8 batch 0 loss = 15.01053\n","epoch 9 batch 0 loss = 15.018244\n","epoch 10 batch 0 loss = 14.918603\n","epoch 11 batch 0 loss = 14.954504\n","epoch 12 batch 0 loss = 14.847336\n","epoch 13 batch 0 loss = 14.902197\n","epoch 14 batch 0 loss = 14.78887\n","epoch 15 batch 0 loss = 14.8578005\n","epoch 16 batch 0 loss = 14.736602\n","epoch 17 batch 0 loss = 14.817933\n","epoch 18 batch 0 loss = 14.689114\n","epoch 19 batch 0 loss = 14.781059\n","epoch 20 batch 0 loss = 14.646168\n","epoch 21 batch 0 loss = 14.738222\n","epoch 22 batch 0 loss = 14.613365\n","epoch 23 batch 0 loss = 14.704475\n","epoch 24 batch 0 loss = 14.574627\n","epoch 25 batch 0 loss = 14.673436\n","epoch 26 batch 0 loss = 14.535892\n","epoch 27 batch 0 loss = 14.641852\n","epoch 28 batch 0 loss = 14.499603\n","epoch 29 batch 0 loss = 14.609483\n","epoch 30 batch 0 loss = 14.466025\n","epoch 31 batch 0 loss = 14.580946\n","epoch 32 batch 0 loss = 14.429604\n","epoch 33 batch 0 loss = 14.552255\n","epoch 34 batch 0 loss = 14.394605\n","epoch 35 batch 0 loss = 14.523981\n","epoch 36 batch 0 loss = 14.359913\n","epoch 37 batch 0 loss = 14.495415\n","epoch 38 batch 0 loss = 14.326389\n","epoch 39 batch 0 loss = 14.466014\n","epoch 40 batch 0 loss = 14.294195\n","epoch 41 batch 0 loss = 14.438463\n","epoch 42 batch 0 loss = 14.260553\n","epoch 43 batch 0 loss = 14.410806\n","epoch 44 batch 0 loss = 14.226969\n","epoch 45 batch 0 loss = 14.383996\n","epoch 46 batch 0 loss = 14.194125\n","epoch 47 batch 0 loss = 14.356218\n","epoch 48 batch 0 loss = 14.161704\n","epoch 49 batch 0 loss = 14.327617\n","epoch 0 batch 0 loss = 18.774652\n","epoch 1 batch 0 loss = 18.738874\n","epoch 2 batch 0 loss = 18.714567\n","epoch 3 batch 0 loss = 18.695423\n","epoch 4 batch 0 loss = 18.676308\n","epoch 5 batch 0 loss = 18.657118\n","epoch 6 batch 0 loss = 18.63788\n","epoch 7 batch 0 loss = 18.618593\n","epoch 8 batch 0 loss = 18.59926\n","epoch 9 batch 0 loss = 18.579891\n","epoch 10 batch 0 loss = 18.560476\n","epoch 11 batch 0 loss = 18.54103\n","epoch 12 batch 0 loss = 18.521574\n","epoch 13 batch 0 loss = 18.502108\n","epoch 14 batch 0 loss = 18.482618\n","epoch 15 batch 0 loss = 18.463089\n","epoch 16 batch 0 loss = 18.443523\n","epoch 17 batch 0 loss = 18.423939\n","epoch 18 batch 0 loss = 18.40434\n","epoch 19 batch 0 loss = 18.384722\n","epoch 20 batch 0 loss = 18.365084\n","epoch 21 batch 0 loss = 18.345434\n","epoch 22 batch 0 loss = 18.325766\n","epoch 23 batch 0 loss = 18.306072\n","epoch 24 batch 0 loss = 18.286383\n","epoch 25 batch 0 loss = 18.266678\n","epoch 26 batch 0 loss = 18.246954\n","epoch 27 batch 0 loss = 18.227228\n","epoch 28 batch 0 loss = 18.207493\n","epoch 29 batch 0 loss = 18.18775\n","epoch 30 batch 0 loss = 18.168007\n","epoch 31 batch 0 loss = 18.148268\n","epoch 32 batch 0 loss = 18.128536\n","epoch 33 batch 0 loss = 18.108784\n","epoch 34 batch 0 loss = 18.089035\n","epoch 35 batch 0 loss = 18.069288\n","epoch 36 batch 0 loss = 18.049541\n","epoch 37 batch 0 loss = 18.029793\n","epoch 38 batch 0 loss = 18.010048\n","epoch 39 batch 0 loss = 17.990307\n","epoch 40 batch 0 loss = 17.97058\n","epoch 41 batch 0 loss = 17.950855\n","epoch 42 batch 0 loss = 17.931135\n","epoch 43 batch 0 loss = 17.911428\n","epoch 44 batch 0 loss = 17.891739\n","epoch 45 batch 0 loss = 17.872057\n","epoch 46 batch 0 loss = 17.852385\n","epoch 47 batch 0 loss = 17.832724\n","epoch 48 batch 0 loss = 17.813074\n","epoch 49 batch 0 loss = 17.79344\n","epoch 0 batch 0 loss = 18.850014\n","epoch 1 batch 0 loss = 18.211964\n","epoch 2 batch 0 loss = 16.93148\n","epoch 3 batch 0 loss = 15.974868\n","epoch 4 batch 0 loss = 15.343632\n","epoch 5 batch 0 loss = 15.012219\n","epoch 6 batch 0 loss = 14.853655\n","epoch 7 batch 0 loss = 14.773576\n","epoch 8 batch 0 loss = 14.874229\n","epoch 9 batch 0 loss = 14.935848\n","epoch 10 batch 0 loss = 14.804766\n","epoch 11 batch 0 loss = 14.870997\n","epoch 12 batch 0 loss = 14.740328\n","epoch 13 batch 0 loss = 14.774704\n","epoch 13 batch 1 loss = 14.608881\n","epoch 14 batch 0 loss = 14.765805\n","epoch 15 batch 0 loss = 14.620689\n","epoch 16 batch 0 loss = 14.7164135\n","epoch 17 batch 0 loss = 14.568876\n","epoch 18 batch 0 loss = 14.672148\n","epoch 19 batch 0 loss = 14.520814\n","epoch 20 batch 0 loss = 14.629105\n","epoch 21 batch 0 loss = 14.431006\n","epoch 21 batch 1 loss = 14.587989\n","epoch 22 batch 0 loss = 14.429336\n","epoch 23 batch 0 loss = 14.544501\n","epoch 24 batch 0 loss = 14.389543\n","epoch 25 batch 0 loss = 14.505968\n","epoch 26 batch 0 loss = 14.349221\n","epoch 27 batch 0 loss = 14.467902\n","epoch 28 batch 0 loss = 14.310066\n","epoch 29 batch 0 loss = 14.431362\n","epoch 30 batch 0 loss = 14.271891\n","epoch 31 batch 0 loss = 14.396154\n","epoch 32 batch 0 loss = 14.2343\n","epoch 33 batch 0 loss = 14.358172\n","epoch 34 batch 0 loss = 14.200388\n","epoch 35 batch 0 loss = 14.324248\n","epoch 36 batch 0 loss = 14.1639385\n","epoch 37 batch 0 loss = 14.290389\n","epoch 38 batch 0 loss = 14.127921\n","epoch 39 batch 0 loss = 14.25718\n","epoch 40 batch 0 loss = 14.092115\n","epoch 41 batch 0 loss = 14.223816\n","epoch 42 batch 0 loss = 14.057287\n","epoch 43 batch 0 loss = 14.189492\n","epoch 44 batch 0 loss = 14.023589\n","epoch 45 batch 0 loss = 14.156615\n","epoch 46 batch 0 loss = 13.9891205\n","epoch 47 batch 0 loss = 14.12339\n","epoch 48 batch 0 loss = 13.954889\n","epoch 49 batch 0 loss = 14.090101\n","epoch 0 batch 0 loss = 19.008526\n","epoch 1 batch 0 loss = 18.214493\n","epoch 2 batch 0 loss = 17.013504\n","epoch 3 batch 0 loss = 16.0685\n","epoch 4 batch 0 loss = 15.386715\n","epoch 4 batch 1 loss = 15.133912\n","epoch 5 batch 0 loss = 14.981933\n","epoch 6 batch 0 loss = 14.9611\n","epoch 7 batch 0 loss = 14.9805565\n","epoch 8 batch 0 loss = 14.865761\n","epoch 9 batch 0 loss = 14.867944\n","epoch 10 batch 0 loss = 14.792944\n","epoch 11 batch 0 loss = 14.737435\n","epoch 11 batch 1 loss = 14.735622\n","epoch 12 batch 0 loss = 14.659119\n","epoch 12 batch 1 loss = 14.68179\n","epoch 13 batch 0 loss = 14.636068\n","epoch 14 batch 0 loss = 14.6342745\n","epoch 15 batch 0 loss = 14.576988\n","epoch 16 batch 0 loss = 14.589944\n","epoch 17 batch 0 loss = 14.523201\n","epoch 18 batch 0 loss = 14.542875\n","epoch 19 batch 0 loss = 14.477059\n","epoch 20 batch 0 loss = 14.502434\n","epoch 21 batch 0 loss = 14.42842\n","epoch 22 batch 0 loss = 14.462863\n","epoch 23 batch 0 loss = 14.381816\n","epoch 24 batch 0 loss = 14.425157\n","epoch 25 batch 0 loss = 14.336034\n","epoch 26 batch 0 loss = 14.385245\n","epoch 27 batch 0 loss = 14.294268\n","epoch 28 batch 0 loss = 14.34713\n","epoch 29 batch 0 loss = 14.252888\n","epoch 30 batch 0 loss = 14.310875\n","epoch 31 batch 0 loss = 14.211274\n","epoch 32 batch 0 loss = 14.274106\n","epoch 33 batch 0 loss = 14.171268\n","epoch 34 batch 0 loss = 14.236539\n","epoch 35 batch 0 loss = 14.132652\n","epoch 36 batch 0 loss = 14.199155\n","epoch 37 batch 0 loss = 14.094789\n","epoch 38 batch 0 loss = 14.164777\n","epoch 39 batch 0 loss = 14.030561\n","epoch 39 batch 1 loss = 14.130159\n","epoch 40 batch 0 loss = 14.014916\n","epoch 41 batch 0 loss = 14.0950575\n","epoch 42 batch 0 loss = 13.975557\n","epoch 43 batch 0 loss = 14.058672\n","epoch 44 batch 0 loss = 13.937665\n","epoch 45 batch 0 loss = 14.022732\n","epoch 46 batch 0 loss = 13.899737\n","epoch 47 batch 0 loss = 13.987418\n","epoch 48 batch 0 loss = 13.860938\n","epoch 49 batch 0 loss = 13.952067\n","epoch 0 batch 0 loss = 19.279459\n","epoch 1 batch 0 loss = 19.237389\n","epoch 2 batch 0 loss = 19.20155\n","epoch 3 batch 0 loss = 19.12035\n","epoch 4 batch 0 loss = 18.956366\n","epoch 5 batch 0 loss = 18.335901\n","epoch 6 batch 0 loss = 17.422554\n","epoch 7 batch 0 loss = 16.599447\n","epoch 8 batch 0 loss = 16.007908\n","epoch 9 batch 0 loss = 15.545837\n","epoch 10 batch 0 loss = 15.255159\n","epoch 11 batch 0 loss = 15.064814\n","epoch 12 batch 0 loss = 14.932293\n","epoch 13 batch 0 loss = 14.836091\n","epoch 14 batch 0 loss = 14.761497\n","epoch 15 batch 0 loss = 14.709971\n","epoch 16 batch 0 loss = 14.675061\n","epoch 17 batch 0 loss = 14.696873\n","epoch 18 batch 0 loss = 14.730449\n","epoch 19 batch 0 loss = 14.6630745\n","epoch 20 batch 0 loss = 14.656216\n","epoch 21 batch 0 loss = 14.612139\n","epoch 22 batch 0 loss = 14.588736\n","epoch 23 batch 0 loss = 14.564719\n","epoch 24 batch 0 loss = 14.528136\n","epoch 25 batch 0 loss = 14.519792\n","epoch 26 batch 0 loss = 14.472189\n","epoch 27 batch 0 loss = 14.477726\n","epoch 28 batch 0 loss = 14.419428\n","epoch 29 batch 0 loss = 14.436564\n","epoch 30 batch 0 loss = 14.369494\n","epoch 31 batch 0 loss = 14.396026\n","epoch 32 batch 0 loss = 14.322126\n","epoch 33 batch 0 loss = 14.356581\n","epoch 34 batch 0 loss = 14.276197\n","epoch 35 batch 0 loss = 14.317992\n","epoch 36 batch 0 loss = 14.231279\n","epoch 37 batch 0 loss = 14.238918\n","epoch 37 batch 1 loss = 14.187579\n","epoch 38 batch 0 loss = 14.24241\n","epoch 39 batch 0 loss = 14.144738\n","epoch 40 batch 0 loss = 13.787228\n","epoch 40 batch 1 loss = 14.101057\n","epoch 41 batch 0 loss = 14.170262\n","epoch 42 batch 0 loss = 14.058325\n","epoch 43 batch 0 loss = 14.132684\n","epoch 44 batch 0 loss = 14.017412\n","epoch 45 batch 0 loss = 14.0956745\n","epoch 46 batch 0 loss = 13.977227\n","epoch 47 batch 0 loss = 14.054499\n","epoch 48 batch 0 loss = 13.94127\n","epoch 49 batch 0 loss = 14.018263\n","epoch 0 batch 0 loss = 19.120182\n","epoch 1 batch 0 loss = 18.592403\n","epoch 2 batch 0 loss = 17.47111\n","epoch 3 batch 0 loss = 16.414068\n","epoch 4 batch 0 loss = 15.747337\n","epoch 5 batch 0 loss = 15.308046\n","epoch 6 batch 0 loss = 15.106128\n","epoch 7 batch 0 loss = 15.017902\n","epoch 8 batch 0 loss = 15.025201\n","epoch 9 batch 0 loss = 14.995442\n","epoch 10 batch 0 loss = 14.916423\n","epoch 11 batch 0 loss = 14.9120865\n","epoch 12 batch 0 loss = 14.828865\n","epoch 13 batch 0 loss = 14.843449\n","epoch 14 batch 0 loss = 14.757108\n","epoch 15 batch 0 loss = 14.785713\n","epoch 16 batch 0 loss = 14.693147\n","epoch 17 batch 0 loss = 14.73391\n","epoch 18 batch 0 loss = 14.636549\n","epoch 19 batch 0 loss = 14.686818\n","epoch 20 batch 0 loss = 14.583961\n","epoch 21 batch 0 loss = 14.642487\n","epoch 22 batch 0 loss = 14.535081\n","epoch 23 batch 0 loss = 14.598426\n","epoch 24 batch 0 loss = 14.490448\n","epoch 25 batch 0 loss = 14.510117\n","epoch 25 batch 1 loss = 14.446045\n","epoch 26 batch 0 loss = 14.5194435\n","epoch 27 batch 0 loss = 14.401714\n","epoch 28 batch 0 loss = 14.480854\n","epoch 29 batch 0 loss = 14.3593235\n","epoch 30 batch 0 loss = 14.443522\n","epoch 31 batch 0 loss = 14.318316\n","epoch 32 batch 0 loss = 14.40698\n","epoch 33 batch 0 loss = 14.278076\n","epoch 34 batch 0 loss = 14.370624\n","epoch 35 batch 0 loss = 14.238794\n","epoch 36 batch 0 loss = 14.334965\n","epoch 37 batch 0 loss = 14.199819\n","epoch 38 batch 0 loss = 14.299435\n","epoch 39 batch 0 loss = 14.161468\n","epoch 40 batch 0 loss = 14.262272\n","epoch 41 batch 0 loss = 14.098732\n","epoch 41 batch 1 loss = 14.227751\n","epoch 42 batch 0 loss = 14.087787\n","epoch 43 batch 0 loss = 14.192908\n","epoch 44 batch 0 loss = 14.050419\n","epoch 45 batch 0 loss = 14.15724\n","epoch 46 batch 0 loss = 14.014137\n","epoch 47 batch 0 loss = 14.120678\n","epoch 48 batch 0 loss = 13.979045\n","epoch 49 batch 0 loss = 14.086223\n","epoch 0 batch 0 loss = 19.358925\n","epoch 1 batch 0 loss = 19.298697\n","epoch 2 batch 0 loss = 19.012243\n","epoch 3 batch 0 loss = 17.976452\n","epoch 4 batch 0 loss = 16.871962\n","epoch 5 batch 0 loss = 16.099834\n","epoch 6 batch 0 loss = 15.557137\n","epoch 7 batch 0 loss = 15.246867\n","epoch 8 batch 0 loss = 15.059145\n","epoch 9 batch 0 loss = 14.935061\n","epoch 10 batch 0 loss = 14.850166\n","epoch 11 batch 0 loss = 14.81725\n","epoch 12 batch 0 loss = 14.8236475\n","epoch 13 batch 0 loss = 14.85173\n","epoch 14 batch 0 loss = 14.726738\n","epoch 15 batch 0 loss = 14.763552\n","epoch 16 batch 0 loss = 14.651484\n","epoch 17 batch 0 loss = 14.694866\n","epoch 18 batch 0 loss = 14.584514\n","epoch 19 batch 0 loss = 14.633994\n","epoch 20 batch 0 loss = 14.525058\n","epoch 21 batch 0 loss = 14.579377\n","epoch 22 batch 0 loss = 14.4691925\n","epoch 23 batch 0 loss = 14.528562\n","epoch 24 batch 0 loss = 14.416565\n","epoch 25 batch 0 loss = 14.480006\n","epoch 26 batch 0 loss = 14.367114\n","epoch 27 batch 0 loss = 14.431253\n","epoch 28 batch 0 loss = 14.3218975\n","epoch 29 batch 0 loss = 14.387277\n","epoch 30 batch 0 loss = 14.275618\n","epoch 31 batch 0 loss = 14.342052\n","epoch 32 batch 0 loss = 14.233039\n","epoch 33 batch 0 loss = 14.301058\n","epoch 34 batch 0 loss = 14.188626\n","epoch 35 batch 0 loss = 14.260397\n","epoch 36 batch 0 loss = 14.145458\n","epoch 37 batch 0 loss = 14.213348\n","epoch 38 batch 0 loss = 14.110378\n","epoch 39 batch 0 loss = 14.1745\n","epoch 40 batch 0 loss = 14.068387\n","epoch 41 batch 0 loss = 14.136538\n","epoch 42 batch 0 loss = 14.02627\n","epoch 43 batch 0 loss = 14.098694\n","epoch 44 batch 0 loss = 13.984651\n","epoch 45 batch 0 loss = 14.060943\n","epoch 46 batch 0 loss = 13.943643\n","epoch 47 batch 0 loss = 14.020676\n","epoch 48 batch 0 loss = 13.905484\n","epoch 49 batch 0 loss = 13.982858\n","epoch 0 batch 0 loss = 19.308533\n","epoch 1 batch 0 loss = 19.203959\n","epoch 2 batch 0 loss = 18.837812\n","epoch 3 batch 0 loss = 18.012812\n","epoch 4 batch 0 loss = 17.066483\n","epoch 5 batch 0 loss = 16.238281\n","epoch 6 batch 0 loss = 15.682137\n","epoch 7 batch 0 loss = 15.315149\n","epoch 8 batch 0 loss = 15.089593\n","epoch 9 batch 0 loss = 14.935231\n","epoch 10 batch 0 loss = 14.822593\n","epoch 11 batch 0 loss = 14.736967\n","epoch 12 batch 0 loss = 14.666367\n","epoch 13 batch 0 loss = 14.611285\n","epoch 14 batch 0 loss = 14.564085\n","epoch 15 batch 0 loss = 14.538581\n","epoch 16 batch 0 loss = 14.520866\n","epoch 17 batch 0 loss = 14.567185\n","epoch 18 batch 0 loss = 14.564632\n","epoch 19 batch 0 loss = 14.51756\n","epoch 20 batch 0 loss = 14.4979515\n","epoch 21 batch 0 loss = 14.471778\n","epoch 22 batch 0 loss = 14.436527\n","epoch 23 batch 0 loss = 14.428905\n","epoch 24 batch 0 loss = 14.379399\n","epoch 25 batch 0 loss = 14.384111\n","epoch 26 batch 0 loss = 14.328735\n","epoch 27 batch 0 loss = 14.342736\n","epoch 28 batch 0 loss = 14.2789545\n","epoch 29 batch 0 loss = 14.302733\n","epoch 30 batch 0 loss = 14.230869\n","epoch 31 batch 0 loss = 14.259307\n","epoch 32 batch 0 loss = 14.188892\n","epoch 33 batch 0 loss = 14.220864\n","epoch 34 batch 0 loss = 14.143281\n","epoch 35 batch 0 loss = 14.182949\n","epoch 36 batch 0 loss = 14.098569\n","epoch 37 batch 0 loss = 14.145275\n","epoch 38 batch 0 loss = 14.054712\n","epoch 39 batch 0 loss = 14.107502\n","epoch 40 batch 0 loss = 14.011729\n","epoch 41 batch 0 loss = 14.069538\n","epoch 42 batch 0 loss = 13.969761\n","epoch 43 batch 0 loss = 14.032048\n","epoch 44 batch 0 loss = 13.927941\n","epoch 45 batch 0 loss = 13.994762\n","epoch 46 batch 0 loss = 13.886471\n","epoch 47 batch 0 loss = 13.9571085\n","epoch 48 batch 0 loss = 13.84578\n","epoch 49 batch 0 loss = 13.918492\n","epoch 0 batch 0 loss = 19.4348\n","epoch 1 batch 0 loss = 19.281147\n","epoch 2 batch 0 loss = 18.590124\n","epoch 3 batch 0 loss = 17.601202\n","epoch 4 batch 0 loss = 16.668854\n","epoch 5 batch 0 loss = 15.983021\n","epoch 6 batch 0 loss = 15.496246\n","epoch 7 batch 0 loss = 15.220487\n","epoch 8 batch 0 loss = 15.047184\n","epoch 9 batch 0 loss = 14.9263\n","epoch 10 batch 0 loss = 14.834067\n","epoch 11 batch 0 loss = 14.7593565\n","epoch 12 batch 0 loss = 14.695608\n","epoch 13 batch 0 loss = 14.640476\n","epoch 14 batch 0 loss = 14.591232\n","epoch 15 batch 0 loss = 14.548257\n","epoch 16 batch 0 loss = 14.510202\n","epoch 17 batch 0 loss = 14.482244\n","epoch 18 batch 0 loss = 14.46332\n","epoch 19 batch 0 loss = 14.479814\n","epoch 20 batch 0 loss = 14.520481\n","epoch 21 batch 0 loss = 14.5455\n","epoch 22 batch 0 loss = 14.458999\n","epoch 23 batch 0 loss = 14.492939\n","epoch 24 batch 0 loss = 14.41112\n","epoch 25 batch 0 loss = 14.449203\n","epoch 26 batch 0 loss = 14.362239\n","epoch 27 batch 0 loss = 14.403525\n","epoch 28 batch 0 loss = 14.318343\n","epoch 29 batch 0 loss = 14.362081\n","epoch 30 batch 0 loss = 14.272865\n","epoch 31 batch 0 loss = 14.320613\n","epoch 32 batch 0 loss = 14.229587\n","epoch 33 batch 0 loss = 14.281219\n","epoch 34 batch 0 loss = 14.185594\n","epoch 35 batch 0 loss = 14.242328\n","epoch 36 batch 0 loss = 14.142473\n","epoch 37 batch 0 loss = 14.200803\n","epoch 38 batch 0 loss = 14.102754\n","epoch 39 batch 0 loss = 14.162674\n","epoch 40 batch 0 loss = 14.060379\n","epoch 41 batch 0 loss = 14.124042\n","epoch 42 batch 0 loss = 14.019072\n","epoch 43 batch 0 loss = 14.08605\n","epoch 44 batch 0 loss = 13.977499\n","epoch 45 batch 0 loss = 14.047864\n","epoch 46 batch 0 loss = 13.9364\n","epoch 47 batch 0 loss = 14.00978\n","epoch 48 batch 0 loss = 13.895477\n","epoch 49 batch 0 loss = 13.969727\n","epoch 0 batch 0 loss = 25.807674\n","epoch 1 batch 0 loss = 24.995249\n","epoch 2 batch 0 loss = 24.154491\n","epoch 3 batch 0 loss = 23.296011\n","epoch 4 batch 0 loss = 22.439213\n","epoch 5 batch 0 loss = 21.579916\n","epoch 6 batch 0 loss = 20.734985\n","epoch 7 batch 0 loss = 19.907003\n","epoch 8 batch 0 loss = 19.10081\n","epoch 9 batch 0 loss = 18.319262\n","epoch 10 batch 0 loss = 17.567745\n","epoch 11 batch 0 loss = 16.851837\n","epoch 12 batch 0 loss = 16.17759\n","epoch 13 batch 0 loss = 15.551524\n","epoch 14 batch 0 loss = 14.99959\n","epoch 15 batch 0 loss = 14.549083\n","epoch 16 batch 0 loss = 14.189515\n","epoch 17 batch 0 loss = 13.875051\n","epoch 18 batch 0 loss = 13.611497\n","epoch 19 batch 0 loss = 13.408468\n","epoch 20 batch 0 loss = 13.254002\n","epoch 21 batch 0 loss = 13.137602\n","epoch 22 batch 0 loss = 13.050483\n","epoch 23 batch 0 loss = 12.985555\n","epoch 24 batch 0 loss = 12.937259\n","epoch 25 batch 0 loss = 12.901335\n","epoch 26 batch 0 loss = 12.874572\n","epoch 27 batch 0 loss = 12.854578\n","epoch 28 batch 0 loss = 12.839585\n","epoch 29 batch 0 loss = 12.828296\n","epoch 30 batch 0 loss = 12.8197565\n","epoch 31 batch 0 loss = 12.813265\n","epoch 32 batch 0 loss = 12.80831\n","epoch 33 batch 0 loss = 12.804508\n","epoch 34 batch 0 loss = 12.801579\n","epoch 35 batch 0 loss = 12.799315\n","epoch 36 batch 0 loss = 12.797553\n","epoch 37 batch 0 loss = 12.79618\n","epoch 38 batch 0 loss = 12.795103\n","epoch 39 batch 0 loss = 12.794254\n","epoch 40 batch 0 loss = 12.793587\n","epoch 41 batch 0 loss = 12.793054\n","epoch 42 batch 0 loss = 12.792628\n","epoch 43 batch 0 loss = 12.792287\n","epoch 44 batch 0 loss = 12.792011\n","epoch 45 batch 0 loss = 12.791786\n","epoch 46 batch 0 loss = 12.791602\n","epoch 47 batch 0 loss = 12.79145\n","epoch 48 batch 0 loss = 12.791323\n","epoch 49 batch 0 loss = 12.791214\n","epoch 0 batch 0 loss = 26.076382\n","epoch 1 batch 0 loss = 25.410082\n","epoch 2 batch 0 loss = 24.738714\n","epoch 3 batch 0 loss = 24.03389\n","epoch 4 batch 0 loss = 23.282887\n","epoch 5 batch 0 loss = 22.519215\n","epoch 6 batch 0 loss = 21.743437\n","epoch 7 batch 0 loss = 20.966589\n","epoch 8 batch 0 loss = 20.1812\n","epoch 9 batch 0 loss = 19.397167\n","epoch 10 batch 0 loss = 18.61451\n","epoch 11 batch 0 loss = 17.844524\n","epoch 12 batch 0 loss = 17.092043\n","epoch 13 batch 0 loss = 16.36196\n","epoch 14 batch 0 loss = 15.658582\n","epoch 15 batch 0 loss = 14.969959\n","epoch 16 batch 0 loss = 14.31977\n","epoch 17 batch 0 loss = 13.712607\n","epoch 18 batch 0 loss = 13.147523\n","epoch 19 batch 0 loss = 12.674344\n","epoch 20 batch 0 loss = 12.2858\n","epoch 21 batch 0 loss = 11.972259\n","epoch 22 batch 0 loss = 11.722979\n","epoch 23 batch 0 loss = 11.527182\n","epoch 24 batch 0 loss = 11.374812\n","epoch 25 batch 0 loss = 11.257004\n","epoch 26 batch 0 loss = 11.16629\n","epoch 27 batch 0 loss = 11.096556\n","epoch 28 batch 0 loss = 11.042944\n","epoch 29 batch 0 loss = 11.001662\n","epoch 30 batch 0 loss = 10.969792\n","epoch 31 batch 0 loss = 10.945105\n","epoch 32 batch 0 loss = 10.925909\n","epoch 33 batch 0 loss = 10.910924\n","epoch 34 batch 0 loss = 10.899183\n","epoch 35 batch 0 loss = 10.889949\n","epoch 36 batch 0 loss = 10.882662\n","epoch 37 batch 0 loss = 10.876896\n","epoch 38 batch 0 loss = 10.872319\n","epoch 39 batch 0 loss = 10.868677\n","epoch 40 batch 0 loss = 10.865773\n","epoch 41 batch 0 loss = 10.863456\n","epoch 42 batch 0 loss = 10.861601\n","epoch 43 batch 0 loss = 10.860115\n","epoch 44 batch 0 loss = 10.858924\n","epoch 45 batch 0 loss = 10.857967\n","epoch 46 batch 0 loss = 10.8572\n","epoch 47 batch 0 loss = 10.856582\n","epoch 48 batch 0 loss = 10.856086\n","epoch 49 batch 0 loss = 10.855686\n","epoch 0 batch 0 loss = 26.912983\n","epoch 1 batch 0 loss = 26.331705\n","epoch 2 batch 0 loss = 25.741817\n","epoch 3 batch 0 loss = 25.144567\n","epoch 4 batch 0 loss = 24.516218\n","epoch 5 batch 0 loss = 23.874641\n","epoch 6 batch 0 loss = 23.20603\n","epoch 7 batch 0 loss = 22.515322\n","epoch 8 batch 0 loss = 21.794865\n","epoch 9 batch 0 loss = 21.059317\n","epoch 10 batch 0 loss = 20.312042\n","epoch 11 batch 0 loss = 19.556845\n","epoch 12 batch 0 loss = 18.796686\n","epoch 13 batch 0 loss = 18.03404\n","epoch 14 batch 0 loss = 17.274414\n","epoch 15 batch 0 loss = 16.514576\n","epoch 16 batch 0 loss = 15.767704\n","epoch 17 batch 0 loss = 15.034126\n","epoch 18 batch 0 loss = 14.333868\n","epoch 19 batch 0 loss = 13.685624\n","epoch 20 batch 0 loss = 13.106093\n","epoch 21 batch 0 loss = 12.657002\n","epoch 22 batch 0 loss = 12.300315\n","epoch 23 batch 0 loss = 11.99699\n","epoch 24 batch 0 loss = 11.732723\n","epoch 25 batch 0 loss = 11.514692\n","epoch 26 batch 0 loss = 11.33498\n","epoch 27 batch 0 loss = 11.145752\n","epoch 28 batch 0 loss = 10.986856\n","epoch 29 batch 0 loss = 10.832092\n","epoch 30 batch 0 loss = 10.691269\n","epoch 31 batch 0 loss = 10.587598\n","epoch 32 batch 0 loss = 10.511947\n","epoch 33 batch 0 loss = 10.456503\n","epoch 34 batch 0 loss = 10.4159775\n","epoch 35 batch 0 loss = 10.386406\n","epoch 36 batch 0 loss = 10.364843\n","epoch 37 batch 0 loss = 10.349113\n","epoch 38 batch 0 loss = 10.3376255\n","epoch 39 batch 0 loss = 10.329213\n","epoch 40 batch 0 loss = 10.323039\n","epoch 41 batch 0 loss = 10.318486\n","epoch 42 batch 0 loss = 10.315127\n","epoch 43 batch 0 loss = 10.312624\n","epoch 44 batch 0 loss = 10.3107395\n","epoch 45 batch 0 loss = 10.309308\n","epoch 46 batch 0 loss = 10.308205\n","epoch 47 batch 0 loss = 10.307339\n","epoch 48 batch 0 loss = 10.306655\n","epoch 49 batch 0 loss = 10.306101\n","epoch 0 batch 0 loss = 27.15407\n","epoch 1 batch 0 loss = 26.568773\n","epoch 2 batch 0 loss = 25.943075\n","epoch 3 batch 0 loss = 25.310144\n","epoch 4 batch 0 loss = 24.661558\n","epoch 5 batch 0 loss = 23.998182\n","epoch 6 batch 0 loss = 23.329508\n","epoch 7 batch 0 loss = 22.649254\n","epoch 8 batch 0 loss = 21.94499\n","epoch 9 batch 0 loss = 21.185823\n","epoch 10 batch 0 loss = 20.407642\n","epoch 11 batch 0 loss = 19.636526\n","epoch 12 batch 0 loss = 18.864065\n","epoch 13 batch 0 loss = 18.106987\n","epoch 14 batch 0 loss = 17.383194\n","epoch 15 batch 0 loss = 16.689392\n","epoch 16 batch 0 loss = 16.040169\n","epoch 17 batch 0 loss = 15.398972\n","epoch 18 batch 0 loss = 14.807146\n","epoch 19 batch 0 loss = 14.327098\n","epoch 20 batch 0 loss = 13.966726\n","epoch 21 batch 0 loss = 13.658369\n","epoch 22 batch 0 loss = 13.357491\n","epoch 23 batch 0 loss = 13.135223\n","epoch 24 batch 0 loss = 12.971757\n","epoch 25 batch 0 loss = 12.851852\n","epoch 26 batch 0 loss = 12.764059\n","epoch 27 batch 0 loss = 12.699767\n","epoch 28 batch 0 loss = 12.627433\n","epoch 29 batch 0 loss = 12.523936\n","epoch 30 batch 0 loss = 12.448415\n","epoch 31 batch 0 loss = 12.393359\n","epoch 32 batch 0 loss = 12.353266\n","epoch 33 batch 0 loss = 12.324071\n","epoch 34 batch 0 loss = 12.302809\n","epoch 35 batch 0 loss = 12.2873125\n","epoch 36 batch 0 loss = 12.275995\n","epoch 37 batch 0 loss = 12.267708\n","epoch 38 batch 0 loss = 12.261617\n","epoch 39 batch 0 loss = 12.257112\n","epoch 40 batch 0 loss = 12.253756\n","epoch 41 batch 0 loss = 12.251229\n","epoch 42 batch 0 loss = 12.24931\n","epoch 43 batch 0 loss = 12.247826\n","epoch 44 batch 0 loss = 12.246659\n","epoch 45 batch 0 loss = 12.245726\n","epoch 46 batch 0 loss = 12.24496\n","epoch 47 batch 0 loss = 12.244321\n","epoch 48 batch 0 loss = 12.243774\n","epoch 49 batch 0 loss = 12.243296\n","epoch 0 batch 0 loss = 26.607952\n","epoch 1 batch 0 loss = 25.977547\n","epoch 2 batch 0 loss = 25.311962\n","epoch 3 batch 0 loss = 24.63023\n","epoch 4 batch 0 loss = 23.941216\n","epoch 5 batch 0 loss = 23.233606\n","epoch 6 batch 0 loss = 22.521854\n","epoch 7 batch 0 loss = 21.8077\n","epoch 8 batch 0 loss = 21.093084\n","epoch 9 batch 0 loss = 20.3746\n","epoch 10 batch 0 loss = 19.648497\n","epoch 11 batch 0 loss = 18.92423\n","epoch 12 batch 0 loss = 18.209892\n","epoch 13 batch 0 loss = 17.518496\n","epoch 14 batch 0 loss = 16.842533\n","epoch 15 batch 0 loss = 16.194197\n","epoch 16 batch 0 loss = 15.551673\n","epoch 17 batch 0 loss = 14.9584875\n","epoch 18 batch 0 loss = 14.426883\n","epoch 19 batch 0 loss = 14.008417\n","epoch 20 batch 0 loss = 13.689181\n","epoch 21 batch 0 loss = 13.431193\n","epoch 22 batch 0 loss = 13.156215\n","epoch 23 batch 0 loss = 12.908032\n","epoch 24 batch 0 loss = 12.673554\n","epoch 25 batch 0 loss = 12.495678\n","epoch 26 batch 0 loss = 12.361719\n","epoch 27 batch 0 loss = 12.261236\n","epoch 28 batch 0 loss = 12.186022\n","epoch 29 batch 0 loss = 12.129761\n","epoch 30 batch 0 loss = 12.087671\n","epoch 31 batch 0 loss = 12.056171\n","epoch 32 batch 0 loss = 12.032577\n","epoch 33 batch 0 loss = 12.014891\n","epoch 34 batch 0 loss = 12.001619\n","epoch 35 batch 0 loss = 11.991651\n","epoch 36 batch 0 loss = 11.984158\n","epoch 37 batch 0 loss = 11.978517\n","epoch 38 batch 0 loss = 11.974265\n","epoch 39 batch 0 loss = 11.971059\n","epoch 40 batch 0 loss = 11.968634\n","epoch 41 batch 0 loss = 11.966799\n","epoch 42 batch 0 loss = 11.965406\n","epoch 43 batch 0 loss = 11.964347\n","epoch 44 batch 0 loss = 11.963536\n","epoch 45 batch 0 loss = 11.962915\n","epoch 46 batch 0 loss = 11.962438\n","epoch 47 batch 0 loss = 11.962066\n","epoch 48 batch 0 loss = 11.961777\n","epoch 49 batch 0 loss = 11.961547\n","epoch 0 batch 0 loss = 28.22957\n","epoch 1 batch 0 loss = 27.754217\n","epoch 2 batch 0 loss = 27.263582\n","epoch 3 batch 0 loss = 26.704994\n","epoch 4 batch 0 loss = 26.12156\n","epoch 5 batch 0 loss = 25.537434\n","epoch 6 batch 0 loss = 24.931248\n","epoch 7 batch 0 loss = 24.30111\n","epoch 8 batch 0 loss = 23.63232\n","epoch 9 batch 0 loss = 22.947872\n","epoch 10 batch 0 loss = 22.256971\n","epoch 11 batch 0 loss = 21.558344\n","epoch 12 batch 0 loss = 20.869625\n","epoch 13 batch 0 loss = 20.191639\n","epoch 14 batch 0 loss = 19.519299\n","epoch 15 batch 0 loss = 18.869066\n","epoch 16 batch 0 loss = 18.220747\n","epoch 17 batch 0 loss = 17.611588\n","epoch 18 batch 0 loss = 17.09036\n","epoch 19 batch 0 loss = 16.708828\n","epoch 20 batch 0 loss = 16.440899\n","epoch 21 batch 0 loss = 16.257318\n","epoch 22 batch 0 loss = 16.131845\n","epoch 23 batch 0 loss = 16.04551\n","epoch 24 batch 0 loss = 15.985202\n","epoch 25 batch 0 loss = 15.942515\n","epoch 26 batch 0 loss = 15.911994\n","epoch 27 batch 0 loss = 15.890013\n","epoch 28 batch 0 loss = 15.874099\n","epoch 29 batch 0 loss = 15.862521\n","epoch 30 batch 0 loss = 15.854065\n","epoch 31 batch 0 loss = 15.847872\n","epoch 32 batch 0 loss = 15.843316\n","epoch 33 batch 0 loss = 15.83994\n","epoch 34 batch 0 loss = 15.837421\n","epoch 35 batch 0 loss = 15.835529\n","epoch 36 batch 0 loss = 15.83409\n","epoch 37 batch 0 loss = 15.832987\n","epoch 38 batch 0 loss = 15.832124\n","epoch 39 batch 0 loss = 15.831442\n","epoch 40 batch 0 loss = 15.830891\n","epoch 41 batch 0 loss = 15.830441\n","epoch 42 batch 0 loss = 15.830065\n","epoch 43 batch 0 loss = 15.829742\n","epoch 44 batch 0 loss = 15.82946\n","epoch 45 batch 0 loss = 15.829212\n","epoch 46 batch 0 loss = 15.828987\n","epoch 47 batch 0 loss = 15.828782\n","epoch 48 batch 0 loss = 15.828592\n","epoch 49 batch 0 loss = 15.828414\n","epoch 0 batch 0 loss = 26.98872\n","epoch 1 batch 0 loss = 26.45994\n","epoch 2 batch 0 loss = 25.889477\n","epoch 3 batch 0 loss = 25.263105\n","epoch 4 batch 0 loss = 24.598072\n","epoch 5 batch 0 loss = 23.901934\n","epoch 6 batch 0 loss = 23.188383\n","epoch 7 batch 0 loss = 22.443449\n","epoch 8 batch 0 loss = 21.674019\n","epoch 9 batch 0 loss = 20.893341\n","epoch 10 batch 0 loss = 20.110632\n","epoch 11 batch 0 loss = 19.323387\n","epoch 12 batch 0 loss = 18.534527\n","epoch 13 batch 0 loss = 17.763588\n","epoch 14 batch 0 loss = 17.021065\n","epoch 15 batch 0 loss = 16.31443\n","epoch 16 batch 0 loss = 15.635424\n","epoch 17 batch 0 loss = 14.9897175\n","epoch 18 batch 0 loss = 14.397843\n","epoch 19 batch 0 loss = 13.929591\n","epoch 20 batch 0 loss = 13.584401\n","epoch 21 batch 0 loss = 13.294243\n","epoch 22 batch 0 loss = 13.0675\n","epoch 23 batch 0 loss = 12.908388\n","epoch 24 batch 0 loss = 12.796864\n","epoch 25 batch 0 loss = 12.71853\n","epoch 26 batch 0 loss = 12.6629\n","epoch 27 batch 0 loss = 12.623328\n","epoch 28 batch 0 loss = 12.5950165\n","epoch 29 batch 0 loss = 12.574691\n","epoch 30 batch 0 loss = 12.560058\n","epoch 31 batch 0 loss = 12.549467\n","epoch 32 batch 0 loss = 12.541757\n","epoch 33 batch 0 loss = 12.536105\n","epoch 34 batch 0 loss = 12.531923\n","epoch 35 batch 0 loss = 12.5287895\n","epoch 36 batch 0 loss = 12.526416\n","epoch 37 batch 0 loss = 12.524586\n","epoch 38 batch 0 loss = 12.523148\n","epoch 39 batch 0 loss = 12.521996\n","epoch 40 batch 0 loss = 12.521057\n","epoch 41 batch 0 loss = 12.520271\n","epoch 42 batch 0 loss = 12.519598\n","epoch 43 batch 0 loss = 12.519017\n","epoch 44 batch 0 loss = 12.518501\n","epoch 45 batch 0 loss = 12.518037\n","epoch 46 batch 0 loss = 12.517613\n","epoch 47 batch 0 loss = 12.517224\n","epoch 48 batch 0 loss = 12.51686\n","epoch 49 batch 0 loss = 12.51652\n","epoch 0 batch 0 loss = 27.686613\n","epoch 1 batch 0 loss = 27.237371\n","epoch 2 batch 0 loss = 26.736725\n","epoch 3 batch 0 loss = 26.240908\n","epoch 4 batch 0 loss = 25.745779\n","epoch 5 batch 0 loss = 25.245117\n","epoch 6 batch 0 loss = 24.713308\n","epoch 7 batch 0 loss = 24.110796\n","epoch 8 batch 0 loss = 23.476091\n","epoch 9 batch 0 loss = 22.835003\n","epoch 10 batch 0 loss = 22.185799\n","epoch 11 batch 0 loss = 21.530426\n","epoch 12 batch 0 loss = 20.87109\n","epoch 13 batch 0 loss = 20.211124\n","epoch 14 batch 0 loss = 19.554884\n","epoch 15 batch 0 loss = 18.906908\n","epoch 16 batch 0 loss = 18.248257\n","epoch 17 batch 0 loss = 17.600992\n","epoch 18 batch 0 loss = 16.97365\n","epoch 19 batch 0 loss = 16.373476\n","epoch 20 batch 0 loss = 15.85529\n","epoch 21 batch 0 loss = 15.423337\n","epoch 22 batch 0 loss = 15.074864\n","epoch 23 batch 0 loss = 14.782225\n","epoch 24 batch 0 loss = 14.494272\n","epoch 25 batch 0 loss = 14.231435\n","epoch 26 batch 0 loss = 14.026472\n","epoch 27 batch 0 loss = 13.867957\n","epoch 28 batch 0 loss = 13.746022\n","epoch 29 batch 0 loss = 13.652426\n","epoch 30 batch 0 loss = 13.580597\n","epoch 31 batch 0 loss = 13.525522\n","epoch 32 batch 0 loss = 13.483272\n","epoch 33 batch 0 loss = 13.450866\n","epoch 34 batch 0 loss = 13.426058\n","epoch 35 batch 0 loss = 13.407048\n","epoch 36 batch 0 loss = 13.392479\n","epoch 37 batch 0 loss = 13.381292\n","epoch 38 batch 0 loss = 13.372694\n","epoch 39 batch 0 loss = 13.366108\n","epoch 40 batch 0 loss = 13.36105\n","epoch 41 batch 0 loss = 13.3571615\n","epoch 42 batch 0 loss = 13.35417\n","epoch 43 batch 0 loss = 13.351861\n","epoch 44 batch 0 loss = 13.350079\n","epoch 45 batch 0 loss = 13.348699\n","epoch 46 batch 0 loss = 13.347625\n","epoch 47 batch 0 loss = 13.346787\n","epoch 48 batch 0 loss = 13.346133\n","epoch 49 batch 0 loss = 13.345616\n","epoch 0 batch 0 loss = 26.504318\n","epoch 1 batch 0 loss = 25.983946\n","epoch 2 batch 0 loss = 25.445211\n","epoch 3 batch 0 loss = 24.913694\n","epoch 4 batch 0 loss = 24.386\n","epoch 5 batch 0 loss = 23.853954\n","epoch 6 batch 0 loss = 23.28018\n","epoch 7 batch 0 loss = 22.671652\n","epoch 8 batch 0 loss = 22.039446\n","epoch 9 batch 0 loss = 21.401995\n","epoch 10 batch 0 loss = 20.750301\n","epoch 11 batch 0 loss = 20.088053\n","epoch 12 batch 0 loss = 19.429\n","epoch 13 batch 0 loss = 18.776138\n","epoch 14 batch 0 loss = 18.12869\n","epoch 15 batch 0 loss = 17.486475\n","epoch 16 batch 0 loss = 16.857868\n","epoch 17 batch 0 loss = 16.23052\n","epoch 18 batch 0 loss = 15.612413\n","epoch 19 batch 0 loss = 15.040971\n","epoch 20 batch 0 loss = 14.528664\n","epoch 21 batch 0 loss = 14.0778\n","epoch 22 batch 0 loss = 13.687654\n","epoch 23 batch 0 loss = 13.342995\n","epoch 24 batch 0 loss = 13.025196\n","epoch 25 batch 0 loss = 12.747552\n","epoch 26 batch 0 loss = 12.4899\n","epoch 27 batch 0 loss = 12.278539\n","epoch 28 batch 0 loss = 12.106785\n","epoch 29 batch 0 loss = 11.96824\n","epoch 30 batch 0 loss = 11.857079\n","epoch 31 batch 0 loss = 11.768189\n","epoch 32 batch 0 loss = 11.697229\n","epoch 33 batch 0 loss = 11.640601\n","epoch 34 batch 0 loss = 11.595371\n","epoch 35 batch 0 loss = 11.559191\n","epoch 36 batch 0 loss = 11.530191\n","epoch 37 batch 0 loss = 11.506898\n","epoch 38 batch 0 loss = 11.488135\n","epoch 39 batch 0 loss = 11.472993\n","epoch 40 batch 0 loss = 11.460744\n","epoch 41 batch 0 loss = 11.450816\n","epoch 42 batch 0 loss = 11.442753\n","epoch 43 batch 0 loss = 11.436196\n","epoch 44 batch 0 loss = 11.430854\n","epoch 45 batch 0 loss = 11.4265\n","epoch 46 batch 0 loss = 11.422945\n","epoch 47 batch 0 loss = 11.42004\n","epoch 48 batch 0 loss = 11.417666\n","epoch 49 batch 0 loss = 11.415723\n","epoch 0 batch 0 loss = 26.807623\n","epoch 1 batch 0 loss = 25.912119\n","epoch 2 batch 0 loss = 24.99292\n","epoch 3 batch 0 loss = 24.057596\n","epoch 4 batch 0 loss = 23.10146\n","epoch 5 batch 0 loss = 22.129787\n","epoch 6 batch 0 loss = 21.151318\n","epoch 7 batch 0 loss = 20.186363\n","epoch 8 batch 0 loss = 19.254093\n","epoch 9 batch 0 loss = 18.36556\n","epoch 10 batch 0 loss = 17.540844\n","epoch 11 batch 0 loss = 16.777864\n","epoch 12 batch 0 loss = 16.073204\n","epoch 13 batch 0 loss = 15.499179\n","epoch 14 batch 0 loss = 15.151637\n","epoch 15 batch 0 loss = 14.965391\n","epoch 16 batch 0 loss = 14.827687\n","epoch 17 batch 0 loss = 14.669088\n","epoch 18 batch 0 loss = 14.578545\n","epoch 19 batch 0 loss = 14.527339\n","epoch 20 batch 0 loss = 14.498434\n","epoch 21 batch 0 loss = 14.481995\n","epoch 22 batch 0 loss = 14.472505\n","epoch 23 batch 0 loss = 14.466895\n","epoch 24 batch 0 loss = 14.4634695\n","epoch 25 batch 0 loss = 14.4612875\n","epoch 26 batch 0 loss = 14.459812\n","epoch 27 batch 0 loss = 14.458751\n","epoch 28 batch 0 loss = 14.457931\n","epoch 29 batch 0 loss = 14.457257\n","epoch 30 batch 0 loss = 14.456677\n","epoch 31 batch 0 loss = 14.456159\n","epoch 32 batch 0 loss = 14.455673\n","epoch 33 batch 0 loss = 14.455223\n","epoch 34 batch 0 loss = 14.454796\n","epoch 35 batch 0 loss = 14.454392\n","epoch 36 batch 0 loss = 14.454006\n","epoch 37 batch 0 loss = 14.453635\n","epoch 38 batch 0 loss = 14.45328\n","epoch 39 batch 0 loss = 14.45294\n","epoch 40 batch 0 loss = 14.452613\n","epoch 41 batch 0 loss = 14.452299\n","epoch 42 batch 0 loss = 14.451997\n","epoch 43 batch 0 loss = 14.451707\n","epoch 44 batch 0 loss = 14.451427\n","epoch 45 batch 0 loss = 14.451159\n","epoch 46 batch 0 loss = 14.450897\n","epoch 47 batch 0 loss = 14.450646\n","epoch 48 batch 0 loss = 14.450403\n","epoch 49 batch 0 loss = 14.450169\n","epoch 0 batch 0 loss = 26.464182\n","epoch 1 batch 0 loss = 25.68469\n","epoch 2 batch 0 loss = 24.895876\n","epoch 3 batch 0 loss = 24.115335\n","epoch 4 batch 0 loss = 23.343716\n","epoch 5 batch 0 loss = 22.582716\n","epoch 6 batch 0 loss = 21.834896\n","epoch 7 batch 0 loss = 21.10382\n","epoch 8 batch 0 loss = 20.393139\n","epoch 9 batch 0 loss = 19.696037\n","epoch 10 batch 0 loss = 18.993443\n","epoch 11 batch 0 loss = 18.312195\n","epoch 12 batch 0 loss = 17.655127\n","epoch 13 batch 0 loss = 17.036695\n","epoch 14 batch 0 loss = 16.488401\n","epoch 15 batch 0 loss = 16.015547\n","epoch 16 batch 0 loss = 15.6501255\n","epoch 17 batch 0 loss = 15.377274\n","epoch 18 batch 0 loss = 15.1793375\n","epoch 19 batch 0 loss = 15.039415\n","epoch 20 batch 0 loss = 14.942327\n","epoch 21 batch 0 loss = 14.875774\n","epoch 22 batch 0 loss = 14.830469\n","epoch 23 batch 0 loss = 14.79954\n","epoch 24 batch 0 loss = 14.77825\n","epoch 25 batch 0 loss = 14.763442\n","epoch 26 batch 0 loss = 14.753031\n","epoch 27 batch 0 loss = 14.74563\n","epoch 28 batch 0 loss = 14.740317\n","epoch 29 batch 0 loss = 14.736474\n","epoch 30 batch 0 loss = 14.733673\n","epoch 31 batch 0 loss = 14.73162\n","epoch 32 batch 0 loss = 14.730109\n","epoch 33 batch 0 loss = 14.7289915\n","epoch 34 batch 0 loss = 14.728163\n","epoch 35 batch 0 loss = 14.727549\n","epoch 36 batch 0 loss = 14.727089\n","epoch 37 batch 0 loss = 14.726745\n","epoch 38 batch 0 loss = 14.726485\n","epoch 39 batch 0 loss = 14.726291\n","epoch 40 batch 0 loss = 14.726142\n","epoch 41 batch 0 loss = 14.726029\n","epoch 42 batch 0 loss = 14.725941\n","epoch 43 batch 0 loss = 14.725874\n","epoch 44 batch 0 loss = 14.725821\n","epoch 45 batch 0 loss = 14.725778\n","epoch 46 batch 0 loss = 14.725742\n","epoch 47 batch 0 loss = 14.725714\n","epoch 48 batch 0 loss = 14.725689\n","epoch 49 batch 0 loss = 14.725669\n","epoch 0 batch 0 loss = 27.466818\n","epoch 1 batch 0 loss = 26.786173\n","epoch 2 batch 0 loss = 26.103271\n","epoch 3 batch 0 loss = 25.400421\n","epoch 4 batch 0 loss = 24.677107\n","epoch 5 batch 0 loss = 23.944109\n","epoch 6 batch 0 loss = 23.189074\n","epoch 7 batch 0 loss = 22.408228\n","epoch 8 batch 0 loss = 21.593765\n","epoch 9 batch 0 loss = 20.755388\n","epoch 10 batch 0 loss = 19.91012\n","epoch 11 batch 0 loss = 19.059214\n","epoch 12 batch 0 loss = 18.207945\n","epoch 13 batch 0 loss = 17.34674\n","epoch 14 batch 0 loss = 16.46761\n","epoch 15 batch 0 loss = 15.596174\n","epoch 16 batch 0 loss = 14.709503\n","epoch 17 batch 0 loss = 13.839781\n","epoch 18 batch 0 loss = 13.008676\n","epoch 19 batch 0 loss = 12.228109\n","epoch 20 batch 0 loss = 11.497923\n","epoch 21 batch 0 loss = 10.851529\n","epoch 22 batch 0 loss = 10.280396\n","epoch 23 batch 0 loss = 9.889542\n","epoch 24 batch 0 loss = 9.652658\n","epoch 25 batch 0 loss = 9.508531\n","epoch 26 batch 0 loss = 9.414483\n","epoch 27 batch 0 loss = 9.350563\n","epoch 28 batch 0 loss = 9.306147\n","epoch 29 batch 0 loss = 9.27502\n","epoch 30 batch 0 loss = 9.253114\n","epoch 31 batch 0 loss = 9.237635\n","epoch 32 batch 0 loss = 9.226649\n","epoch 33 batch 0 loss = 9.218828\n","epoch 34 batch 0 loss = 9.21324\n","epoch 35 batch 0 loss = 9.209205\n","epoch 36 batch 0 loss = 9.206268\n","epoch 37 batch 0 loss = 9.20411\n","epoch 38 batch 0 loss = 9.202496\n","epoch 39 batch 0 loss = 9.201264\n","epoch 40 batch 0 loss = 9.200307\n","epoch 41 batch 0 loss = 9.199547\n","epoch 42 batch 0 loss = 9.198934\n","epoch 43 batch 0 loss = 9.1984215\n","epoch 44 batch 0 loss = 9.197981\n","epoch 45 batch 0 loss = 9.197594\n","epoch 46 batch 0 loss = 9.197244\n","epoch 47 batch 0 loss = 9.196926\n","epoch 48 batch 0 loss = 9.196635\n","epoch 49 batch 0 loss = 9.196361\n","epoch 0 batch 0 loss = 28.042871\n","epoch 1 batch 0 loss = 27.402178\n","epoch 2 batch 0 loss = 26.751318\n","epoch 3 batch 0 loss = 26.08883\n","epoch 4 batch 0 loss = 25.391193\n","epoch 5 batch 0 loss = 24.66338\n","epoch 6 batch 0 loss = 23.916286\n","epoch 7 batch 0 loss = 23.164742\n","epoch 8 batch 0 loss = 22.397121\n","epoch 9 batch 0 loss = 21.58591\n","epoch 10 batch 0 loss = 20.772099\n","epoch 11 batch 0 loss = 19.955471\n","epoch 12 batch 0 loss = 19.139133\n","epoch 13 batch 0 loss = 18.325802\n","epoch 14 batch 0 loss = 17.53927\n","epoch 15 batch 0 loss = 16.787075\n","epoch 16 batch 0 loss = 16.057392\n","epoch 17 batch 0 loss = 15.381735\n","epoch 18 batch 0 loss = 14.792807\n","epoch 19 batch 0 loss = 14.320442\n","epoch 20 batch 0 loss = 13.963866\n","epoch 21 batch 0 loss = 13.668464\n","epoch 22 batch 0 loss = 13.41897\n","epoch 23 batch 0 loss = 13.222222\n","epoch 24 batch 0 loss = 13.087321\n","epoch 25 batch 0 loss = 12.995083\n","epoch 26 batch 0 loss = 12.931899\n","epoch 27 batch 0 loss = 12.857516\n","epoch 28 batch 0 loss = 12.749314\n","epoch 29 batch 0 loss = 12.674922\n","epoch 30 batch 0 loss = 12.623996\n","epoch 31 batch 0 loss = 12.589237\n","epoch 32 batch 0 loss = 12.565566\n","epoch 33 batch 0 loss = 12.54946\n","epoch 34 batch 0 loss = 12.538498\n","epoch 35 batch 0 loss = 12.531024\n","epoch 36 batch 0 loss = 12.525913\n","epoch 37 batch 0 loss = 12.522405\n","epoch 38 batch 0 loss = 12.51998\n","epoch 39 batch 0 loss = 12.518288\n","epoch 40 batch 0 loss = 12.517091\n","epoch 41 batch 0 loss = 12.51623\n","epoch 42 batch 0 loss = 12.515597\n","epoch 43 batch 0 loss = 12.515119\n","epoch 44 batch 0 loss = 12.514743\n","epoch 45 batch 0 loss = 12.51444\n","epoch 46 batch 0 loss = 12.514186\n","epoch 47 batch 0 loss = 12.513966\n","epoch 48 batch 0 loss = 12.513772\n","epoch 49 batch 0 loss = 12.513596\n","epoch 0 batch 0 loss = 26.997065\n","epoch 1 batch 0 loss = 26.23721\n","epoch 2 batch 0 loss = 25.449362\n","epoch 3 batch 0 loss = 24.625534\n","epoch 4 batch 0 loss = 23.776043\n","epoch 5 batch 0 loss = 22.893156\n","epoch 6 batch 0 loss = 21.984282\n","epoch 7 batch 0 loss = 21.066988\n","epoch 8 batch 0 loss = 20.14497\n","epoch 9 batch 0 loss = 19.224604\n","epoch 10 batch 0 loss = 18.313282\n","epoch 11 batch 0 loss = 17.421282\n","epoch 12 batch 0 loss = 16.543856\n","epoch 13 batch 0 loss = 15.717415\n","epoch 14 batch 0 loss = 14.966775\n","epoch 15 batch 0 loss = 14.317021\n","epoch 16 batch 0 loss = 13.817621\n","epoch 17 batch 0 loss = 13.519866\n","epoch 18 batch 0 loss = 13.348701\n","epoch 19 batch 0 loss = 13.247755\n","epoch 20 batch 0 loss = 13.185559\n","epoch 21 batch 0 loss = 13.145818\n","epoch 22 batch 0 loss = 13.119835\n","epoch 23 batch 0 loss = 13.102689\n","epoch 24 batch 0 loss = 13.091306\n","epoch 25 batch 0 loss = 13.083699\n","epoch 26 batch 0 loss = 13.078587\n","epoch 27 batch 0 loss = 13.075118\n","epoch 28 batch 0 loss = 13.072747\n","epoch 29 batch 0 loss = 13.071099\n","epoch 30 batch 0 loss = 13.069936\n","epoch 31 batch 0 loss = 13.069096\n","epoch 32 batch 0 loss = 13.068471\n","epoch 33 batch 0 loss = 13.067991\n","epoch 34 batch 0 loss = 13.067608\n","epoch 35 batch 0 loss = 13.067293\n","epoch 36 batch 0 loss = 13.067021\n","epoch 37 batch 0 loss = 13.066783\n","epoch 38 batch 0 loss = 13.066565\n","epoch 39 batch 0 loss = 13.066362\n","epoch 40 batch 0 loss = 13.066175\n","epoch 41 batch 0 loss = 13.065994\n","epoch 42 batch 0 loss = 13.065822\n","epoch 43 batch 0 loss = 13.065653\n","epoch 44 batch 0 loss = 13.06549\n","epoch 45 batch 0 loss = 13.065332\n","epoch 46 batch 0 loss = 13.065174\n","epoch 47 batch 0 loss = 13.065019\n","epoch 48 batch 0 loss = 13.064867\n","epoch 49 batch 0 loss = 13.064722\n","epoch 0 batch 0 loss = 27.960861\n","epoch 1 batch 0 loss = 27.32543\n","epoch 2 batch 0 loss = 26.650097\n","epoch 3 batch 0 loss = 25.936127\n","epoch 4 batch 0 loss = 25.21544\n","epoch 5 batch 0 loss = 24.475836\n","epoch 6 batch 0 loss = 23.699524\n","epoch 7 batch 0 loss = 22.880339\n","epoch 8 batch 0 loss = 22.017103\n","epoch 9 batch 0 loss = 21.130817\n","epoch 10 batch 0 loss = 20.223783\n","epoch 11 batch 0 loss = 19.292883\n","epoch 12 batch 0 loss = 18.336166\n","epoch 13 batch 0 loss = 17.394701\n","epoch 14 batch 0 loss = 16.469423\n","epoch 15 batch 0 loss = 15.589163\n","epoch 16 batch 0 loss = 14.753564\n","epoch 17 batch 0 loss = 13.993131\n","epoch 18 batch 0 loss = 13.362927\n","epoch 19 batch 0 loss = 12.865242\n","epoch 20 batch 0 loss = 12.538853\n","epoch 21 batch 0 loss = 12.3364105\n","epoch 22 batch 0 loss = 12.209417\n","epoch 23 batch 0 loss = 12.129448\n","epoch 24 batch 0 loss = 12.078838\n","epoch 25 batch 0 loss = 12.015649\n","epoch 26 batch 0 loss = 11.9058075\n","epoch 27 batch 0 loss = 11.834849\n","epoch 28 batch 0 loss = 11.789182\n","epoch 29 batch 0 loss = 11.759801\n","epoch 30 batch 0 loss = 11.740839\n","epoch 31 batch 0 loss = 11.728495\n","epoch 32 batch 0 loss = 11.720337\n","epoch 33 batch 0 loss = 11.714822\n","epoch 34 batch 0 loss = 11.710971\n","epoch 35 batch 0 loss = 11.708179\n","epoch 36 batch 0 loss = 11.706068\n","epoch 37 batch 0 loss = 11.704388\n","epoch 38 batch 0 loss = 11.702999\n","epoch 39 batch 0 loss = 11.701802\n","epoch 40 batch 0 loss = 11.700738\n","epoch 41 batch 0 loss = 11.699775\n","epoch 42 batch 0 loss = 11.698888\n","epoch 43 batch 0 loss = 11.698062\n","epoch 44 batch 0 loss = 11.697288\n","epoch 45 batch 0 loss = 11.696556\n","epoch 46 batch 0 loss = 11.695864\n","epoch 47 batch 0 loss = 11.695206\n","epoch 48 batch 0 loss = 11.694577\n","epoch 49 batch 0 loss = 11.693981\n","epoch 0 batch 0 loss = 27.630112\n","epoch 1 batch 0 loss = 26.857122\n","epoch 2 batch 0 loss = 26.07469\n","epoch 3 batch 0 loss = 25.275883\n","epoch 4 batch 0 loss = 24.480724\n","epoch 5 batch 0 loss = 23.685297\n","epoch 6 batch 0 loss = 22.873426\n","epoch 7 batch 0 loss = 22.049788\n","epoch 8 batch 0 loss = 21.22533\n","epoch 9 batch 0 loss = 20.401525\n","epoch 10 batch 0 loss = 19.579958\n","epoch 11 batch 0 loss = 18.759161\n","epoch 12 batch 0 loss = 17.932714\n","epoch 13 batch 0 loss = 17.110292\n","epoch 14 batch 0 loss = 16.29777\n","epoch 15 batch 0 loss = 15.511517\n","epoch 16 batch 0 loss = 14.764751\n","epoch 17 batch 0 loss = 14.073506\n","epoch 18 batch 0 loss = 13.430001\n","epoch 19 batch 0 loss = 12.879275\n","epoch 20 batch 0 loss = 12.461837\n","epoch 21 batch 0 loss = 12.13099\n","epoch 22 batch 0 loss = 11.904954\n","epoch 23 batch 0 loss = 11.752474\n","epoch 24 batch 0 loss = 11.624233\n","epoch 25 batch 0 loss = 11.476253\n","epoch 26 batch 0 loss = 11.372448\n","epoch 27 batch 0 loss = 11.299923\n","epoch 28 batch 0 loss = 11.249324\n","epoch 29 batch 0 loss = 11.2140255\n","epoch 30 batch 0 loss = 11.189383\n","epoch 31 batch 0 loss = 11.172153\n","epoch 32 batch 0 loss = 11.1600895\n","epoch 33 batch 0 loss = 11.151623\n","epoch 34 batch 0 loss = 11.145664\n","epoch 35 batch 0 loss = 11.1414585\n","epoch 36 batch 0 loss = 11.138478\n","epoch 37 batch 0 loss = 11.136357\n","epoch 38 batch 0 loss = 11.134837\n","epoch 39 batch 0 loss = 11.1337385\n","epoch 40 batch 0 loss = 11.1329365\n","epoch 41 batch 0 loss = 11.132341\n","epoch 42 batch 0 loss = 11.131893\n","epoch 43 batch 0 loss = 11.131548\n","epoch 44 batch 0 loss = 11.131276\n","epoch 45 batch 0 loss = 11.131056\n","epoch 46 batch 0 loss = 11.130872\n","epoch 47 batch 0 loss = 11.130714\n","epoch 48 batch 0 loss = 11.130576\n","epoch 49 batch 0 loss = 11.130451\n","epoch 0 batch 0 loss = 27.829632\n","epoch 1 batch 0 loss = 27.168894\n","epoch 2 batch 0 loss = 26.497734\n","epoch 3 batch 0 loss = 25.820122\n","epoch 4 batch 0 loss = 25.128063\n","epoch 5 batch 0 loss = 24.42009\n","epoch 6 batch 0 loss = 23.703856\n","epoch 7 batch 0 loss = 22.976284\n","epoch 8 batch 0 loss = 22.230719\n","epoch 9 batch 0 loss = 21.469448\n","epoch 10 batch 0 loss = 20.705729\n","epoch 11 batch 0 loss = 19.93868\n","epoch 12 batch 0 loss = 19.17252\n","epoch 13 batch 0 loss = 18.423786\n","epoch 14 batch 0 loss = 17.702728\n","epoch 15 batch 0 loss = 17.021648\n","epoch 16 batch 0 loss = 16.39527\n","epoch 17 batch 0 loss = 15.8652935\n","epoch 18 batch 0 loss = 15.451127\n","epoch 19 batch 0 loss = 15.118557\n","epoch 20 batch 0 loss = 14.849266\n","epoch 21 batch 0 loss = 14.640874\n","epoch 22 batch 0 loss = 14.424321\n","epoch 23 batch 0 loss = 14.1969795\n","epoch 24 batch 0 loss = 14.031257\n","epoch 25 batch 0 loss = 13.912158\n","epoch 26 batch 0 loss = 13.827001\n","epoch 27 batch 0 loss = 13.766296\n","epoch 28 batch 0 loss = 13.72319\n","epoch 29 batch 0 loss = 13.6925955\n","epoch 30 batch 0 loss = 13.670886\n","epoch 31 batch 0 loss = 13.655487\n","epoch 32 batch 0 loss = 13.644557\n","epoch 33 batch 0 loss = 13.636801\n","epoch 34 batch 0 loss = 13.631291\n","epoch 35 batch 0 loss = 13.627376\n","epoch 36 batch 0 loss = 13.624589\n","epoch 37 batch 0 loss = 13.6226015\n","epoch 38 batch 0 loss = 13.621178\n","epoch 39 batch 0 loss = 13.620157\n","epoch 40 batch 0 loss = 13.619423\n","epoch 41 batch 0 loss = 13.61889\n","epoch 42 batch 0 loss = 13.618499\n","epoch 43 batch 0 loss = 13.618213\n","epoch 44 batch 0 loss = 13.617999\n","epoch 45 batch 0 loss = 13.617835\n","epoch 46 batch 0 loss = 13.617709\n","epoch 47 batch 0 loss = 13.617608\n","epoch 48 batch 0 loss = 13.617527\n","epoch 49 batch 0 loss = 13.617459\n","epoch 0 batch 0 loss = 28.261896\n","epoch 1 batch 0 loss = 27.629036\n","epoch 2 batch 0 loss = 26.989607\n","epoch 3 batch 0 loss = 26.322052\n","epoch 4 batch 0 loss = 25.605713\n","epoch 5 batch 0 loss = 24.846365\n","epoch 6 batch 0 loss = 24.059895\n","epoch 7 batch 0 loss = 23.241034\n","epoch 8 batch 0 loss = 22.39879\n","epoch 9 batch 0 loss = 21.52891\n","epoch 10 batch 0 loss = 20.632557\n","epoch 11 batch 0 loss = 19.718586\n","epoch 12 batch 0 loss = 18.796204\n","epoch 13 batch 0 loss = 17.859707\n","epoch 14 batch 0 loss = 16.925629\n","epoch 15 batch 0 loss = 16.00688\n","epoch 16 batch 0 loss = 15.1171875\n","epoch 17 batch 0 loss = 14.263979\n","epoch 18 batch 0 loss = 13.485228\n","epoch 19 batch 0 loss = 12.809187\n","epoch 20 batch 0 loss = 12.266653\n","epoch 21 batch 0 loss = 11.945383\n","epoch 22 batch 0 loss = 11.758233\n","epoch 23 batch 0 loss = 11.644082\n","epoch 24 batch 0 loss = 11.570508\n","epoch 25 batch 0 loss = 11.521319\n","epoch 26 batch 0 loss = 11.487826\n","epoch 27 batch 0 loss = 11.436902\n","epoch 28 batch 0 loss = 11.345419\n","epoch 29 batch 0 loss = 11.281219\n","epoch 30 batch 0 loss = 11.236337\n","epoch 31 batch 0 loss = 11.205063\n","epoch 32 batch 0 loss = 11.183315\n","epoch 33 batch 0 loss = 11.168216\n","epoch 34 batch 0 loss = 11.157716\n","epoch 35 batch 0 loss = 11.150379\n","epoch 36 batch 0 loss = 11.145249\n","epoch 37 batch 0 loss = 11.141632\n","epoch 38 batch 0 loss = 11.139059\n","epoch 39 batch 0 loss = 11.137206\n","epoch 40 batch 0 loss = 11.135843\n","epoch 41 batch 0 loss = 11.134822\n","epoch 42 batch 0 loss = 11.134035\n","epoch 43 batch 0 loss = 11.133412\n","epoch 44 batch 0 loss = 11.1329\n","epoch 45 batch 0 loss = 11.132472\n","epoch 46 batch 0 loss = 11.132099\n","epoch 47 batch 0 loss = 11.131768\n","epoch 48 batch 0 loss = 11.13147\n","epoch 49 batch 0 loss = 11.131193\n","epoch 0 batch 0 loss = 27.120903\n","epoch 1 batch 0 loss = 26.045488\n","epoch 2 batch 0 loss = 25.009865\n","epoch 3 batch 0 loss = 24.010508\n","epoch 4 batch 0 loss = 23.047035\n","epoch 5 batch 0 loss = 22.117846\n","epoch 6 batch 0 loss = 21.224983\n","epoch 7 batch 0 loss = 20.36407\n","epoch 8 batch 0 loss = 19.534882\n","epoch 9 batch 0 loss = 18.757967\n","epoch 10 batch 0 loss = 18.025763\n","epoch 11 batch 0 loss = 17.343649\n","epoch 12 batch 0 loss = 16.72301\n","epoch 13 batch 0 loss = 16.176819\n","epoch 14 batch 0 loss = 15.687981\n","epoch 15 batch 0 loss = 15.310181\n","epoch 16 batch 0 loss = 15.035103\n","epoch 17 batch 0 loss = 14.799795\n","epoch 18 batch 0 loss = 14.65724\n","epoch 19 batch 0 loss = 14.572534\n","epoch 20 batch 0 loss = 14.522368\n","epoch 21 batch 0 loss = 14.492506\n","epoch 22 batch 0 loss = 14.474607\n","epoch 23 batch 0 loss = 14.463856\n","epoch 24 batch 0 loss = 14.4573555\n","epoch 25 batch 0 loss = 14.453396\n","epoch 26 batch 0 loss = 14.450964\n","epoch 27 batch 0 loss = 14.449454\n","epoch 28 batch 0 loss = 14.4485035\n","epoch 29 batch 0 loss = 14.447891\n","epoch 30 batch 0 loss = 14.447486\n","epoch 31 batch 0 loss = 14.4472065\n","epoch 32 batch 0 loss = 14.447002\n","epoch 33 batch 0 loss = 14.446846\n","epoch 34 batch 0 loss = 14.446719\n","epoch 35 batch 0 loss = 14.44661\n","epoch 36 batch 0 loss = 14.446513\n","epoch 37 batch 0 loss = 14.4464245\n","epoch 38 batch 0 loss = 14.4463415\n","epoch 39 batch 0 loss = 14.44626\n","epoch 40 batch 0 loss = 14.446184\n","epoch 41 batch 0 loss = 14.446109\n","epoch 42 batch 0 loss = 14.446036\n","epoch 43 batch 0 loss = 14.445965\n","epoch 44 batch 0 loss = 14.445895\n","epoch 45 batch 0 loss = 14.445825\n","epoch 46 batch 0 loss = 14.445758\n","epoch 47 batch 0 loss = 14.445691\n","epoch 48 batch 0 loss = 14.445623\n","epoch 49 batch 0 loss = 14.445559\n","epoch 0 batch 0 loss = 27.07292\n","epoch 1 batch 0 loss = 26.17301\n","epoch 2 batch 0 loss = 25.255991\n","epoch 3 batch 0 loss = 24.35006\n","epoch 4 batch 0 loss = 23.46417\n","epoch 5 batch 0 loss = 22.59916\n","epoch 6 batch 0 loss = 21.74432\n","epoch 7 batch 0 loss = 20.913279\n","epoch 8 batch 0 loss = 20.11148\n","epoch 9 batch 0 loss = 19.342932\n","epoch 10 batch 0 loss = 18.613508\n","epoch 11 batch 0 loss = 17.926947\n","epoch 12 batch 0 loss = 17.290785\n","epoch 13 batch 0 loss = 16.725859\n","epoch 14 batch 0 loss = 16.27193\n","epoch 15 batch 0 loss = 15.885294\n","epoch 16 batch 0 loss = 15.511725\n","epoch 17 batch 0 loss = 15.208103\n","epoch 18 batch 0 loss = 14.984026\n","epoch 19 batch 0 loss = 14.822072\n","epoch 20 batch 0 loss = 14.693666\n","epoch 21 batch 0 loss = 14.567897\n","epoch 22 batch 0 loss = 14.45674\n","epoch 23 batch 0 loss = 14.376696\n","epoch 24 batch 0 loss = 14.31942\n","epoch 25 batch 0 loss = 14.278382\n","epoch 26 batch 0 loss = 14.248935\n","epoch 27 batch 0 loss = 14.227827\n","epoch 28 batch 0 loss = 14.212627\n","epoch 29 batch 0 loss = 14.201635\n","epoch 30 batch 0 loss = 14.193652\n","epoch 31 batch 0 loss = 14.187847\n","epoch 32 batch 0 loss = 14.183608\n","epoch 33 batch 0 loss = 14.180502\n","epoch 34 batch 0 loss = 14.178223\n","epoch 35 batch 0 loss = 14.176538\n","epoch 36 batch 0 loss = 14.175288\n","epoch 37 batch 0 loss = 14.1743555\n","epoch 38 batch 0 loss = 14.173658\n","epoch 39 batch 0 loss = 14.173132\n","epoch 40 batch 0 loss = 14.172731\n","epoch 41 batch 0 loss = 14.172421\n","epoch 42 batch 0 loss = 14.17218\n","epoch 43 batch 0 loss = 14.17199\n","epoch 44 batch 0 loss = 14.171837\n","epoch 45 batch 0 loss = 14.171711\n","epoch 46 batch 0 loss = 14.171603\n","epoch 47 batch 0 loss = 14.1715145\n","epoch 48 batch 0 loss = 14.171433\n","epoch 49 batch 0 loss = 14.171361\n","epoch 0 batch 0 loss = 28.038343\n","epoch 1 batch 0 loss = 27.127874\n","epoch 2 batch 0 loss = 26.224007\n","epoch 3 batch 0 loss = 25.333735\n","epoch 4 batch 0 loss = 24.4558\n","epoch 5 batch 0 loss = 23.588633\n","epoch 6 batch 0 loss = 22.720535\n","epoch 7 batch 0 loss = 21.861652\n","epoch 8 batch 0 loss = 21.015299\n","epoch 9 batch 0 loss = 20.17534\n","epoch 10 batch 0 loss = 19.345232\n","epoch 11 batch 0 loss = 18.520317\n","epoch 12 batch 0 loss = 17.723835\n","epoch 13 batch 0 loss = 16.95029\n","epoch 14 batch 0 loss = 16.192518\n","epoch 15 batch 0 loss = 15.472382\n","epoch 16 batch 0 loss = 14.777045\n","epoch 17 batch 0 loss = 14.107549\n","epoch 18 batch 0 loss = 13.459172\n","epoch 19 batch 0 loss = 12.839449\n","epoch 20 batch 0 loss = 12.349887\n","epoch 21 batch 0 loss = 11.906738\n","epoch 22 batch 0 loss = 11.405885\n","epoch 23 batch 0 loss = 11.003614\n","epoch 24 batch 0 loss = 10.674953\n","epoch 25 batch 0 loss = 10.452716\n","epoch 26 batch 0 loss = 10.305062\n","epoch 27 batch 0 loss = 10.207817\n","epoch 28 batch 0 loss = 10.144038\n","epoch 29 batch 0 loss = 10.102327\n","epoch 30 batch 0 loss = 10.075101\n","epoch 31 batch 0 loss = 10.057315\n","epoch 32 batch 0 loss = 10.045691\n","epoch 33 batch 0 loss = 10.038086\n","epoch 34 batch 0 loss = 10.033101\n","epoch 35 batch 0 loss = 10.02982\n","epoch 36 batch 0 loss = 10.027652\n","epoch 37 batch 0 loss = 10.026206\n","epoch 38 batch 0 loss = 10.025232\n","epoch 39 batch 0 loss = 10.024565\n","epoch 40 batch 0 loss = 10.024097\n","epoch 41 batch 0 loss = 10.023764\n","epoch 42 batch 0 loss = 10.023516\n","epoch 43 batch 0 loss = 10.023322\n","epoch 44 batch 0 loss = 10.0231695\n","epoch 45 batch 0 loss = 10.023042\n","epoch 46 batch 0 loss = 10.022928\n","epoch 47 batch 0 loss = 10.022828\n","epoch 48 batch 0 loss = 10.022737\n","epoch 49 batch 0 loss = 10.022649\n","epoch 0 batch 0 loss = 28.167282\n","epoch 1 batch 0 loss = 27.292347\n","epoch 2 batch 0 loss = 26.39798\n","epoch 3 batch 0 loss = 25.517809\n","epoch 4 batch 0 loss = 24.650915\n","epoch 5 batch 0 loss = 23.786434\n","epoch 6 batch 0 loss = 22.922989\n","epoch 7 batch 0 loss = 22.076015\n","epoch 8 batch 0 loss = 21.247858\n","epoch 9 batch 0 loss = 20.442099\n","epoch 10 batch 0 loss = 19.661636\n","epoch 11 batch 0 loss = 18.91388\n","epoch 12 batch 0 loss = 18.206053\n","epoch 13 batch 0 loss = 17.545738\n","epoch 14 batch 0 loss = 16.940937\n","epoch 15 batch 0 loss = 16.427673\n","epoch 16 batch 0 loss = 15.965652\n","epoch 17 batch 0 loss = 15.562765\n","epoch 18 batch 0 loss = 15.254114\n","epoch 19 batch 0 loss = 15.023442\n","epoch 20 batch 0 loss = 14.854815\n","epoch 21 batch 0 loss = 14.733778\n","epoch 22 batch 0 loss = 14.64809\n","epoch 23 batch 0 loss = 14.588057\n","epoch 24 batch 0 loss = 14.546204\n","epoch 25 batch 0 loss = 14.51706\n","epoch 26 batch 0 loss = 14.4967375\n","epoch 27 batch 0 loss = 14.482502\n","epoch 28 batch 0 loss = 14.472472\n","epoch 29 batch 0 loss = 14.465361\n","epoch 30 batch 0 loss = 14.460274\n","epoch 31 batch 0 loss = 14.456611\n","epoch 32 batch 0 loss = 14.453956\n","epoch 33 batch 0 loss = 14.452018\n","epoch 34 batch 0 loss = 14.450593\n","epoch 35 batch 0 loss = 14.449543\n","epoch 36 batch 0 loss = 14.448762\n","epoch 37 batch 0 loss = 14.448178\n","epoch 38 batch 0 loss = 14.4477415\n","epoch 39 batch 0 loss = 14.447412\n","epoch 40 batch 0 loss = 14.44716\n","epoch 41 batch 0 loss = 14.446968\n","epoch 42 batch 0 loss = 14.44682\n","epoch 43 batch 0 loss = 14.446704\n","epoch 44 batch 0 loss = 14.446613\n","epoch 45 batch 0 loss = 14.446541\n","epoch 46 batch 0 loss = 14.44648\n","epoch 47 batch 0 loss = 14.44643\n","epoch 48 batch 0 loss = 14.446389\n","epoch 49 batch 0 loss = 14.446353\n","epoch 0 batch 0 loss = 28.76763\n","epoch 1 batch 0 loss = 27.996557\n","epoch 2 batch 0 loss = 27.199375\n","epoch 3 batch 0 loss = 26.386782\n","epoch 4 batch 0 loss = 25.573002\n","epoch 5 batch 0 loss = 24.722595\n","epoch 6 batch 0 loss = 23.828745\n","epoch 7 batch 0 loss = 22.9215\n","epoch 8 batch 0 loss = 21.991074\n","epoch 9 batch 0 loss = 21.05241\n","epoch 10 batch 0 loss = 20.117523\n","epoch 11 batch 0 loss = 19.176022\n","epoch 12 batch 0 loss = 18.22027\n","epoch 13 batch 0 loss = 17.298386\n","epoch 14 batch 0 loss = 16.42423\n","epoch 15 batch 0 loss = 15.591661\n","epoch 16 batch 0 loss = 14.760065\n","epoch 17 batch 0 loss = 13.987726\n","epoch 18 batch 0 loss = 13.320844\n","epoch 19 batch 0 loss = 12.768066\n","epoch 20 batch 0 loss = 12.3083315\n","epoch 21 batch 0 loss = 12.043528\n","epoch 22 batch 0 loss = 11.855894\n","epoch 23 batch 0 loss = 11.631224\n","epoch 24 batch 0 loss = 11.383224\n","epoch 25 batch 0 loss = 11.159952\n","epoch 26 batch 0 loss = 11.030153\n","epoch 27 batch 0 loss = 10.955836\n","epoch 28 batch 0 loss = 10.913678\n","epoch 29 batch 0 loss = 10.889831\n","epoch 30 batch 0 loss = 10.876302\n","epoch 31 batch 0 loss = 10.868555\n","epoch 32 batch 0 loss = 10.864041\n","epoch 33 batch 0 loss = 10.861332\n","epoch 34 batch 0 loss = 10.859632\n","epoch 35 batch 0 loss = 10.8585\n","epoch 36 batch 0 loss = 10.857688\n","epoch 37 batch 0 loss = 10.857063\n","epoch 38 batch 0 loss = 10.856548\n","epoch 39 batch 0 loss = 10.8561\n","epoch 40 batch 0 loss = 10.855695\n","epoch 41 batch 0 loss = 10.855322\n","epoch 42 batch 0 loss = 10.85497\n","epoch 43 batch 0 loss = 10.854636\n","epoch 44 batch 0 loss = 10.854319\n","epoch 45 batch 0 loss = 10.854014\n","epoch 46 batch 0 loss = 10.85372\n","epoch 47 batch 0 loss = 10.853436\n","epoch 48 batch 0 loss = 10.853165\n","epoch 49 batch 0 loss = 10.852901\n","epoch 0 batch 0 loss = 28.058363\n","epoch 1 batch 0 loss = 27.28076\n","epoch 2 batch 0 loss = 26.468468\n","epoch 3 batch 0 loss = 25.632973\n","epoch 4 batch 0 loss = 24.768986\n","epoch 5 batch 0 loss = 23.878965\n","epoch 6 batch 0 loss = 22.946608\n","epoch 7 batch 0 loss = 21.973444\n","epoch 8 batch 0 loss = 20.970966\n","epoch 9 batch 0 loss = 19.920868\n","epoch 10 batch 0 loss = 18.846062\n","epoch 11 batch 0 loss = 17.757948\n","epoch 12 batch 0 loss = 16.654465\n","epoch 13 batch 0 loss = 15.545528\n","epoch 14 batch 0 loss = 14.429338\n","epoch 15 batch 0 loss = 13.304854\n","epoch 16 batch 0 loss = 12.214284\n","epoch 17 batch 0 loss = 11.216052\n","epoch 18 batch 0 loss = 10.349256\n","epoch 19 batch 0 loss = 9.650939\n","epoch 20 batch 0 loss = 9.159901\n","epoch 21 batch 0 loss = 8.929077\n","epoch 22 batch 0 loss = 8.809169\n","epoch 23 batch 0 loss = 8.742712\n","epoch 24 batch 0 loss = 8.665815\n","epoch 25 batch 0 loss = 8.53775\n","epoch 26 batch 0 loss = 8.466031\n","epoch 27 batch 0 loss = 8.425865\n","epoch 28 batch 0 loss = 8.403238\n","epoch 29 batch 0 loss = 8.390307\n","epoch 30 batch 0 loss = 8.382715\n","epoch 31 batch 0 loss = 8.378075\n","epoch 32 batch 0 loss = 8.375072\n","epoch 33 batch 0 loss = 8.372987\n","epoch 34 batch 0 loss = 8.371425\n","epoch 35 batch 0 loss = 8.370173\n","epoch 36 batch 0 loss = 8.369117\n","epoch 37 batch 0 loss = 8.368185\n","epoch 38 batch 0 loss = 8.367344\n","epoch 39 batch 0 loss = 8.366569\n","epoch 40 batch 0 loss = 8.36585\n","epoch 41 batch 0 loss = 8.365176\n","epoch 42 batch 0 loss = 8.364544\n","epoch 43 batch 0 loss = 8.363947\n","epoch 44 batch 0 loss = 8.363387\n","epoch 45 batch 0 loss = 8.362861\n","epoch 46 batch 0 loss = 8.362362\n","epoch 47 batch 0 loss = 8.361888\n","epoch 48 batch 0 loss = 8.361438\n","epoch 49 batch 0 loss = 8.361011\n","epoch 0 batch 0 loss = 28.380154\n","epoch 1 batch 0 loss = 27.631302\n","epoch 2 batch 0 loss = 26.866526\n","epoch 3 batch 0 loss = 26.096584\n","epoch 4 batch 0 loss = 25.325087\n","epoch 5 batch 0 loss = 24.550325\n","epoch 6 batch 0 loss = 23.75842\n","epoch 7 batch 0 loss = 22.953367\n","epoch 8 batch 0 loss = 22.14195\n","epoch 9 batch 0 loss = 21.327929\n","epoch 10 batch 0 loss = 20.500246\n","epoch 11 batch 0 loss = 19.673185\n","epoch 12 batch 0 loss = 18.856337\n","epoch 13 batch 0 loss = 18.054083\n","epoch 14 batch 0 loss = 17.272038\n","epoch 15 batch 0 loss = 16.514826\n","epoch 16 batch 0 loss = 15.787441\n","epoch 17 batch 0 loss = 15.083773\n","epoch 18 batch 0 loss = 14.386736\n","epoch 19 batch 0 loss = 13.693314\n","epoch 20 batch 0 loss = 13.040927\n","epoch 21 batch 0 loss = 12.453363\n","epoch 22 batch 0 loss = 11.997323\n","epoch 23 batch 0 loss = 11.656899\n","epoch 24 batch 0 loss = 11.412387\n","epoch 25 batch 0 loss = 11.241126\n","epoch 26 batch 0 loss = 11.123336\n","epoch 27 batch 0 loss = 11.042869\n","epoch 28 batch 0 loss = 10.987738\n","epoch 29 batch 0 loss = 10.949641\n","epoch 30 batch 0 loss = 10.922994\n","epoch 31 batch 0 loss = 10.904105\n","epoch 32 batch 0 loss = 10.890566\n","epoch 33 batch 0 loss = 10.8807535\n","epoch 34 batch 0 loss = 10.873586\n","epoch 35 batch 0 loss = 10.868309\n","epoch 36 batch 0 loss = 10.864404\n","epoch 37 batch 0 loss = 10.861497\n","epoch 38 batch 0 loss = 10.8593235\n","epoch 39 batch 0 loss = 10.857693\n","epoch 40 batch 0 loss = 10.856461\n","epoch 41 batch 0 loss = 10.855522\n","epoch 42 batch 0 loss = 10.854807\n","epoch 43 batch 0 loss = 10.854253\n","epoch 44 batch 0 loss = 10.853822\n","epoch 45 batch 0 loss = 10.85348\n","epoch 46 batch 0 loss = 10.853208\n","epoch 47 batch 0 loss = 10.852986\n","epoch 48 batch 0 loss = 10.8528\n","epoch 49 batch 0 loss = 10.852644\n","epoch 0 batch 0 loss = 28.359045\n","epoch 1 batch 0 loss = 27.612759\n","epoch 2 batch 0 loss = 26.854185\n","epoch 3 batch 0 loss = 26.081696\n","epoch 4 batch 0 loss = 25.286573\n","epoch 5 batch 0 loss = 24.479958\n","epoch 6 batch 0 loss = 23.668325\n","epoch 7 batch 0 loss = 22.848429\n","epoch 8 batch 0 loss = 22.024208\n","epoch 9 batch 0 loss = 21.196253\n","epoch 10 batch 0 loss = 20.371237\n","epoch 11 batch 0 loss = 19.56325\n","epoch 12 batch 0 loss = 18.778631\n","epoch 13 batch 0 loss = 17.994867\n","epoch 14 batch 0 loss = 17.237291\n","epoch 15 batch 0 loss = 16.513597\n","epoch 16 batch 0 loss = 15.834304\n","epoch 17 batch 0 loss = 15.248228\n","epoch 18 batch 0 loss = 14.783594\n","epoch 19 batch 0 loss = 14.349335\n","epoch 20 batch 0 loss = 14.006144\n","epoch 21 batch 0 loss = 13.778218\n","epoch 22 batch 0 loss = 13.576081\n","epoch 23 batch 0 loss = 13.409778\n","epoch 24 batch 0 loss = 13.297643\n","epoch 25 batch 0 loss = 13.22237\n","epoch 26 batch 0 loss = 13.171942\n","epoch 27 batch 0 loss = 13.138184\n","epoch 28 batch 0 loss = 13.115577\n","epoch 29 batch 0 loss = 13.100413\n","epoch 30 batch 0 loss = 13.090213\n","epoch 31 batch 0 loss = 13.083316\n","epoch 32 batch 0 loss = 13.0786295\n","epoch 33 batch 0 loss = 13.075422\n","epoch 34 batch 0 loss = 13.0732\n","epoch 35 batch 0 loss = 13.07164\n","epoch 36 batch 0 loss = 13.070522\n","epoch 37 batch 0 loss = 13.069701\n","epoch 38 batch 0 loss = 13.069079\n","epoch 39 batch 0 loss = 13.068592\n","epoch 40 batch 0 loss = 13.0682\n","epoch 41 batch 0 loss = 13.067873\n","epoch 42 batch 0 loss = 13.067596\n","epoch 43 batch 0 loss = 13.067347\n","epoch 44 batch 0 loss = 13.067121\n","epoch 45 batch 0 loss = 13.06691\n","epoch 46 batch 0 loss = 13.066711\n","epoch 47 batch 0 loss = 13.066524\n","epoch 48 batch 0 loss = 13.066341\n","epoch 49 batch 0 loss = 13.066168\n","epoch 0 batch 0 loss = 27.867329\n","epoch 1 batch 0 loss = 27.076239\n","epoch 2 batch 0 loss = 26.254644\n","epoch 3 batch 0 loss = 25.418304\n","epoch 4 batch 0 loss = 24.573076\n","epoch 5 batch 0 loss = 23.718178\n","epoch 6 batch 0 loss = 22.84332\n","epoch 7 batch 0 loss = 21.957506\n","epoch 8 batch 0 loss = 21.061472\n","epoch 9 batch 0 loss = 20.14222\n","epoch 10 batch 0 loss = 19.189482\n","epoch 11 batch 0 loss = 18.229185\n","epoch 12 batch 0 loss = 17.265305\n","epoch 13 batch 0 loss = 16.310095\n","epoch 14 batch 0 loss = 15.389087\n","epoch 15 batch 0 loss = 14.501413\n","epoch 16 batch 0 loss = 13.671264\n","epoch 17 batch 0 loss = 12.924304\n","epoch 18 batch 0 loss = 12.306552\n","epoch 19 batch 0 loss = 11.898298\n","epoch 20 batch 0 loss = 11.686644\n","epoch 21 batch 0 loss = 11.575821\n","epoch 22 batch 0 loss = 11.513441\n","epoch 23 batch 0 loss = 11.43759\n","epoch 24 batch 0 loss = 11.318333\n","epoch 25 batch 0 loss = 11.246039\n","epoch 26 batch 0 loss = 11.202326\n","epoch 27 batch 0 loss = 11.175891\n","epoch 28 batch 0 loss = 11.159858\n","epoch 29 batch 0 loss = 11.150072\n","epoch 30 batch 0 loss = 11.144041\n","epoch 31 batch 0 loss = 11.14026\n","epoch 32 batch 0 loss = 11.137834\n","epoch 33 batch 0 loss = 11.136226\n","epoch 34 batch 0 loss = 11.135113\n","epoch 35 batch 0 loss = 11.134304\n","epoch 36 batch 0 loss = 11.13368\n","epoch 37 batch 0 loss = 11.133176\n","epoch 38 batch 0 loss = 11.132743\n","epoch 39 batch 0 loss = 11.1323595\n","epoch 40 batch 0 loss = 11.132014\n","epoch 41 batch 0 loss = 11.13169\n","epoch 42 batch 0 loss = 11.131384\n","epoch 43 batch 0 loss = 11.13109\n","epoch 44 batch 0 loss = 11.130808\n","epoch 45 batch 0 loss = 11.130537\n","epoch 46 batch 0 loss = 11.130273\n","epoch 47 batch 0 loss = 11.130017\n","epoch 48 batch 0 loss = 11.129769\n","epoch 49 batch 0 loss = 11.129529\n","epoch 0 batch 0 loss = 39.315662\n","epoch 1 batch 0 loss = 38.882454\n","epoch 2 batch 0 loss = 38.41449\n","epoch 3 batch 0 loss = 37.90911\n","epoch 4 batch 0 loss = 37.366497\n","epoch 5 batch 0 loss = 36.78953\n","epoch 6 batch 0 loss = 36.18106\n","epoch 7 batch 0 loss = 35.548466\n","epoch 8 batch 0 loss = 34.90217\n","epoch 9 batch 0 loss = 34.24446\n","epoch 10 batch 0 loss = 33.580566\n","epoch 11 batch 0 loss = 32.909424\n","epoch 12 batch 0 loss = 32.232502\n","epoch 13 batch 0 loss = 31.552198\n","epoch 14 batch 0 loss = 30.866743\n","epoch 15 batch 0 loss = 30.178978\n","epoch 16 batch 0 loss = 29.49421\n","epoch 17 batch 0 loss = 28.813837\n","epoch 18 batch 0 loss = 28.140545\n","epoch 19 batch 0 loss = 27.476215\n","epoch 20 batch 0 loss = 26.825285\n","epoch 21 batch 0 loss = 26.189585\n","epoch 22 batch 0 loss = 25.565516\n","epoch 23 batch 0 loss = 24.961868\n","epoch 24 batch 0 loss = 24.399216\n","epoch 25 batch 0 loss = 23.878641\n","epoch 26 batch 0 loss = 23.399906\n","epoch 27 batch 0 loss = 22.955801\n","epoch 28 batch 0 loss = 22.552483\n","epoch 29 batch 0 loss = 22.180506\n","epoch 30 batch 0 loss = 21.838898\n","epoch 31 batch 0 loss = 21.525648\n","epoch 32 batch 0 loss = 21.24616\n","epoch 33 batch 0 loss = 20.99775\n","epoch 34 batch 0 loss = 20.777689\n","epoch 35 batch 0 loss = 20.583284\n","epoch 36 batch 0 loss = 20.411932\n","epoch 37 batch 0 loss = 20.261175\n","epoch 38 batch 0 loss = 20.128717\n","epoch 39 batch 0 loss = 20.012457\n","epoch 40 batch 0 loss = 19.910477\n","epoch 41 batch 0 loss = 19.821058\n","epoch 42 batch 0 loss = 19.74261\n","epoch 43 batch 0 loss = 19.665323\n","epoch 44 batch 0 loss = 19.597261\n","epoch 45 batch 0 loss = 19.528934\n","epoch 46 batch 0 loss = 19.468664\n","epoch 47 batch 0 loss = 19.415504\n","epoch 48 batch 0 loss = 19.368624\n","epoch 49 batch 0 loss = 19.327272\n","epoch 0 batch 0 loss = 39.74104\n","epoch 1 batch 0 loss = 39.235577\n","epoch 2 batch 0 loss = 38.621773\n","epoch 3 batch 0 loss = 37.900658\n","epoch 4 batch 0 loss = 37.015694\n","epoch 5 batch 0 loss = 35.737312\n","epoch 6 batch 0 loss = 34.158028\n","epoch 7 batch 0 loss = 32.46426\n","epoch 8 batch 0 loss = 31.00614\n","epoch 9 batch 0 loss = 29.966837\n","epoch 10 batch 0 loss = 28.99724\n","epoch 11 batch 0 loss = 28.069056\n","epoch 12 batch 0 loss = 27.17329\n","epoch 13 batch 0 loss = 26.320375\n","epoch 14 batch 0 loss = 25.51437\n","epoch 15 batch 0 loss = 24.753479\n","epoch 16 batch 0 loss = 24.039007\n","epoch 17 batch 0 loss = 23.371012\n","epoch 18 batch 0 loss = 22.73995\n","epoch 19 batch 0 loss = 22.180029\n","epoch 20 batch 0 loss = 21.812426\n","epoch 21 batch 0 loss = 21.706432\n","epoch 22 batch 0 loss = 21.478174\n","epoch 23 batch 0 loss = 21.317266\n","epoch 24 batch 0 loss = 21.19756\n","epoch 25 batch 0 loss = 21.021528\n","epoch 26 batch 0 loss = 20.957872\n","epoch 27 batch 0 loss = 20.773703\n","epoch 28 batch 0 loss = 20.7536\n","epoch 29 batch 0 loss = 20.570261\n","epoch 30 batch 0 loss = 20.581913\n","epoch 31 batch 0 loss = 20.398468\n","epoch 32 batch 0 loss = 20.437769\n","epoch 33 batch 0 loss = 20.25788\n","epoch 34 batch 0 loss = 20.320374\n","epoch 35 batch 0 loss = 20.139866\n","epoch 36 batch 0 loss = 20.221088\n","epoch 37 batch 0 loss = 20.030361\n","epoch 38 batch 0 loss = 20.112082\n","epoch 39 batch 0 loss = 19.923683\n","epoch 40 batch 0 loss = 20.023499\n","epoch 41 batch 0 loss = 19.833902\n","epoch 42 batch 0 loss = 19.950203\n","epoch 43 batch 0 loss = 19.756939\n","epoch 44 batch 0 loss = 19.885965\n","epoch 45 batch 0 loss = 19.688858\n","epoch 46 batch 0 loss = 19.83161\n","epoch 47 batch 0 loss = 19.628334\n","epoch 48 batch 0 loss = 19.766726\n","epoch 49 batch 0 loss = 19.561909\n","epoch 0 batch 0 loss = 39.261333\n","epoch 1 batch 0 loss = 38.559208\n","epoch 2 batch 0 loss = 37.718185\n","epoch 3 batch 0 loss = 36.69075\n","epoch 4 batch 0 loss = 35.409485\n","epoch 5 batch 0 loss = 33.830627\n","epoch 6 batch 0 loss = 32.132767\n","epoch 7 batch 0 loss = 30.564463\n","epoch 8 batch 0 loss = 29.377563\n","epoch 9 batch 0 loss = 28.387505\n","epoch 10 batch 0 loss = 27.433104\n","epoch 11 batch 0 loss = 26.524843\n","epoch 12 batch 0 loss = 25.660316\n","epoch 13 batch 0 loss = 24.848629\n","epoch 14 batch 0 loss = 24.08529\n","epoch 15 batch 0 loss = 23.380964\n","epoch 16 batch 0 loss = 22.748432\n","epoch 17 batch 0 loss = 22.24219\n","epoch 18 batch 0 loss = 21.969563\n","epoch 19 batch 0 loss = 21.688501\n","epoch 20 batch 0 loss = 21.51219\n","epoch 21 batch 0 loss = 21.238836\n","epoch 22 batch 0 loss = 21.106482\n","epoch 23 batch 0 loss = 20.855345\n","epoch 24 batch 0 loss = 20.747595\n","epoch 25 batch 0 loss = 20.511673\n","epoch 26 batch 0 loss = 20.447794\n","epoch 27 batch 0 loss = 20.23708\n","epoch 28 batch 0 loss = 20.209524\n","epoch 29 batch 0 loss = 19.998922\n","epoch 30 batch 0 loss = 19.973124\n","epoch 31 batch 0 loss = 19.773949\n","epoch 32 batch 0 loss = 19.77807\n","epoch 33 batch 0 loss = 19.592216\n","epoch 34 batch 0 loss = 19.620518\n","epoch 35 batch 0 loss = 19.431454\n","epoch 36 batch 0 loss = 19.473392\n","epoch 37 batch 0 loss = 19.289085\n","epoch 38 batch 0 loss = 19.353539\n","epoch 39 batch 0 loss = 19.1699\n","epoch 40 batch 0 loss = 19.247353\n","epoch 41 batch 0 loss = 19.06797\n","epoch 42 batch 0 loss = 19.162844\n","epoch 43 batch 0 loss = 18.984543\n","epoch 44 batch 0 loss = 19.092953\n","epoch 45 batch 0 loss = 18.905668\n","epoch 46 batch 0 loss = 19.022848\n","epoch 47 batch 0 loss = 18.836588\n","epoch 48 batch 0 loss = 18.966557\n","epoch 49 batch 0 loss = 18.77639\n","epoch 0 batch 0 loss = 40.089813\n","epoch 1 batch 0 loss = 39.724735\n","epoch 2 batch 0 loss = 39.24729\n","epoch 3 batch 0 loss = 38.53387\n","epoch 4 batch 0 loss = 37.618294\n","epoch 5 batch 0 loss = 36.451355\n","epoch 6 batch 0 loss = 35.0654\n","epoch 7 batch 0 loss = 33.55654\n","epoch 8 batch 0 loss = 32.068596\n","epoch 9 batch 0 loss = 30.796146\n","epoch 10 batch 0 loss = 29.77468\n","epoch 11 batch 0 loss = 28.767296\n","epoch 12 batch 0 loss = 27.795692\n","epoch 13 batch 0 loss = 26.859514\n","epoch 14 batch 0 loss = 25.960234\n","epoch 15 batch 0 loss = 25.10266\n","epoch 16 batch 0 loss = 24.293793\n","epoch 17 batch 0 loss = 23.543552\n","epoch 18 batch 0 loss = 22.856344\n","epoch 19 batch 0 loss = 22.236063\n","epoch 20 batch 0 loss = 21.70115\n","epoch 21 batch 0 loss = 21.326054\n","epoch 22 batch 0 loss = 21.194584\n","epoch 23 batch 0 loss = 20.903254\n","epoch 24 batch 0 loss = 20.795792\n","epoch 25 batch 0 loss = 20.552128\n","epoch 26 batch 0 loss = 20.482811\n","epoch 27 batch 0 loss = 20.259218\n","epoch 28 batch 0 loss = 20.225441\n","epoch 29 batch 0 loss = 20.024633\n","epoch 30 batch 0 loss = 20.019552\n","epoch 31 batch 0 loss = 19.826416\n","epoch 32 batch 0 loss = 19.82927\n","epoch 33 batch 0 loss = 19.647968\n","epoch 34 batch 0 loss = 19.650843\n","epoch 35 batch 0 loss = 19.470093\n","epoch 36 batch 0 loss = 19.489618\n","epoch 37 batch 0 loss = 19.305555\n","epoch 38 batch 0 loss = 19.337969\n","epoch 39 batch 0 loss = 19.163557\n","epoch 40 batch 0 loss = 19.219652\n","epoch 41 batch 0 loss = 19.05015\n","epoch 42 batch 0 loss = 19.12546\n","epoch 43 batch 0 loss = 18.959457\n","epoch 44 batch 0 loss = 19.050724\n","epoch 45 batch 0 loss = 18.881184\n","epoch 46 batch 0 loss = 18.98797\n","epoch 47 batch 0 loss = 18.814718\n","epoch 48 batch 0 loss = 18.933355\n","epoch 49 batch 0 loss = 18.755653\n","epoch 0 batch 0 loss = 40.527405\n","epoch 1 batch 0 loss = 40.14946\n","epoch 2 batch 0 loss = 39.50126\n","epoch 3 batch 0 loss = 38.50824\n","epoch 4 batch 0 loss = 37.125\n","epoch 5 batch 0 loss = 35.39003\n","epoch 6 batch 0 loss = 33.462536\n","epoch 7 batch 0 loss = 31.691286\n","epoch 8 batch 0 loss = 30.380829\n","epoch 9 batch 0 loss = 29.375042\n","epoch 10 batch 0 loss = 28.413803\n","epoch 11 batch 0 loss = 27.507067\n","epoch 12 batch 0 loss = 26.65974\n","epoch 13 batch 0 loss = 25.88851\n","epoch 14 batch 0 loss = 25.215569\n","epoch 15 batch 0 loss = 24.690203\n","epoch 16 batch 0 loss = 24.314125\n","epoch 17 batch 0 loss = 23.89689\n","epoch 18 batch 0 loss = 23.624664\n","epoch 19 batch 0 loss = 23.278315\n","epoch 20 batch 0 loss = 23.089615\n","epoch 21 batch 0 loss = 22.794355\n","epoch 22 batch 0 loss = 22.672182\n","epoch 23 batch 0 loss = 22.412466\n","epoch 24 batch 0 loss = 22.323076\n","epoch 25 batch 0 loss = 22.08324\n","epoch 26 batch 0 loss = 22.03466\n","epoch 27 batch 0 loss = 21.806463\n","epoch 28 batch 0 loss = 21.785288\n","epoch 29 batch 0 loss = 21.567081\n","epoch 30 batch 0 loss = 21.570341\n","epoch 31 batch 0 loss = 21.359241\n","epoch 32 batch 0 loss = 21.386559\n","epoch 33 batch 0 loss = 21.184029\n","epoch 34 batch 0 loss = 21.21764\n","epoch 35 batch 0 loss = 21.016851\n","epoch 36 batch 0 loss = 21.069954\n","epoch 37 batch 0 loss = 20.86981\n","epoch 38 batch 0 loss = 20.934923\n","epoch 39 batch 0 loss = 20.729324\n","epoch 40 batch 0 loss = 20.801493\n","epoch 41 batch 0 loss = 20.581762\n","epoch 42 batch 0 loss = 20.635164\n","epoch 43 batch 0 loss = 20.409588\n","epoch 44 batch 0 loss = 20.470007\n","epoch 45 batch 0 loss = 20.25468\n","epoch 46 batch 0 loss = 20.332071\n","epoch 47 batch 0 loss = 20.120836\n","epoch 48 batch 0 loss = 20.208729\n","epoch 49 batch 0 loss = 20.001123\n","epoch 0 batch 0 loss = 40.690067\n","epoch 1 batch 0 loss = 40.320827\n","epoch 2 batch 0 loss = 39.783867\n","epoch 3 batch 0 loss = 38.948296\n","epoch 4 batch 0 loss = 37.824287\n","epoch 5 batch 0 loss = 36.433628\n","epoch 6 batch 0 loss = 34.856083\n","epoch 7 batch 0 loss = 33.29686\n","epoch 8 batch 0 loss = 31.94886\n","epoch 9 batch 0 loss = 30.931591\n","epoch 10 batch 0 loss = 29.977991\n","epoch 11 batch 0 loss = 29.052143\n","epoch 12 batch 0 loss = 28.154938\n","epoch 13 batch 0 loss = 27.290497\n","epoch 14 batch 0 loss = 26.462688\n","epoch 15 batch 0 loss = 25.674881\n","epoch 16 batch 0 loss = 24.91875\n","epoch 17 batch 0 loss = 24.210464\n","epoch 18 batch 0 loss = 23.557087\n","epoch 19 batch 0 loss = 22.954182\n","epoch 20 batch 0 loss = 22.455345\n","epoch 21 batch 0 loss = 22.067\n","epoch 22 batch 0 loss = 21.824062\n","epoch 23 batch 0 loss = 21.769676\n","epoch 24 batch 0 loss = 21.561205\n","epoch 25 batch 0 loss = 21.446478\n","epoch 26 batch 0 loss = 21.33801\n","epoch 27 batch 0 loss = 21.193316\n","epoch 28 batch 0 loss = 21.138338\n","epoch 29 batch 0 loss = 20.98721\n","epoch 30 batch 0 loss = 20.974207\n","epoch 31 batch 0 loss = 20.81152\n","epoch 32 batch 0 loss = 20.824339\n","epoch 33 batch 0 loss = 20.66251\n","epoch 34 batch 0 loss = 20.702507\n","epoch 35 batch 0 loss = 20.540058\n","epoch 36 batch 0 loss = 20.602781\n","epoch 37 batch 0 loss = 20.438305\n","epoch 38 batch 0 loss = 20.520548\n","epoch 39 batch 0 loss = 20.352943\n","epoch 40 batch 0 loss = 20.451466\n","epoch 41 batch 0 loss = 20.281118\n","epoch 42 batch 0 loss = 20.39479\n","epoch 43 batch 0 loss = 20.218407\n","epoch 44 batch 0 loss = 20.347128\n","epoch 45 batch 0 loss = 20.16394\n","epoch 46 batch 0 loss = 20.306965\n","epoch 47 batch 0 loss = 20.116064\n","epoch 48 batch 0 loss = 20.272863\n","epoch 49 batch 0 loss = 20.073797\n","epoch 0 batch 0 loss = 39.978905\n","epoch 1 batch 0 loss = 39.6484\n","epoch 2 batch 0 loss = 39.186592\n","epoch 3 batch 0 loss = 38.641235\n","epoch 4 batch 0 loss = 37.895466\n","epoch 5 batch 0 loss = 36.956135\n","epoch 6 batch 0 loss = 35.740936\n","epoch 7 batch 0 loss = 34.27537\n","epoch 8 batch 0 loss = 32.695976\n","epoch 9 batch 0 loss = 31.173136\n","epoch 10 batch 0 loss = 29.949818\n","epoch 11 batch 0 loss = 28.927223\n","epoch 12 batch 0 loss = 27.924025\n","epoch 13 batch 0 loss = 26.964289\n","epoch 14 batch 0 loss = 26.044718\n","epoch 15 batch 0 loss = 25.183134\n","epoch 16 batch 0 loss = 24.382605\n","epoch 17 batch 0 loss = 23.643295\n","epoch 18 batch 0 loss = 22.95875\n","epoch 19 batch 0 loss = 22.335987\n","epoch 20 batch 0 loss = 21.795927\n","epoch 21 batch 0 loss = 21.4158\n","epoch 22 batch 0 loss = 21.281502\n","epoch 23 batch 0 loss = 21.014359\n","epoch 24 batch 0 loss = 20.905699\n","epoch 25 batch 0 loss = 20.66978\n","epoch 26 batch 0 loss = 20.602255\n","epoch 27 batch 0 loss = 20.393173\n","epoch 28 batch 0 loss = 20.355122\n","epoch 29 batch 0 loss = 20.157389\n","epoch 30 batch 0 loss = 20.1521\n","epoch 31 batch 0 loss = 19.971294\n","epoch 32 batch 0 loss = 19.992151\n","epoch 33 batch 0 loss = 19.819826\n","epoch 34 batch 0 loss = 19.864777\n","epoch 35 batch 0 loss = 19.697271\n","epoch 36 batch 0 loss = 19.76213\n","epoch 37 batch 0 loss = 19.598526\n","epoch 38 batch 0 loss = 19.680048\n","epoch 39 batch 0 loss = 19.513306\n","epoch 40 batch 0 loss = 19.60786\n","epoch 41 batch 0 loss = 19.431864\n","epoch 42 batch 0 loss = 19.535402\n","epoch 43 batch 0 loss = 19.35823\n","epoch 44 batch 0 loss = 19.474836\n","epoch 45 batch 0 loss = 19.290583\n","epoch 46 batch 0 loss = 19.40743\n","epoch 47 batch 0 loss = 19.22328\n","epoch 48 batch 0 loss = 19.351877\n","epoch 49 batch 0 loss = 19.166891\n","epoch 0 batch 0 loss = 40.16807\n","epoch 1 batch 0 loss = 39.755978\n","epoch 2 batch 0 loss = 39.240227\n","epoch 3 batch 0 loss = 38.514107\n","epoch 4 batch 0 loss = 37.486595\n","epoch 5 batch 0 loss = 36.14551\n","epoch 6 batch 0 loss = 34.525814\n","epoch 7 batch 0 loss = 32.739304\n","epoch 8 batch 0 loss = 31.004896\n","epoch 9 batch 0 loss = 29.582457\n","epoch 10 batch 0 loss = 28.515503\n","epoch 11 batch 0 loss = 27.51703\n","epoch 12 batch 0 loss = 26.568039\n","epoch 13 batch 0 loss = 25.669514\n","epoch 14 batch 0 loss = 24.829762\n","epoch 15 batch 0 loss = 24.04917\n","epoch 16 batch 0 loss = 23.337002\n","epoch 17 batch 0 loss = 22.681328\n","epoch 18 batch 0 loss = 22.10606\n","epoch 19 batch 0 loss = 21.684679\n","epoch 20 batch 0 loss = 21.521772\n","epoch 21 batch 0 loss = 21.360012\n","epoch 22 batch 0 loss = 21.132586\n","epoch 23 batch 0 loss = 21.013245\n","epoch 24 batch 0 loss = 20.775366\n","epoch 25 batch 0 loss = 20.701363\n","epoch 26 batch 0 loss = 20.494686\n","epoch 27 batch 0 loss = 20.459011\n","epoch 28 batch 0 loss = 20.243994\n","epoch 29 batch 0 loss = 20.229998\n","epoch 30 batch 0 loss = 20.022888\n","epoch 31 batch 0 loss = 20.013174\n","epoch 32 batch 0 loss = 19.827303\n","epoch 33 batch 0 loss = 19.84803\n","epoch 34 batch 0 loss = 19.668892\n","epoch 35 batch 0 loss = 19.695993\n","epoch 36 batch 0 loss = 19.50885\n","epoch 37 batch 0 loss = 19.556087\n","epoch 38 batch 0 loss = 19.378162\n","epoch 39 batch 0 loss = 19.44552\n","epoch 40 batch 0 loss = 19.265148\n","epoch 41 batch 0 loss = 19.341326\n","epoch 42 batch 0 loss = 19.164751\n","epoch 43 batch 0 loss = 19.259474\n","epoch 44 batch 0 loss = 19.082441\n","epoch 45 batch 0 loss = 19.193727\n","epoch 46 batch 0 loss = 19.013361\n","epoch 47 batch 0 loss = 19.138859\n","epoch 48 batch 0 loss = 18.943048\n","epoch 49 batch 0 loss = 19.075214\n","epoch 0 batch 0 loss = 40.736065\n","epoch 1 batch 0 loss = 40.487984\n","epoch 2 batch 0 loss = 40.239094\n","epoch 3 batch 0 loss = 39.97291\n","epoch 4 batch 0 loss = 39.691624\n","epoch 5 batch 0 loss = 39.386936\n","epoch 6 batch 0 loss = 39.05427\n","epoch 7 batch 0 loss = 38.684605\n","epoch 8 batch 0 loss = 38.286358\n","epoch 9 batch 0 loss = 37.84549\n","epoch 10 batch 0 loss = 37.33406\n","epoch 11 batch 0 loss = 36.718987\n","epoch 12 batch 0 loss = 36.009846\n","epoch 13 batch 0 loss = 35.19623\n","epoch 14 batch 0 loss = 34.214825\n","epoch 15 batch 0 loss = 33.08408\n","epoch 16 batch 0 loss = 31.844215\n","epoch 17 batch 0 loss = 30.653389\n","epoch 18 batch 0 loss = 29.666018\n","epoch 19 batch 0 loss = 28.818409\n","epoch 20 batch 0 loss = 28.013494\n","epoch 21 batch 0 loss = 27.242464\n","epoch 22 batch 0 loss = 26.49721\n","epoch 23 batch 0 loss = 25.779396\n","epoch 24 batch 0 loss = 25.093561\n","epoch 25 batch 0 loss = 24.44058\n","epoch 26 batch 0 loss = 23.823719\n","epoch 27 batch 0 loss = 23.25547\n","epoch 28 batch 0 loss = 22.746464\n","epoch 29 batch 0 loss = 22.29809\n","epoch 30 batch 0 loss = 21.923939\n","epoch 31 batch 0 loss = 21.616611\n","epoch 32 batch 0 loss = 21.368427\n","epoch 33 batch 0 loss = 21.193745\n","epoch 34 batch 0 loss = 21.141516\n","epoch 35 batch 0 loss = 21.091059\n","epoch 36 batch 0 loss = 20.936203\n","epoch 37 batch 0 loss = 20.916376\n","epoch 38 batch 0 loss = 20.75676\n","epoch 39 batch 0 loss = 20.757544\n","epoch 40 batch 0 loss = 20.599401\n","epoch 41 batch 0 loss = 20.61821\n","epoch 42 batch 0 loss = 20.467665\n","epoch 43 batch 0 loss = 20.505337\n","epoch 44 batch 0 loss = 20.352379\n","epoch 45 batch 0 loss = 20.393253\n","epoch 46 batch 0 loss = 20.242338\n","epoch 47 batch 0 loss = 20.297693\n","epoch 48 batch 0 loss = 20.143719\n","epoch 49 batch 0 loss = 20.19727\n","epoch 0 batch 0 loss = 39.33738\n","epoch 1 batch 0 loss = 38.543083\n","epoch 2 batch 0 loss = 37.653553\n","epoch 3 batch 0 loss = 36.538963\n","epoch 4 batch 0 loss = 35.234936\n","epoch 5 batch 0 loss = 33.693554\n","epoch 6 batch 0 loss = 32.085075\n","epoch 7 batch 0 loss = 30.601954\n","epoch 8 batch 0 loss = 29.350624\n","epoch 9 batch 0 loss = 28.173773\n","epoch 10 batch 0 loss = 27.067343\n","epoch 11 batch 0 loss = 26.038809\n","epoch 12 batch 0 loss = 25.093637\n","epoch 13 batch 0 loss = 24.23726\n","epoch 14 batch 0 loss = 23.47246\n","epoch 15 batch 0 loss = 22.794577\n","epoch 16 batch 0 loss = 22.209238\n","epoch 17 batch 0 loss = 21.766005\n","epoch 18 batch 0 loss = 21.511768\n","epoch 19 batch 0 loss = 21.57235\n","epoch 20 batch 0 loss = 21.27473\n","epoch 21 batch 0 loss = 21.33059\n","epoch 22 batch 0 loss = 21.101183\n","epoch 23 batch 0 loss = 21.178656\n","epoch 24 batch 0 loss = 20.97678\n","epoch 25 batch 0 loss = 21.071066\n","epoch 26 batch 0 loss = 20.865963\n","epoch 27 batch 0 loss = 20.963709\n","epoch 28 batch 0 loss = 20.768517\n","epoch 29 batch 0 loss = 20.876095\n","epoch 30 batch 0 loss = 20.665388\n","epoch 31 batch 0 loss = 20.764086\n","epoch 32 batch 0 loss = 20.529991\n","epoch 33 batch 0 loss = 20.584219\n","epoch 34 batch 0 loss = 20.379692\n","epoch 35 batch 0 loss = 20.446674\n","epoch 36 batch 0 loss = 20.257128\n","epoch 37 batch 0 loss = 20.333523\n","epoch 38 batch 0 loss = 20.158321\n","epoch 39 batch 0 loss = 20.257542\n","epoch 40 batch 0 loss = 20.0865\n","epoch 41 batch 0 loss = 20.204548\n","epoch 42 batch 0 loss = 20.032583\n","epoch 43 batch 0 loss = 20.167042\n","epoch 44 batch 0 loss = 19.98972\n","epoch 45 batch 0 loss = 20.139317\n","epoch 46 batch 0 loss = 19.954586\n","epoch 47 batch 0 loss = 20.11831\n","epoch 48 batch 0 loss = 19.924889\n","epoch 49 batch 0 loss = 20.10171\n","epoch 0 batch 0 loss = 40.03973\n","epoch 1 batch 0 loss = 39.426094\n","epoch 2 batch 0 loss = 38.767937\n","epoch 3 batch 0 loss = 38.05306\n","epoch 4 batch 0 loss = 37.253952\n","epoch 5 batch 0 loss = 36.345783\n","epoch 6 batch 0 loss = 35.246735\n","epoch 7 batch 0 loss = 33.923794\n","epoch 8 batch 0 loss = 32.416584\n","epoch 9 batch 0 loss = 30.869394\n","epoch 10 batch 0 loss = 29.525702\n","epoch 11 batch 0 loss = 28.343214\n","epoch 12 batch 0 loss = 27.16149\n","epoch 13 batch 0 loss = 26.027054\n","epoch 14 batch 0 loss = 24.937145\n","epoch 15 batch 0 loss = 23.897583\n","epoch 16 batch 0 loss = 22.93788\n","epoch 17 batch 0 loss = 22.076292\n","epoch 18 batch 0 loss = 21.323864\n","epoch 19 batch 0 loss = 20.654634\n","epoch 20 batch 0 loss = 20.111433\n","epoch 21 batch 0 loss = 19.921616\n","epoch 22 batch 0 loss = 19.730476\n","epoch 23 batch 0 loss = 19.60379\n","epoch 24 batch 0 loss = 19.40039\n","epoch 25 batch 0 loss = 19.353987\n","epoch 26 batch 0 loss = 19.158936\n","epoch 27 batch 0 loss = 19.162569\n","epoch 28 batch 0 loss = 18.97277\n","epoch 29 batch 0 loss = 18.997343\n","epoch 30 batch 0 loss = 18.816345\n","epoch 31 batch 0 loss = 18.86711\n","epoch 32 batch 0 loss = 18.68028\n","epoch 33 batch 0 loss = 18.747572\n","epoch 34 batch 0 loss = 18.564177\n","epoch 35 batch 0 loss = 18.644077\n","epoch 36 batch 0 loss = 18.450842\n","epoch 37 batch 0 loss = 18.523499\n","epoch 38 batch 0 loss = 18.335943\n","epoch 39 batch 0 loss = 18.428862\n","epoch 40 batch 0 loss = 18.245413\n","epoch 41 batch 0 loss = 18.355864\n","epoch 42 batch 0 loss = 18.170206\n","epoch 43 batch 0 loss = 18.297152\n","epoch 44 batch 0 loss = 18.107048\n","epoch 45 batch 0 loss = 18.24555\n","epoch 46 batch 0 loss = 18.053932\n","epoch 47 batch 0 loss = 18.199944\n","epoch 48 batch 0 loss = 17.999914\n","epoch 49 batch 0 loss = 18.14142\n","epoch 0 batch 0 loss = 39.738686\n","epoch 1 batch 0 loss = 38.900097\n","epoch 2 batch 0 loss = 37.78609\n","epoch 3 batch 0 loss = 36.447876\n","epoch 4 batch 0 loss = 34.815533\n","epoch 5 batch 0 loss = 33.069187\n","epoch 6 batch 0 loss = 31.336275\n","epoch 7 batch 0 loss = 29.76411\n","epoch 8 batch 0 loss = 28.400936\n","epoch 9 batch 0 loss = 27.118382\n","epoch 10 batch 0 loss = 25.905111\n","epoch 11 batch 0 loss = 24.790674\n","epoch 12 batch 0 loss = 23.775503\n","epoch 13 batch 0 loss = 22.853876\n","epoch 14 batch 0 loss = 22.012817\n","epoch 15 batch 0 loss = 21.286976\n","epoch 16 batch 0 loss = 20.682817\n","epoch 17 batch 0 loss = 20.247501\n","epoch 18 batch 0 loss = 19.964237\n","epoch 19 batch 0 loss = 19.78497\n","epoch 20 batch 0 loss = 19.931017\n","epoch 21 batch 0 loss = 19.623003\n","epoch 22 batch 0 loss = 19.758543\n","epoch 23 batch 0 loss = 19.499739\n","epoch 24 batch 0 loss = 19.638096\n","epoch 25 batch 0 loss = 19.400932\n","epoch 26 batch 0 loss = 19.543678\n","epoch 27 batch 0 loss = 19.309982\n","epoch 28 batch 0 loss = 19.43354\n","epoch 29 batch 0 loss = 19.202662\n","epoch 30 batch 0 loss = 19.320078\n","epoch 31 batch 0 loss = 19.100035\n","epoch 32 batch 0 loss = 19.21002\n","epoch 33 batch 0 loss = 19.004269\n","epoch 34 batch 0 loss = 19.129702\n","epoch 35 batch 0 loss = 18.915894\n","epoch 36 batch 0 loss = 19.0376\n","epoch 37 batch 0 loss = 18.82064\n","epoch 38 batch 0 loss = 18.941372\n","epoch 39 batch 0 loss = 18.731882\n","epoch 40 batch 0 loss = 18.851406\n","epoch 41 batch 0 loss = 18.63282\n","epoch 42 batch 0 loss = 18.762442\n","epoch 43 batch 0 loss = 18.553003\n","epoch 44 batch 0 loss = 18.697844\n","epoch 45 batch 0 loss = 18.491737\n","epoch 46 batch 0 loss = 18.64846\n","epoch 47 batch 0 loss = 18.43548\n","epoch 48 batch 0 loss = 18.595503\n","epoch 49 batch 0 loss = 18.371302\n","epoch 0 batch 0 loss = 41.435574\n","epoch 1 batch 0 loss = 40.75043\n","epoch 2 batch 0 loss = 39.840454\n","epoch 3 batch 0 loss = 38.591774\n","epoch 4 batch 0 loss = 37.057053\n","epoch 5 batch 0 loss = 35.363495\n","epoch 6 batch 0 loss = 33.695282\n","epoch 7 batch 0 loss = 32.166256\n","epoch 8 batch 0 loss = 30.831688\n","epoch 9 batch 0 loss = 29.565212\n","epoch 10 batch 0 loss = 28.354982\n","epoch 11 batch 0 loss = 27.210077\n","epoch 12 batch 0 loss = 26.14102\n","epoch 13 batch 0 loss = 25.163713\n","epoch 14 batch 0 loss = 24.290522\n","epoch 15 batch 0 loss = 23.50267\n","epoch 16 batch 0 loss = 22.831347\n","epoch 17 batch 0 loss = 22.264795\n","epoch 18 batch 0 loss = 21.838032\n","epoch 19 batch 0 loss = 21.515833\n","epoch 20 batch 0 loss = 21.222195\n","epoch 21 batch 0 loss = 21.225233\n","epoch 22 batch 0 loss = 20.994904\n","epoch 23 batch 0 loss = 21.001293\n","epoch 24 batch 0 loss = 20.782024\n","epoch 25 batch 0 loss = 20.807413\n","epoch 26 batch 0 loss = 20.608282\n","epoch 27 batch 0 loss = 20.632582\n","epoch 28 batch 0 loss = 20.425085\n","epoch 29 batch 0 loss = 20.425146\n","epoch 30 batch 0 loss = 20.244606\n","epoch 31 batch 0 loss = 20.274221\n","epoch 32 batch 0 loss = 20.077991\n","epoch 33 batch 0 loss = 20.115786\n","epoch 34 batch 0 loss = 19.931051\n","epoch 35 batch 0 loss = 19.971678\n","epoch 36 batch 0 loss = 19.807833\n","epoch 37 batch 0 loss = 19.869633\n","epoch 38 batch 0 loss = 19.696795\n","epoch 39 batch 0 loss = 19.771381\n","epoch 40 batch 0 loss = 19.603212\n","epoch 41 batch 0 loss = 19.699772\n","epoch 42 batch 0 loss = 19.529903\n","epoch 43 batch 0 loss = 19.645254\n","epoch 44 batch 0 loss = 19.470188\n","epoch 45 batch 0 loss = 19.602322\n","epoch 46 batch 0 loss = 19.420048\n","epoch 47 batch 0 loss = 19.567694\n","epoch 48 batch 0 loss = 19.376827\n","epoch 49 batch 0 loss = 19.539234\n","epoch 0 batch 0 loss = 41.066998\n","epoch 1 batch 0 loss = 40.585106\n","epoch 2 batch 0 loss = 39.97281\n","epoch 3 batch 0 loss = 39.16068\n","epoch 4 batch 0 loss = 38.04887\n","epoch 5 batch 0 loss = 36.60846\n","epoch 6 batch 0 loss = 34.873787\n","epoch 7 batch 0 loss = 32.99285\n","epoch 8 batch 0 loss = 31.155636\n","epoch 9 batch 0 loss = 29.526197\n","epoch 10 batch 0 loss = 28.14242\n","epoch 11 batch 0 loss = 26.870588\n","epoch 12 batch 0 loss = 25.690674\n","epoch 13 batch 0 loss = 24.610281\n","epoch 14 batch 0 loss = 23.625874\n","epoch 15 batch 0 loss = 22.748669\n","epoch 16 batch 0 loss = 21.97854\n","epoch 17 batch 0 loss = 21.323679\n","epoch 18 batch 0 loss = 20.815496\n","epoch 19 batch 0 loss = 20.456278\n","epoch 20 batch 0 loss = 20.169685\n","epoch 21 batch 0 loss = 20.02189\n","epoch 22 batch 0 loss = 20.079092\n","epoch 23 batch 0 loss = 19.856333\n","epoch 24 batch 0 loss = 19.814344\n","epoch 25 batch 0 loss = 19.677197\n","epoch 26 batch 0 loss = 19.610275\n","epoch 27 batch 0 loss = 19.547432\n","epoch 28 batch 0 loss = 19.459349\n","epoch 29 batch 0 loss = 19.441\n","epoch 30 batch 0 loss = 19.3039\n","epoch 31 batch 0 loss = 19.2761\n","epoch 32 batch 0 loss = 19.120241\n","epoch 33 batch 0 loss = 19.122725\n","epoch 34 batch 0 loss = 18.944973\n","epoch 35 batch 0 loss = 18.980982\n","epoch 36 batch 0 loss = 18.816765\n","epoch 37 batch 0 loss = 18.87177\n","epoch 38 batch 0 loss = 18.703182\n","epoch 39 batch 0 loss = 18.781385\n","epoch 40 batch 0 loss = 18.615953\n","epoch 41 batch 0 loss = 18.714886\n","epoch 42 batch 0 loss = 18.545137\n","epoch 43 batch 0 loss = 18.65041\n","epoch 44 batch 0 loss = 18.453043\n","epoch 45 batch 0 loss = 18.572554\n","epoch 46 batch 0 loss = 18.370892\n","epoch 47 batch 0 loss = 18.494286\n","epoch 48 batch 0 loss = 18.291042\n","epoch 49 batch 0 loss = 18.395582\n","epoch 0 batch 0 loss = 41.858246\n","epoch 1 batch 0 loss = 41.264133\n","epoch 2 batch 0 loss = 40.4585\n","epoch 3 batch 0 loss = 39.381805\n","epoch 4 batch 0 loss = 38.013863\n","epoch 5 batch 0 loss = 36.420666\n","epoch 6 batch 0 loss = 34.702827\n","epoch 7 batch 0 loss = 33.022182\n","epoch 8 batch 0 loss = 31.516542\n","epoch 9 batch 0 loss = 30.219423\n","epoch 10 batch 0 loss = 29.021023\n","epoch 11 batch 0 loss = 27.884012\n","epoch 12 batch 0 loss = 26.843739\n","epoch 13 batch 0 loss = 25.901342\n","epoch 14 batch 0 loss = 25.063877\n","epoch 15 batch 0 loss = 24.31702\n","epoch 16 batch 0 loss = 23.659725\n","epoch 17 batch 0 loss = 23.120203\n","epoch 18 batch 0 loss = 22.683973\n","epoch 19 batch 0 loss = 22.382654\n","epoch 20 batch 0 loss = 22.16988\n","epoch 21 batch 0 loss = 22.033297\n","epoch 22 batch 0 loss = 22.144796\n","epoch 23 batch 0 loss = 21.877142\n","epoch 24 batch 0 loss = 21.976349\n","epoch 25 batch 0 loss = 21.722515\n","epoch 26 batch 0 loss = 21.768513\n","epoch 27 batch 0 loss = 21.549744\n","epoch 28 batch 0 loss = 21.609688\n","epoch 29 batch 0 loss = 21.418545\n","epoch 30 batch 0 loss = 21.493944\n","epoch 31 batch 0 loss = 21.298847\n","epoch 32 batch 0 loss = 21.374105\n","epoch 33 batch 0 loss = 21.195108\n","epoch 34 batch 0 loss = 21.287107\n","epoch 35 batch 0 loss = 21.11302\n","epoch 36 batch 0 loss = 21.21968\n","epoch 37 batch 0 loss = 21.04515\n","epoch 38 batch 0 loss = 21.164743\n","epoch 39 batch 0 loss = 20.984503\n","epoch 40 batch 0 loss = 21.11413\n","epoch 41 batch 0 loss = 20.929785\n","epoch 42 batch 0 loss = 21.07033\n","epoch 43 batch 0 loss = 20.873955\n","epoch 44 batch 0 loss = 21.0176\n","epoch 45 batch 0 loss = 20.806671\n","epoch 46 batch 0 loss = 20.947987\n","epoch 47 batch 0 loss = 20.743803\n","epoch 48 batch 0 loss = 20.89726\n","epoch 49 batch 0 loss = 20.69312\n","epoch 0 batch 0 loss = 41.83974\n","epoch 1 batch 0 loss = 41.398132\n","epoch 2 batch 0 loss = 40.748993\n","epoch 3 batch 0 loss = 39.832905\n","epoch 4 batch 0 loss = 38.62864\n","epoch 5 batch 0 loss = 37.16331\n","epoch 6 batch 0 loss = 35.498978\n","epoch 7 batch 0 loss = 33.76942\n","epoch 8 batch 0 loss = 32.149105\n","epoch 9 batch 0 loss = 30.758348\n","epoch 10 batch 0 loss = 29.52202\n","epoch 11 batch 0 loss = 28.340254\n","epoch 12 batch 0 loss = 27.23085\n","epoch 13 batch 0 loss = 26.204922\n","epoch 14 batch 0 loss = 25.279037\n","epoch 15 batch 0 loss = 24.45189\n","epoch 16 batch 0 loss = 23.716387\n","epoch 17 batch 0 loss = 23.058094\n","epoch 18 batch 0 loss = 22.506546\n","epoch 19 batch 0 loss = 22.0886\n","epoch 20 batch 0 loss = 21.749605\n","epoch 21 batch 0 loss = 21.49014\n","epoch 22 batch 0 loss = 21.373785\n","epoch 23 batch 0 loss = 21.409733\n","epoch 24 batch 0 loss = 21.21114\n","epoch 25 batch 0 loss = 21.187815\n","epoch 26 batch 0 loss = 21.066605\n","epoch 27 batch 0 loss = 21.006239\n","epoch 28 batch 0 loss = 20.945475\n","epoch 29 batch 0 loss = 20.860453\n","epoch 30 batch 0 loss = 20.83162\n","epoch 31 batch 0 loss = 20.72278\n","epoch 32 batch 0 loss = 20.728878\n","epoch 33 batch 0 loss = 20.602222\n","epoch 34 batch 0 loss = 20.604288\n","epoch 35 batch 0 loss = 20.454756\n","epoch 36 batch 0 loss = 20.47511\n","epoch 37 batch 0 loss = 20.318195\n","epoch 38 batch 0 loss = 20.365282\n","epoch 39 batch 0 loss = 20.21612\n","epoch 40 batch 0 loss = 20.290386\n","epoch 41 batch 0 loss = 20.139965\n","epoch 42 batch 0 loss = 20.235432\n","epoch 43 batch 0 loss = 20.079372\n","epoch 44 batch 0 loss = 20.192871\n","epoch 45 batch 0 loss = 20.02896\n","epoch 46 batch 0 loss = 20.15821\n","epoch 47 batch 0 loss = 19.986094\n","epoch 48 batch 0 loss = 20.130152\n","epoch 49 batch 0 loss = 19.947931\n","epoch 0 batch 0 loss = 41.531517\n","epoch 1 batch 0 loss = 40.898144\n","epoch 2 batch 0 loss = 40.09916\n","epoch 3 batch 0 loss = 39.118366\n","epoch 4 batch 0 loss = 37.878773\n","epoch 5 batch 0 loss = 36.35235\n","epoch 6 batch 0 loss = 34.593224\n","epoch 7 batch 0 loss = 32.79484\n","epoch 8 batch 0 loss = 31.14222\n","epoch 9 batch 0 loss = 29.735079\n","epoch 10 batch 0 loss = 28.442274\n","epoch 11 batch 0 loss = 27.20751\n","epoch 12 batch 0 loss = 26.042856\n","epoch 13 batch 0 loss = 24.957344\n","epoch 14 batch 0 loss = 23.962036\n","epoch 15 batch 0 loss = 23.079605\n","epoch 16 batch 0 loss = 22.320751\n","epoch 17 batch 0 loss = 21.680515\n","epoch 18 batch 0 loss = 21.142935\n","epoch 19 batch 0 loss = 20.763939\n","epoch 20 batch 0 loss = 20.501762\n","epoch 21 batch 0 loss = 20.294521\n","epoch 22 batch 0 loss = 20.145927\n","epoch 23 batch 0 loss = 20.22308\n","epoch 24 batch 0 loss = 20.117025\n","epoch 25 batch 0 loss = 20.045753\n","epoch 26 batch 0 loss = 19.977196\n","epoch 27 batch 0 loss = 19.87193\n","epoch 28 batch 0 loss = 19.847292\n","epoch 29 batch 0 loss = 19.734962\n","epoch 30 batch 0 loss = 19.747961\n","epoch 31 batch 0 loss = 19.622313\n","epoch 32 batch 0 loss = 19.653948\n","epoch 33 batch 0 loss = 19.51042\n","epoch 34 batch 0 loss = 19.56583\n","epoch 35 batch 0 loss = 19.420784\n","epoch 36 batch 0 loss = 19.497496\n","epoch 37 batch 0 loss = 19.346474\n","epoch 38 batch 0 loss = 19.441975\n","epoch 39 batch 0 loss = 19.2833\n","epoch 40 batch 0 loss = 19.395424\n","epoch 41 batch 0 loss = 19.228197\n","epoch 42 batch 0 loss = 19.355322\n","epoch 43 batch 0 loss = 19.176693\n","epoch 44 batch 0 loss = 19.316813\n","epoch 45 batch 0 loss = 19.1276\n","epoch 46 batch 0 loss = 19.268227\n","epoch 47 batch 0 loss = 19.064741\n","epoch 48 batch 0 loss = 19.21526\n","epoch 49 batch 0 loss = 19.00252\n","epoch 0 batch 0 loss = 41.432354\n","epoch 1 batch 0 loss = 40.924652\n","epoch 2 batch 0 loss = 40.30618\n","epoch 3 batch 0 loss = 39.49538\n","epoch 4 batch 0 loss = 38.43462\n","epoch 5 batch 0 loss = 37.06479\n","epoch 6 batch 0 loss = 35.436947\n","epoch 7 batch 0 loss = 33.63583\n","epoch 8 batch 0 loss = 31.823675\n","epoch 9 batch 0 loss = 30.21413\n","epoch 10 batch 0 loss = 28.90045\n","epoch 11 batch 0 loss = 27.699938\n","epoch 12 batch 0 loss = 26.50445\n","epoch 13 batch 0 loss = 25.372467\n","epoch 14 batch 0 loss = 24.317646\n","epoch 15 batch 0 loss = 23.323044\n","epoch 16 batch 0 loss = 22.40237\n","epoch 17 batch 0 loss = 21.564928\n","epoch 18 batch 0 loss = 20.816048\n","epoch 19 batch 0 loss = 20.167063\n","epoch 20 batch 0 loss = 19.645773\n","epoch 21 batch 0 loss = 19.287073\n","epoch 22 batch 0 loss = 19.075876\n","epoch 23 batch 0 loss = 19.180319\n","epoch 24 batch 0 loss = 18.895374\n","epoch 25 batch 0 loss = 18.891048\n","epoch 26 batch 0 loss = 18.736067\n","epoch 27 batch 0 loss = 18.690454\n","epoch 28 batch 0 loss = 18.610226\n","epoch 29 batch 0 loss = 18.538692\n","epoch 30 batch 0 loss = 18.508162\n","epoch 31 batch 0 loss = 18.417831\n","epoch 32 batch 0 loss = 18.42206\n","epoch 33 batch 0 loss = 18.303722\n","epoch 34 batch 0 loss = 18.329897\n","epoch 35 batch 0 loss = 18.202475\n","epoch 36 batch 0 loss = 18.256096\n","epoch 37 batch 0 loss = 18.118942\n","epoch 38 batch 0 loss = 18.195604\n","epoch 39 batch 0 loss = 18.046257\n","epoch 40 batch 0 loss = 18.142683\n","epoch 41 batch 0 loss = 17.979208\n","epoch 42 batch 0 loss = 18.079302\n","epoch 43 batch 0 loss = 17.908716\n","epoch 44 batch 0 loss = 18.02378\n","epoch 45 batch 0 loss = 17.833277\n","epoch 46 batch 0 loss = 17.956127\n","epoch 47 batch 0 loss = 17.759062\n","epoch 48 batch 0 loss = 17.88123\n","epoch 49 batch 0 loss = 17.676332\n","epoch 0 batch 0 loss = 40.451622\n","epoch 1 batch 0 loss = 39.71349\n","epoch 2 batch 0 loss = 38.968952\n","epoch 3 batch 0 loss = 38.213295\n","epoch 4 batch 0 loss = 37.43429\n","epoch 5 batch 0 loss = 36.635445\n","epoch 6 batch 0 loss = 35.810715\n","epoch 7 batch 0 loss = 34.933113\n","epoch 8 batch 0 loss = 33.93385\n","epoch 9 batch 0 loss = 32.716496\n","epoch 10 batch 0 loss = 31.217224\n","epoch 11 batch 0 loss = 29.50126\n","epoch 12 batch 0 loss = 27.839127\n","epoch 13 batch 0 loss = 26.449314\n","epoch 14 batch 0 loss = 25.106703\n","epoch 15 batch 0 loss = 23.835732\n","epoch 16 batch 0 loss = 22.664572\n","epoch 17 batch 0 loss = 21.60372\n","epoch 18 batch 0 loss = 20.668339\n","epoch 19 batch 0 loss = 19.86852\n","epoch 20 batch 0 loss = 19.207237\n","epoch 21 batch 0 loss = 18.750755\n","epoch 22 batch 0 loss = 18.830317\n","epoch 23 batch 0 loss = 18.468273\n","epoch 24 batch 0 loss = 18.557178\n","epoch 25 batch 0 loss = 18.26948\n","epoch 26 batch 0 loss = 18.381147\n","epoch 27 batch 0 loss = 18.121155\n","epoch 28 batch 0 loss = 18.22328\n","epoch 29 batch 0 loss = 17.979538\n","epoch 30 batch 0 loss = 18.063393\n","epoch 31 batch 0 loss = 17.799929\n","epoch 32 batch 0 loss = 17.894312\n","epoch 33 batch 0 loss = 17.65715\n","epoch 34 batch 0 loss = 17.777077\n","epoch 35 batch 0 loss = 17.547781\n","epoch 36 batch 0 loss = 17.685772\n","epoch 37 batch 0 loss = 17.434921\n","epoch 38 batch 0 loss = 17.572317\n","epoch 39 batch 0 loss = 17.328123\n","epoch 40 batch 0 loss = 17.484018\n","epoch 41 batch 0 loss = 17.23456\n","epoch 42 batch 0 loss = 17.399618\n","epoch 43 batch 0 loss = 17.156322\n","epoch 44 batch 0 loss = 17.337954\n","epoch 45 batch 0 loss = 17.094402\n","epoch 46 batch 0 loss = 17.29119\n","epoch 47 batch 0 loss = 17.043688\n","epoch 48 batch 0 loss = 17.254103\n","epoch 49 batch 0 loss = 16.999872\n","epoch 0 batch 0 loss = 41.869095\n","epoch 1 batch 0 loss = 41.230488\n","epoch 2 batch 0 loss = 40.574966\n","epoch 3 batch 0 loss = 39.896652\n","epoch 4 batch 0 loss = 39.191944\n","epoch 5 batch 0 loss = 38.423473\n","epoch 6 batch 0 loss = 37.575634\n","epoch 7 batch 0 loss = 36.572044\n","epoch 8 batch 0 loss = 35.292717\n","epoch 9 batch 0 loss = 33.738243\n","epoch 10 batch 0 loss = 31.93716\n","epoch 11 batch 0 loss = 30.08323\n","epoch 12 batch 0 loss = 28.347343\n","epoch 13 batch 0 loss = 26.810345\n","epoch 14 batch 0 loss = 25.41037\n","epoch 15 batch 0 loss = 24.106287\n","epoch 16 batch 0 loss = 22.936121\n","epoch 17 batch 0 loss = 21.912691\n","epoch 18 batch 0 loss = 21.02422\n","epoch 19 batch 0 loss = 20.237\n","epoch 20 batch 0 loss = 19.594604\n","epoch 21 batch 0 loss = 19.12772\n","epoch 22 batch 0 loss = 18.810665\n","epoch 23 batch 0 loss = 18.685337\n","epoch 24 batch 0 loss = 18.717972\n","epoch 25 batch 0 loss = 18.51847\n","epoch 26 batch 0 loss = 18.451532\n","epoch 27 batch 0 loss = 18.296041\n","epoch 28 batch 0 loss = 18.174854\n","epoch 29 batch 0 loss = 18.105358\n","epoch 30 batch 0 loss = 17.949022\n","epoch 31 batch 0 loss = 17.928244\n","epoch 32 batch 0 loss = 17.785318\n","epoch 33 batch 0 loss = 17.80689\n","epoch 34 batch 0 loss = 17.64825\n","epoch 35 batch 0 loss = 17.68727\n","epoch 36 batch 0 loss = 17.517273\n","epoch 37 batch 0 loss = 17.574068\n","epoch 38 batch 0 loss = 17.41239\n","epoch 39 batch 0 loss = 17.493546\n","epoch 40 batch 0 loss = 17.333813\n","epoch 41 batch 0 loss = 17.43348\n","epoch 42 batch 0 loss = 17.267635\n","epoch 43 batch 0 loss = 17.384817\n","epoch 44 batch 0 loss = 17.211338\n","epoch 45 batch 0 loss = 17.34288\n","epoch 46 batch 0 loss = 17.16222\n","epoch 47 batch 0 loss = 17.306763\n","epoch 48 batch 0 loss = 17.119251\n","epoch 49 batch 0 loss = 17.274946\n","epoch 0 batch 0 loss = 41.50899\n","epoch 1 batch 0 loss = 40.83136\n","epoch 2 batch 0 loss = 40.072235\n","epoch 3 batch 0 loss = 39.178223\n","epoch 4 batch 0 loss = 38.05427\n","epoch 5 batch 0 loss = 36.65512\n","epoch 6 batch 0 loss = 34.950714\n","epoch 7 batch 0 loss = 33.042557\n","epoch 8 batch 0 loss = 31.063683\n","epoch 9 batch 0 loss = 29.18829\n","epoch 10 batch 0 loss = 27.511946\n","epoch 11 batch 0 loss = 26.031956\n","epoch 12 batch 0 loss = 24.673016\n","epoch 13 batch 0 loss = 23.458271\n","epoch 14 batch 0 loss = 22.394447\n","epoch 15 batch 0 loss = 21.488203\n","epoch 16 batch 0 loss = 20.733274\n","epoch 17 batch 0 loss = 20.127872\n","epoch 18 batch 0 loss = 19.690514\n","epoch 19 batch 0 loss = 19.362059\n","epoch 20 batch 0 loss = 19.16848\n","epoch 21 batch 0 loss = 19.093666\n","epoch 22 batch 0 loss = 19.09225\n","epoch 23 batch 0 loss = 18.870674\n","epoch 24 batch 0 loss = 18.878935\n","epoch 25 batch 0 loss = 18.670565\n","epoch 26 batch 0 loss = 18.700432\n","epoch 27 batch 0 loss = 18.499794\n","epoch 28 batch 0 loss = 18.54731\n","epoch 29 batch 0 loss = 18.36901\n","epoch 30 batch 0 loss = 18.438389\n","epoch 31 batch 0 loss = 18.26651\n","epoch 32 batch 0 loss = 18.352188\n","epoch 33 batch 0 loss = 18.1583\n","epoch 34 batch 0 loss = 18.250086\n","epoch 35 batch 0 loss = 18.067575\n","epoch 36 batch 0 loss = 18.175646\n","epoch 37 batch 0 loss = 17.994715\n","epoch 38 batch 0 loss = 18.11745\n","epoch 39 batch 0 loss = 17.933353\n","epoch 40 batch 0 loss = 18.068705\n","epoch 41 batch 0 loss = 17.879555\n","epoch 42 batch 0 loss = 18.025766\n","epoch 43 batch 0 loss = 17.814531\n","epoch 44 batch 0 loss = 17.952883\n","epoch 45 batch 0 loss = 17.751297\n","epoch 46 batch 0 loss = 17.896526\n","epoch 47 batch 0 loss = 17.666496\n","epoch 48 batch 0 loss = 17.819162\n","epoch 49 batch 0 loss = 17.6043\n","epoch 0 batch 0 loss = 42.06465\n","epoch 1 batch 0 loss = 41.313004\n","epoch 2 batch 0 loss = 40.348843\n","epoch 3 batch 0 loss = 39.092354\n","epoch 4 batch 0 loss = 37.493923\n","epoch 5 batch 0 loss = 35.64309\n","epoch 6 batch 0 loss = 33.69589\n","epoch 7 batch 0 loss = 31.806267\n","epoch 8 batch 0 loss = 30.094717\n","epoch 9 batch 0 loss = 28.564056\n","epoch 10 batch 0 loss = 27.147276\n","epoch 11 batch 0 loss = 25.853197\n","epoch 12 batch 0 loss = 24.695755\n","epoch 13 batch 0 loss = 23.683146\n","epoch 14 batch 0 loss = 22.863104\n","epoch 15 batch 0 loss = 22.186686\n","epoch 16 batch 0 loss = 21.608833\n","epoch 17 batch 0 loss = 21.198929\n","epoch 18 batch 0 loss = 20.90663\n","epoch 19 batch 0 loss = 20.67895\n","epoch 20 batch 0 loss = 20.702795\n","epoch 21 batch 0 loss = 20.60685\n","epoch 22 batch 0 loss = 20.46683\n","epoch 23 batch 0 loss = 20.426256\n","epoch 24 batch 0 loss = 20.291245\n","epoch 25 batch 0 loss = 20.296623\n","epoch 26 batch 0 loss = 20.134106\n","epoch 27 batch 0 loss = 20.157616\n","epoch 28 batch 0 loss = 19.972334\n","epoch 29 batch 0 loss = 20.014227\n","epoch 30 batch 0 loss = 19.84176\n","epoch 31 batch 0 loss = 19.91003\n","epoch 32 batch 0 loss = 19.699032\n","epoch 33 batch 0 loss = 19.76309\n","epoch 34 batch 0 loss = 19.559847\n","epoch 35 batch 0 loss = 19.637028\n","epoch 36 batch 0 loss = 19.452206\n","epoch 37 batch 0 loss = 19.554932\n","epoch 38 batch 0 loss = 19.371773\n","epoch 39 batch 0 loss = 19.494297\n","epoch 40 batch 0 loss = 19.306421\n","epoch 41 batch 0 loss = 19.446157\n","epoch 42 batch 0 loss = 19.251535\n","epoch 43 batch 0 loss = 19.405792\n","epoch 44 batch 0 loss = 19.184433\n","epoch 45 batch 0 loss = 19.33372\n","epoch 46 batch 0 loss = 19.087685\n","epoch 47 batch 0 loss = 19.244757\n","epoch 48 batch 0 loss = 19.01591\n","epoch 49 batch 0 loss = 19.190136\n","epoch 0 batch 0 loss = 42.011005\n","epoch 1 batch 0 loss = 41.404778\n","epoch 2 batch 0 loss = 40.732914\n","epoch 3 batch 0 loss = 39.941593\n","epoch 4 batch 0 loss = 38.96502\n","epoch 5 batch 0 loss = 37.677406\n","epoch 6 batch 0 loss = 36.079174\n","epoch 7 batch 0 loss = 34.224823\n","epoch 8 batch 0 loss = 32.219105\n","epoch 9 batch 0 loss = 30.225168\n","epoch 10 batch 0 loss = 28.386812\n","epoch 11 batch 0 loss = 26.740421\n","epoch 12 batch 0 loss = 25.22239\n","epoch 13 batch 0 loss = 23.812374\n","epoch 14 batch 0 loss = 22.563915\n","epoch 15 batch 0 loss = 21.499363\n","epoch 16 batch 0 loss = 20.616745\n","epoch 17 batch 0 loss = 19.899904\n","epoch 18 batch 0 loss = 19.348642\n","epoch 19 batch 0 loss = 19.020397\n","epoch 20 batch 0 loss = 18.775309\n","epoch 21 batch 0 loss = 18.519712\n","epoch 22 batch 0 loss = 18.50875\n","epoch 23 batch 0 loss = 18.345695\n","epoch 24 batch 0 loss = 18.295443\n","epoch 25 batch 0 loss = 18.21023\n","epoch 26 batch 0 loss = 18.108778\n","epoch 27 batch 0 loss = 18.070192\n","epoch 28 batch 0 loss = 17.903694\n","epoch 29 batch 0 loss = 17.875444\n","epoch 30 batch 0 loss = 17.719185\n","epoch 31 batch 0 loss = 17.71575\n","epoch 32 batch 0 loss = 17.56324\n","epoch 33 batch 0 loss = 17.575863\n","epoch 34 batch 0 loss = 17.426113\n","epoch 35 batch 0 loss = 17.45656\n","epoch 36 batch 0 loss = 17.309692\n","epoch 37 batch 0 loss = 17.372746\n","epoch 38 batch 0 loss = 17.221766\n","epoch 39 batch 0 loss = 17.309927\n","epoch 40 batch 0 loss = 17.1518\n","epoch 41 batch 0 loss = 17.260832\n","epoch 42 batch 0 loss = 17.09572\n","epoch 43 batch 0 loss = 17.218884\n","epoch 44 batch 0 loss = 17.025383\n","epoch 45 batch 0 loss = 17.15607\n","epoch 46 batch 0 loss = 16.963266\n","epoch 47 batch 0 loss = 17.113121\n","epoch 48 batch 0 loss = 16.915329\n","epoch 49 batch 0 loss = 17.081963\n","epoch 0 batch 0 loss = 41.844418\n","epoch 1 batch 0 loss = 41.07215\n","epoch 2 batch 0 loss = 40.162025\n","epoch 3 batch 0 loss = 38.984043\n","epoch 4 batch 0 loss = 37.52326\n","epoch 5 batch 0 loss = 35.785786\n","epoch 6 batch 0 loss = 33.843838\n","epoch 7 batch 0 loss = 31.838657\n","epoch 8 batch 0 loss = 29.888689\n","epoch 9 batch 0 loss = 28.10083\n","epoch 10 batch 0 loss = 26.49611\n","epoch 11 batch 0 loss = 25.046434\n","epoch 12 batch 0 loss = 23.74438\n","epoch 13 batch 0 loss = 22.606964\n","epoch 14 batch 0 loss = 21.649298\n","epoch 15 batch 0 loss = 20.8817\n","epoch 16 batch 0 loss = 20.29269\n","epoch 17 batch 0 loss = 19.885002\n","epoch 18 batch 0 loss = 19.587255\n","epoch 19 batch 0 loss = 19.368542\n","epoch 20 batch 0 loss = 19.206017\n","epoch 21 batch 0 loss = 19.065596\n","epoch 22 batch 0 loss = 19.136381\n","epoch 23 batch 0 loss = 19.012945\n","epoch 24 batch 0 loss = 18.95809\n","epoch 25 batch 0 loss = 18.875957\n","epoch 26 batch 0 loss = 18.770264\n","epoch 27 batch 0 loss = 18.744474\n","epoch 28 batch 0 loss = 18.637005\n","epoch 29 batch 0 loss = 18.649967\n","epoch 30 batch 0 loss = 18.521198\n","epoch 31 batch 0 loss = 18.540796\n","epoch 32 batch 0 loss = 18.404264\n","epoch 33 batch 0 loss = 18.427427\n","epoch 34 batch 0 loss = 18.295525\n","epoch 35 batch 0 loss = 18.34908\n","epoch 36 batch 0 loss = 18.215113\n","epoch 37 batch 0 loss = 18.288918\n","epoch 38 batch 0 loss = 18.124243\n","epoch 39 batch 0 loss = 18.2098\n","epoch 40 batch 0 loss = 18.048296\n","epoch 41 batch 0 loss = 18.155933\n","epoch 42 batch 0 loss = 17.989693\n","epoch 43 batch 0 loss = 18.115204\n","epoch 44 batch 0 loss = 17.94119\n","epoch 45 batch 0 loss = 18.082144\n","epoch 46 batch 0 loss = 17.897507\n","epoch 47 batch 0 loss = 18.053282\n","epoch 48 batch 0 loss = 17.85846\n","epoch 49 batch 0 loss = 18.027943\n","epoch 0 batch 0 loss = 42.141403\n","epoch 1 batch 0 loss = 41.37115\n","epoch 2 batch 0 loss = 40.44463\n","epoch 3 batch 0 loss = 39.27348\n","epoch 4 batch 0 loss = 37.797703\n","epoch 5 batch 0 loss = 36.046654\n","epoch 6 batch 0 loss = 34.05916\n","epoch 7 batch 0 loss = 31.978315\n","epoch 8 batch 0 loss = 29.970448\n","epoch 9 batch 0 loss = 28.17551\n","epoch 10 batch 0 loss = 26.612883\n","epoch 11 batch 0 loss = 25.248014\n","epoch 12 batch 0 loss = 24.049547\n","epoch 13 batch 0 loss = 23.045341\n","epoch 14 batch 0 loss = 22.229448\n","epoch 15 batch 0 loss = 21.576683\n","epoch 16 batch 0 loss = 21.055305\n","epoch 17 batch 0 loss = 20.59658\n","epoch 18 batch 0 loss = 20.238863\n","epoch 19 batch 0 loss = 20.015734\n","epoch 20 batch 0 loss = 20.058355\n","epoch 21 batch 0 loss = 19.784166\n","epoch 22 batch 0 loss = 19.83809\n","epoch 23 batch 0 loss = 19.610136\n","epoch 24 batch 0 loss = 19.651249\n","epoch 25 batch 0 loss = 19.457582\n","epoch 26 batch 0 loss = 19.521065\n","epoch 27 batch 0 loss = 19.336746\n","epoch 28 batch 0 loss = 19.416372\n","epoch 29 batch 0 loss = 19.236753\n","epoch 30 batch 0 loss = 19.329792\n","epoch 31 batch 0 loss = 19.151407\n","epoch 32 batch 0 loss = 19.256277\n","epoch 33 batch 0 loss = 19.074936\n","epoch 34 batch 0 loss = 19.190825\n","epoch 35 batch 0 loss = 19.007439\n","epoch 36 batch 0 loss = 19.133247\n","epoch 37 batch 0 loss = 18.945505\n","epoch 38 batch 0 loss = 19.08253\n","epoch 39 batch 0 loss = 18.888773\n","epoch 40 batch 0 loss = 19.034876\n","epoch 41 batch 0 loss = 18.814106\n","epoch 42 batch 0 loss = 18.959759\n","epoch 43 batch 0 loss = 18.747255\n","epoch 44 batch 0 loss = 18.90508\n","epoch 45 batch 0 loss = 18.693272\n","epoch 46 batch 0 loss = 18.860868\n","epoch 47 batch 0 loss = 18.6337\n","epoch 48 batch 0 loss = 18.792076\n","epoch 49 batch 0 loss = 18.57472\n","epoch 0 batch 0 loss = 41.886097\n","epoch 1 batch 0 loss = 41.11358\n","epoch 2 batch 0 loss = 40.04225\n","epoch 3 batch 0 loss = 38.64166\n","epoch 4 batch 0 loss = 36.92887\n","epoch 5 batch 0 loss = 34.949528\n","epoch 6 batch 0 loss = 32.837402\n","epoch 7 batch 0 loss = 30.732199\n","epoch 8 batch 0 loss = 28.771856\n","epoch 9 batch 0 loss = 27.057371\n","epoch 10 batch 0 loss = 25.581865\n","epoch 11 batch 0 loss = 24.274841\n","epoch 12 batch 0 loss = 23.132555\n","epoch 13 batch 0 loss = 22.151922\n","epoch 14 batch 0 loss = 21.306118\n","epoch 15 batch 0 loss = 20.584259\n","epoch 16 batch 0 loss = 20.002802\n","epoch 17 batch 0 loss = 19.585238\n","epoch 18 batch 0 loss = 19.306084\n","epoch 19 batch 0 loss = 19.077827\n","epoch 20 batch 0 loss = 19.034687\n","epoch 21 batch 0 loss = 19.131662\n","epoch 22 batch 0 loss = 18.885803\n","epoch 23 batch 0 loss = 18.960512\n","epoch 24 batch 0 loss = 18.745668\n","epoch 25 batch 0 loss = 18.81856\n","epoch 26 batch 0 loss = 18.604853\n","epoch 27 batch 0 loss = 18.695553\n","epoch 28 batch 0 loss = 18.498974\n","epoch 29 batch 0 loss = 18.605564\n","epoch 30 batch 0 loss = 18.414213\n","epoch 31 batch 0 loss = 18.534132\n","epoch 32 batch 0 loss = 18.341354\n","epoch 33 batch 0 loss = 18.47394\n","epoch 34 batch 0 loss = 18.27788\n","epoch 35 batch 0 loss = 18.419878\n","epoch 36 batch 0 loss = 18.22264\n","epoch 37 batch 0 loss = 18.374704\n","epoch 38 batch 0 loss = 18.16974\n","epoch 39 batch 0 loss = 18.333874\n","epoch 40 batch 0 loss = 18.12285\n","epoch 41 batch 0 loss = 18.299324\n","epoch 42 batch 0 loss = 18.080368\n","epoch 43 batch 0 loss = 18.268732\n","epoch 44 batch 0 loss = 18.0414\n","epoch 45 batch 0 loss = 18.241201\n","epoch 46 batch 0 loss = 18.005833\n","epoch 47 batch 0 loss = 18.21656\n","epoch 48 batch 0 loss = 17.973059\n","epoch 49 batch 0 loss = 18.195309\n","epoch 0 batch 0 loss = 42.678593\n","epoch 1 batch 0 loss = 42.139065\n","epoch 2 batch 0 loss = 41.498444\n","epoch 3 batch 0 loss = 40.705902\n","epoch 4 batch 0 loss = 39.6762\n","epoch 5 batch 0 loss = 38.365124\n","epoch 6 batch 0 loss = 36.754868\n","epoch 7 batch 0 loss = 34.909054\n","epoch 8 batch 0 loss = 32.915302\n","epoch 9 batch 0 loss = 30.935238\n","epoch 10 batch 0 loss = 29.113522\n","epoch 11 batch 0 loss = 27.523693\n","epoch 12 batch 0 loss = 26.12929\n","epoch 13 batch 0 loss = 24.884312\n","epoch 14 batch 0 loss = 23.796337\n","epoch 15 batch 0 loss = 22.878391\n","epoch 16 batch 0 loss = 22.115763\n","epoch 17 batch 0 loss = 21.525425\n","epoch 18 batch 0 loss = 21.131968\n","epoch 19 batch 0 loss = 20.862457\n","epoch 20 batch 0 loss = 20.682281\n","epoch 21 batch 0 loss = 20.583591\n","epoch 22 batch 0 loss = 20.729183\n","epoch 23 batch 0 loss = 20.463123\n","epoch 24 batch 0 loss = 20.55474\n","epoch 25 batch 0 loss = 20.333017\n","epoch 26 batch 0 loss = 20.431297\n","epoch 27 batch 0 loss = 20.226307\n","epoch 28 batch 0 loss = 20.327494\n","epoch 29 batch 0 loss = 20.086649\n","epoch 30 batch 0 loss = 20.170406\n","epoch 31 batch 0 loss = 19.963625\n","epoch 32 batch 0 loss = 19.99896\n","epoch 33 batch 0 loss = 19.800785\n","epoch 34 batch 0 loss = 19.874178\n","epoch 35 batch 0 loss = 19.699352\n","epoch 36 batch 0 loss = 19.794815\n","epoch 37 batch 0 loss = 19.616236\n","epoch 38 batch 0 loss = 19.701738\n","epoch 39 batch 0 loss = 19.527744\n","epoch 40 batch 0 loss = 19.63076\n","epoch 41 batch 0 loss = 19.459402\n","epoch 42 batch 0 loss = 19.577274\n","epoch 43 batch 0 loss = 19.403723\n","epoch 44 batch 0 loss = 19.53235\n","epoch 45 batch 0 loss = 19.334093\n","epoch 46 batch 0 loss = 19.461985\n","epoch 47 batch 0 loss = 19.264807\n","epoch 48 batch 0 loss = 19.376474\n","epoch 49 batch 0 loss = 19.191204\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gTigyaXlP0Lr"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uvs441JnP0mv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E-0Vjen4P0BS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YGrhmnhNIhXa"},"source":["# 20200811 继续测试一下不同embsize 对结果的影响\n","emb_size_list= [2,3,4,5,6,7,8,9,10]\n","for emb_size in emb_size_list:\n","  ## 20200725 弄一下所有的数据集看看结果;\n","  ## 20200727 看一下mv 失败的task的 结果;\n","  pathlist = ['/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/CF/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/CF_amt/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/MS/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/SP/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/SP_amt/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/ZenCrowd_all/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/ZenCrowd_in/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/ZenCrowd_us/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/SpectralMethodsMeetEM/bluebird/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/SpectralMethodsMeetEM/dog/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/SpectralMethodsMeetEM/rte/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/SpectralMethodsMeetEM/trec/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/SpectralMethodsMeetEM/web/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowdscale2013/fact_eval/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowdscale2013/sentiment/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/d_Duck Identification/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/d_jn-product/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/d_sentiment/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/s4_Dog data/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/s4_Face Sentiment Identification/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/s4_Relevance/',\n","          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/s5_AdultContent/'\n","            ]\n","  emb = str(emb_size)\n","  emb_av_acc = str(emb_size)+'16'+'av_acc'\n","  max = str(emb_size) +'16'+ 'max_acc'\n","  headers = ['Method','dataset','epoch_10_acc','Right','Wrong','epoch_20_acc','Right','Wrong','epoch_30_acc','Right','Wrong','epoch_40_acc','Right','Wrong','epoch_50_acc','Right','Wrong',emb_av_acc,max]\n","  text_top1 = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/' + emb +'_16_mv_failed_ISWorker_Top1_acc.csv'\n","  text_mv = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/'+ emb + '_16_mv_failed_ISWorker_MV_acc.csv'\n","\n","\n","  with open(text_top1,'w')as f:\n","    f_csv = csv.writer(f)\n","    f_csv.writerow(headers)\n","    # f_csv.writerows(rows)\n","\n","  with open(text_mv,'w')as f:\n","    f_csv = csv.writer(f)\n","    f_csv.writerow(headers)\n","\n","  for path in pathlist:\n","    seed = 1234\n","    device = torch.device('cpu')\n","    # 这几个要调整一下的;\n","    b_sz = 10\n","    hidden_size = 128\n","    emb_size = 4\n","    epochs = 50\n","    # Hyper parameters\n","    alpha = 10\n","    beta = 20\n","    gamma = 0.01\n","    lr = 0.025\n","\n","    # datafile = path+'answer_3.csv'\n","    datafile = path + 'answer_mv_failed.csv'\n","    ########## pb + ab 模式的input\n","    with open(path+'w2task_input.pickle','rb') as file:\n","      w2task_input = pickle.load(file)\n","\n","    # with open(path+'w2w_input.pickle','rb') as file:\n","    #   w2w_input = pickle.load(file) \n","    #######################################################################\n","    with open(path+'wnumid2wstrid.pickle','rb') as file:\n","      wnumid2wstrid = pickle.load(file)\n","\n","    with open(path+'wstrid2wnumid.pickle','rb') as file:\n","      wstrid2wnumid = pickle.load(file)\n","\n","    with open(path+'t2w.pickle','rb') as file:\n","      t2w = pickle.load(file)\n","    with open(path+'t2wl.pickle','rb') as file:\n","      t2wl = pickle.load(file)\n","\n","\n","    with open(path+'t2truth.pickle','rb') as file:\n","      t2truth = pickle.load(file)\n","\n","\n","    with open(path+'w2t.pickle','rb') as file:\n","      w2t = pickle.load(file)\n","\n","    with open(path+'w2tl.pickle','rb') as file:\n","      w2tl = pickle.load(file)\n","\n","    with open(path+'worker_acc_dict.pickle','rb') as file:\n","      worker_acc_dict = pickle.load(file)\n","\n","\n","\n","    with open(path+'w2w.pickle','rb') as file:\n","      w2w = pickle.load(file) \n","\n","    with open(path+'training_cps.pickle','rb') as file:\n","      training_cps = pickle.load(file) \n","\n","\n","    with open(path+'label_set.pickle','rb') as file:\n","      label_set = pickle.load(file)\n","\n","    # 20200622 worker 之间 的 similarity 和loss 的计算有关系\n","    # \n","    \n","    dataset = path[78:]\n","    # rows = []\n","    csv_data = pd.read_csv(path+'worker_simlarity.csv',low_memory = False,index_col=0)\n","    worker_similarity_df = pd.DataFrame(csv_data)\n","\n","    # model 构建和训练\n","    pb_feat_size = len(t2w)\n","    w_pb_input = input2numpy(w2task_input,wstrid2wnumid,wnumid2wstrid,pb_feat_size,path,'w_pb_input.npy')\n","    w_pb_input_tensor = torch.FloatTensor(w_pb_input).to(device)\n","    \n","    deepFD = DeepFD(w_pb_input_tensor, w_pb_input_tensor.size(1), emb_size)\n","    deepFD.to(device)\n","    model_loss = Loss_DeepFD(w_pb_input_tensor, worker_similarity_df, wnumid2wstrid, training_cps, device, alpha, beta, gamma)\n","\n","    # 需要写下来的东西, 保存起来的;\n","    Top1_value_list = []\n","    acc_Top1_all = 0\n","    Top1_value_list.append('Top1')\n","    Top1_value_list.append(dataset)\n","\n","    WM_value_list = []\n","    acc_WM_all = 0\n","    WM_value_list.append('Weight_MV')\n","    WM_value_list.append(dataset)\n","\n","    ####20200811多一个max指标\n","    max_temp_Top1 = 0\n","    max_temp_MV = 0\n","\n","    for epoch in range(epochs):\n","      #logger.info(f'----------------------EPOCH {epoch}-----------------------')\n","      \n","      deepFD = train_model(wnumid2wstrid, training_cps, deepFD, model_loss, device, epoch)\n","      #deepSW = train_model_DeepSW(wnumid2wstrid, deepSW, model_loss, device, epoch)\n","      \n","       \n","      #每训练10次出一个结果;\n","      if (epoch+1) % 10 == 0:    \n","        w2sumdistance_dict = distance_experts(wstrid2wnumid, wnumid2wstrid, worker_acc_dict, t2w, t2wl, w2t, path, deepFD)\n","        \n","        ##Top1 方法\n","        Top1_epoch_list = []\n","        t2pwld  = Top1_experts(w2sumdistance_dict, wstrid2wnumid, wnumid2wstrid, t2wl)\n","        acc_Top1, rightlist_Top1, wronglist_Top1  = getaccuracy_Top1(t2truth, t2pwld, t2w)\n","        \n","                \n","        \n","        Top1_epoch_list.append(acc_Top1)\n","        Top1_epoch_list.append(len(rightlist_Top1))\n","        Top1_epoch_list.append(len(wronglist_Top1))\n","        \n","        Top1_value_list.extend(Top1_epoch_list)\n","        acc_Top1_all = acc_Top1_all + acc_Top1\n","        # 20200811 多加的MAX的计算\n","        if acc_Top1 > max_temp_Top1:\n","          max_temp_Top1 = acc_Top1\n","\n","        ## weighted mv 方法\n","        WM_epoch_list = []\n","        mv_t2pwld  = Sort_experts(w2sumdistance_dict, wstrid2wnumid, wnumid2wstrid, t2wl)\n","        mv_t2lpd = W_MV_AggMethod(mv_t2pwld,label_set)\n","        acc_mv, rightlist_mv, wronglist_mv  = W_MV_getaccuracy(t2truth, mv_t2lpd, t2w)\n","        \n","        WM_epoch_list.append(acc_mv)\n","        WM_epoch_list.append(len(rightlist_mv))\n","        WM_epoch_list.append(len(wronglist_mv))\n","        \n","        WM_value_list.extend(WM_epoch_list)\n","        acc_WM_all = acc_WM_all + acc_mv\n","        # 20200811 多加的MAX的计算\n","        if acc_mv > max_temp_MV:\n","          max_temp_MV = acc_mv\n","        \n","        ## 训练完毕,计算最后结果写入文件\n","        if (epoch+1) % 50 == 0:\n","          acc_Top1_all = acc_Top1_all / 5\n","          Top1_value_list.append(acc_Top1_all)\n","          Top1_value_list.append(max_temp_Top1)\n","\n","          acc_WM_all = acc_WM_all / 5\n","          WM_value_list.append(acc_WM_all)\n","          WM_value_list.append(max_temp_MV)\n","\n","          with open(text_top1,'a+')as f:\n","            f_csv = csv.writer(f)\n","            f_csv.writerow(Top1_value_list)\n","            # f_csv.writerow(WM_value_list)\n","          \n","          with open(text_mv,'a+')as f:\n","            f_csv = csv.writer(f)\n","            # f_csv.writerow(Top1_value_list)\n","            f_csv.writerow(WM_value_list)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nedyo3bmPrhj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1_2O4hMFPr23"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wg1P5kS1PrWz"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gJW2MJV9rJON","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1595845969429,"user_tz":-600,"elapsed":1719284,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}},"outputId":"315b56f0-f543-4086-d254-814b34da21d2"},"source":["## 20200725 弄一下所有的数据集看看结果;\n","## 20200727 看一下mv 失败的task的 结果;\n","pathlist = ['/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/CF/',\n","        '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/CF_amt/',\n","        '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/MS/',\n","         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/SP/',\n","         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/SP_amt/',\n","         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/ZenCrowd_all/',\n","         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/ZenCrowd_in/',\n","         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/ZenCrowd_us/',\n","         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/SpectralMethodsMeetEM/bluebird/',\n","         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/SpectralMethodsMeetEM/dog/',\n","         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/SpectralMethodsMeetEM/rte/',\n","         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/SpectralMethodsMeetEM/trec/',\n","         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/SpectralMethodsMeetEM/web/',\n","         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowdscale2013/fact_eval/',\n","         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowdscale2013/sentiment/',\n","         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/d_Duck Identification/',\n","         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/d_jn-product/',\n","         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/d_sentiment/',\n","         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/s4_Dog data/',\n","         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/s4_Face Sentiment Identification/',\n","         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/s4_Relevance/',\n","         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/crowd_truth_inference/s5_AdultContent/'\n","          ]\n","\n","headers = ['Method','dataset','epoch_10_acc','Right','Wrong','epoch_20_acc','Right','Wrong','epoch_30_acc','Right','Wrong','epoch_40_acc','Right','Wrong','epoch_50_acc','Right','Wrong','av_acc']\n","text_top1 = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/' + 'mv_failed_ISWorker_Top1_acc.csv'\n","text_mv = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/' + 'mv_failed_ISWorker_MV_acc.csv'\n","\n","\n","with open(text_top1,'w')as f:\n","  f_csv = csv.writer(f)\n","  f_csv.writerow(headers)\n","  # f_csv.writerows(rows)\n","\n","with open(text_mv,'w')as f:\n","  f_csv = csv.writer(f)\n","  f_csv.writerow(headers)\n","\n","for path in pathlist:\n","  seed = 1234\n","  device = torch.device('cpu')\n","  # 这几个要调整一下的;\n","  b_sz = 10\n","  hidden_size = 128\n","  emb_size = 4\n","  epochs = 50\n","  # Hyper parameters\n","  alpha = 10\n","  beta = 20\n","  gamma = 0.01\n","  lr = 0.025\n","\n","  # datafile = path+'answer_3.csv'\n","  datafile = path + 'answer_mv_failed.csv'\n","  ########## pb + ab 模式的input\n","  with open(path+'w2task_input.pickle','rb') as file:\n","    w2task_input = pickle.load(file)\n","\n","  # with open(path+'w2w_input.pickle','rb') as file:\n","  #   w2w_input = pickle.load(file) \n","  #######################################################################\n","  with open(path+'wnumid2wstrid.pickle','rb') as file:\n","    wnumid2wstrid = pickle.load(file)\n","\n","  with open(path+'wstrid2wnumid.pickle','rb') as file:\n","    wstrid2wnumid = pickle.load(file)\n","\n","  with open(path+'t2w.pickle','rb') as file:\n","    t2w = pickle.load(file)\n","  with open(path+'t2wl.pickle','rb') as file:\n","    t2wl = pickle.load(file)\n","\n","\n","  with open(path+'t2truth.pickle','rb') as file:\n","    t2truth = pickle.load(file)\n","\n","\n","  with open(path+'w2t.pickle','rb') as file:\n","    w2t = pickle.load(file)\n","\n","  with open(path+'w2tl.pickle','rb') as file:\n","    w2tl = pickle.load(file)\n","\n","  with open(path+'worker_acc_dict.pickle','rb') as file:\n","    worker_acc_dict = pickle.load(file)\n","\n","\n","\n","  with open(path+'w2w.pickle','rb') as file:\n","    w2w = pickle.load(file) \n","\n","  with open(path+'training_cps.pickle','rb') as file:\n","    training_cps = pickle.load(file) \n","\n","\n","  with open(path+'label_set.pickle','rb') as file:\n","    label_set = pickle.load(file)\n","\n","  # 20200622 worker 之间 的 similarity 和loss 的计算有关系\n","  # \n","  \n","  dataset = path[78:]\n","  # rows = []\n","  csv_data = pd.read_csv(path+'worker_simlarity.csv',low_memory = False,index_col=0)\n","  worker_similarity_df = pd.DataFrame(csv_data)\n","\n","  # model 构建和训练\n","  pb_feat_size = len(t2w)\n","  w_pb_input = input2numpy(w2task_input,wstrid2wnumid,wnumid2wstrid,pb_feat_size,path,'w_pb_input.npy')\n","  w_pb_input_tensor = torch.FloatTensor(w_pb_input).to(device)\n","  \n","  deepFD = DeepFD(w_pb_input_tensor, w_pb_input_tensor.size(1), emb_size)\n","  deepFD.to(device)\n","  model_loss = Loss_DeepFD(w_pb_input_tensor, worker_similarity_df, wnumid2wstrid, training_cps, device, alpha, beta, gamma)\n","\n","  # 需要写下来的东西, 保存起来的;\n","  Top1_value_list = []\n","  acc_Top1_all = 0\n","  Top1_value_list.append('Top1')\n","  Top1_value_list.append(dataset)\n","\n","  WM_value_list = []\n","  acc_WM_all = 0\n","  WM_value_list.append('Weight_MV')\n","  WM_value_list.append(dataset)\n","\n","  for epoch in range(epochs):\n","    #logger.info(f'----------------------EPOCH {epoch}-----------------------')\n","    \n","    deepFD = train_model(wnumid2wstrid, training_cps, deepFD, model_loss, device, epoch)\n","    #deepSW = train_model_DeepSW(wnumid2wstrid, deepSW, model_loss, device, epoch)\n","    \n","    \n","    #每训练10次出一个结果;\n","    if (epoch+1) % 10 == 0:    \n","      w2sumdistance_dict = distance_experts(wstrid2wnumid, wnumid2wstrid, worker_acc_dict, t2w, t2wl, w2t, path, deepFD)\n","      \n","      ##Top1 方法\n","      Top1_epoch_list = []\n","      t2pwld  = Top1_experts(w2sumdistance_dict, wstrid2wnumid, wnumid2wstrid, t2wl)\n","      acc_Top1, rightlist_Top1, wronglist_Top1  = getaccuracy_Top1(t2truth, t2pwld, t2w)\n","              \n","      \n","      Top1_epoch_list.append(acc_Top1)\n","      Top1_epoch_list.append(len(rightlist_Top1))\n","      Top1_epoch_list.append(len(wronglist_Top1))\n","      \n","      Top1_value_list.extend(Top1_epoch_list)\n","      acc_Top1_all = acc_Top1_all + acc_Top1\n","\n","      ## weighted mv 方法\n","      WM_epoch_list = []\n","      mv_t2pwld  = Sort_experts(w2sumdistance_dict, wstrid2wnumid, wnumid2wstrid, t2wl)\n","      mv_t2lpd = W_MV_AggMethod(mv_t2pwld,label_set)\n","      acc_mv, rightlist_mv, wronglist_mv  = W_MV_getaccuracy(t2truth, mv_t2lpd, t2w)\n","      \n","      WM_epoch_list.append(acc_mv)\n","      WM_epoch_list.append(len(rightlist_mv))\n","      WM_epoch_list.append(len(wronglist_mv))\n","      \n","      WM_value_list.extend(WM_epoch_list)\n","      acc_WM_all = acc_WM_all + acc_mv\n","      \n","      ## 训练完毕,计算最后结果写入文件\n","      if (epoch+1) % 50 == 0:\n","        acc_Top1_all = acc_Top1_all / 5\n","        Top1_value_list.append(acc_Top1_all)\n","        \n","        acc_WM_all = acc_WM_all / 5\n","        WM_value_list.append(acc_WM_all)\n","\n","        with open(text_top1,'a+')as f:\n","          f_csv = csv.writer(f)\n","          f_csv.writerow(Top1_value_list)\n","          # f_csv.writerow(WM_value_list)\n","        \n","        with open(text_mv,'a+')as f:\n","          f_csv = csv.writer(f)\n","          # f_csv.writerow(Top1_value_list)\n","          f_csv.writerow(WM_value_list)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["epoch 0 batch 0 loss = 14.415296\n","epoch 0 batch 1 loss = 13.77039\n","epoch 0 batch 2 loss = 12.443021\n","epoch 0 batch 3 loss = 12.132344\n","epoch 1 batch 0 loss = 11.522124\n","epoch 1 batch 1 loss = 11.77288\n","epoch 1 batch 2 loss = 11.527043\n","epoch 1 batch 3 loss = 10.44588\n","epoch 2 batch 0 loss = 10.727869\n","epoch 2 batch 1 loss = 10.318957\n","epoch 2 batch 2 loss = 10.279736\n","epoch 3 batch 0 loss = 9.799745\n","epoch 3 batch 1 loss = 9.88352\n","epoch 3 batch 2 loss = 10.362183\n","epoch 3 batch 3 loss = 9.917488\n","epoch 3 batch 4 loss = 9.771293\n","epoch 4 batch 0 loss = 10.102808\n","epoch 4 batch 1 loss = 9.64191\n","epoch 4 batch 2 loss = 9.833815\n","epoch 4 batch 3 loss = 9.6812935\n","epoch 5 batch 0 loss = 9.717601\n","epoch 5 batch 1 loss = 9.723365\n","epoch 5 batch 2 loss = 10.370446\n","epoch 6 batch 0 loss = 9.700329\n","epoch 6 batch 1 loss = 10.429016\n","epoch 6 batch 2 loss = 9.742939\n","epoch 7 batch 0 loss = 9.744229\n","epoch 7 batch 1 loss = 9.590604\n","epoch 8 batch 0 loss = 9.948745\n","epoch 8 batch 1 loss = 9.635075\n","epoch 8 batch 2 loss = 9.578674\n","epoch 8 batch 3 loss = 9.947538\n","epoch 9 batch 0 loss = 9.243403\n","epoch 9 batch 1 loss = 10.1291065\n","epoch 9 batch 2 loss = 9.581525\n","epoch 10 batch 0 loss = 9.721608\n","epoch 10 batch 1 loss = 9.532424\n","epoch 10 batch 2 loss = 9.836738\n","epoch 10 batch 3 loss = 9.69259\n","epoch 11 batch 0 loss = 9.618555\n","epoch 11 batch 1 loss = 10.037165\n","epoch 11 batch 2 loss = 9.716655\n","epoch 12 batch 0 loss = 9.655577\n","epoch 12 batch 1 loss = 9.692092\n","epoch 12 batch 2 loss = 9.413845\n","epoch 13 batch 0 loss = 9.86611\n","epoch 13 batch 1 loss = 10.024646\n","epoch 13 batch 2 loss = 10.240974\n","epoch 14 batch 0 loss = 9.947872\n","epoch 14 batch 1 loss = 9.52697\n","epoch 14 batch 2 loss = 10.052787\n","epoch 15 batch 0 loss = 9.470451\n","epoch 15 batch 1 loss = 9.936506\n","epoch 15 batch 2 loss = 9.787797\n","epoch 15 batch 3 loss = 9.249568\n","epoch 15 batch 4 loss = 9.8689\n","epoch 16 batch 0 loss = 9.638866\n","epoch 16 batch 1 loss = 9.841431\n","epoch 16 batch 2 loss = 9.893037\n","epoch 17 batch 0 loss = 9.455916\n","epoch 17 batch 1 loss = 10.242006\n","epoch 17 batch 2 loss = 9.660488\n","epoch 17 batch 3 loss = 9.556284\n","epoch 18 batch 0 loss = 9.841984\n","epoch 18 batch 1 loss = 10.132674\n","epoch 18 batch 2 loss = 9.793681\n","epoch 19 batch 0 loss = 9.408145\n","epoch 19 batch 1 loss = 9.7805605\n","epoch 19 batch 2 loss = 10.018859\n","epoch 20 batch 0 loss = 9.612711\n","epoch 20 batch 1 loss = 9.680132\n","epoch 20 batch 2 loss = 10.056011\n","epoch 21 batch 0 loss = 9.426401\n","epoch 21 batch 1 loss = 9.99894\n","epoch 21 batch 2 loss = 10.275963\n","epoch 21 batch 3 loss = 9.615324\n","epoch 22 batch 0 loss = 9.317036\n","epoch 22 batch 1 loss = 9.781555\n","epoch 22 batch 2 loss = 9.912447\n","epoch 22 batch 3 loss = 9.54029\n","epoch 23 batch 0 loss = 9.809462\n","epoch 23 batch 1 loss = 9.91889\n","epoch 23 batch 2 loss = 9.35169\n","epoch 24 batch 0 loss = 9.708287\n","epoch 24 batch 1 loss = 9.840209\n","epoch 24 batch 2 loss = 9.291333\n","epoch 24 batch 3 loss = 9.85149\n","epoch 25 batch 0 loss = 9.749146\n","epoch 25 batch 1 loss = 9.611313\n","epoch 25 batch 2 loss = 9.725352\n","epoch 25 batch 3 loss = 10.14663\n","epoch 26 batch 0 loss = 9.825953\n","epoch 26 batch 1 loss = 9.665071\n","epoch 26 batch 2 loss = 9.600963\n","epoch 27 batch 0 loss = 9.900158\n","epoch 27 batch 1 loss = 9.631158\n","epoch 27 batch 2 loss = 10.129471\n","epoch 28 batch 0 loss = 9.805615\n","epoch 28 batch 1 loss = 9.627027\n","epoch 28 batch 2 loss = 10.167154\n","epoch 29 batch 0 loss = 10.250738\n","epoch 29 batch 1 loss = 9.778782\n","epoch 29 batch 2 loss = 10.081892\n","epoch 30 batch 0 loss = 9.65202\n","epoch 30 batch 1 loss = 9.453\n","epoch 30 batch 2 loss = 9.83455\n","epoch 31 batch 0 loss = 9.763327\n","epoch 31 batch 1 loss = 9.534194\n","epoch 31 batch 2 loss = 9.9485855\n","epoch 31 batch 3 loss = 9.982498\n","epoch 32 batch 0 loss = 9.773694\n","epoch 32 batch 1 loss = 9.70776\n","epoch 32 batch 2 loss = 9.243399\n","epoch 32 batch 3 loss = 9.610097\n","epoch 33 batch 0 loss = 9.648317\n","epoch 33 batch 1 loss = 9.778404\n","epoch 33 batch 2 loss = 9.709887\n","epoch 33 batch 3 loss = 9.794751\n","epoch 34 batch 0 loss = 9.41256\n","epoch 34 batch 1 loss = 9.870252\n","epoch 34 batch 2 loss = 9.98305\n","epoch 35 batch 0 loss = 9.554473\n","epoch 35 batch 1 loss = 9.88619\n","epoch 35 batch 2 loss = 9.841594\n","epoch 36 batch 0 loss = 9.814068\n","epoch 36 batch 1 loss = 9.5574255\n","epoch 36 batch 2 loss = 9.609492\n","epoch 36 batch 3 loss = 9.905139\n","epoch 37 batch 0 loss = 9.509825\n","epoch 37 batch 1 loss = 9.095427\n","epoch 37 batch 2 loss = 9.771185\n","epoch 37 batch 3 loss = 9.618115\n","epoch 38 batch 0 loss = 9.973728\n","epoch 38 batch 1 loss = 10.104248\n","epoch 38 batch 2 loss = 9.609707\n","epoch 38 batch 3 loss = 9.863645\n","epoch 39 batch 0 loss = 9.983039\n","epoch 39 batch 1 loss = 9.688477\n","epoch 39 batch 2 loss = 9.393295\n","epoch 39 batch 3 loss = 9.84898\n","epoch 40 batch 0 loss = 9.391618\n","epoch 40 batch 1 loss = 9.713007\n","epoch 40 batch 2 loss = 9.94503\n","epoch 40 batch 3 loss = 10.09106\n","epoch 40 batch 4 loss = 9.736607\n","epoch 40 batch 5 loss = 10.333102\n","epoch 41 batch 0 loss = 9.671695\n","epoch 41 batch 1 loss = 9.668075\n","epoch 41 batch 2 loss = 10.211737\n","epoch 42 batch 0 loss = 9.820421\n","epoch 42 batch 1 loss = 9.629015\n","epoch 42 batch 2 loss = 10.062448\n","epoch 42 batch 3 loss = 9.503552\n","epoch 43 batch 0 loss = 9.930562\n","epoch 43 batch 1 loss = 9.727444\n","epoch 43 batch 2 loss = 9.790386\n","epoch 43 batch 3 loss = 9.819761\n","epoch 44 batch 0 loss = 9.489269\n","epoch 44 batch 1 loss = 10.27636\n","epoch 44 batch 2 loss = 9.579843\n","epoch 44 batch 3 loss = 9.400516\n","epoch 45 batch 0 loss = 9.438332\n","epoch 45 batch 1 loss = 9.809128\n","epoch 46 batch 0 loss = 9.980212\n","epoch 46 batch 1 loss = 9.633248\n","epoch 46 batch 2 loss = 9.446873\n","epoch 47 batch 0 loss = 9.399889\n","epoch 47 batch 1 loss = 10.027518\n","epoch 47 batch 2 loss = 9.601978\n","epoch 47 batch 3 loss = 10.008461\n","epoch 48 batch 0 loss = 9.709251\n","epoch 48 batch 1 loss = 10.079951\n","epoch 48 batch 2 loss = 8.886268\n","epoch 48 batch 3 loss = 10.374498\n","epoch 49 batch 0 loss = 9.793571\n","epoch 49 batch 1 loss = 9.508379\n","epoch 49 batch 2 loss = 9.568209\n","epoch 49 batch 3 loss = 9.572482\n","epoch 0 batch 0 loss = 80.228714\n","epoch 1 batch 0 loss = 75.89088\n","epoch 2 batch 0 loss = 71.61111\n","epoch 3 batch 0 loss = 67.37449\n","epoch 4 batch 0 loss = 63.14974\n","epoch 5 batch 0 loss = 59.036594\n","epoch 6 batch 0 loss = 54.724003\n","epoch 7 batch 0 loss = 50.301258\n","epoch 7 batch 1 loss = 46.100258\n","epoch 8 batch 0 loss = 41.422382\n","epoch 9 batch 0 loss = 37.592266\n","epoch 10 batch 0 loss = 34.503387\n","epoch 11 batch 0 loss = 32.8578\n","epoch 11 batch 1 loss = 31.732533\n","epoch 12 batch 0 loss = 31.677753\n","epoch 12 batch 1 loss = 31.361158\n","epoch 13 batch 0 loss = 31.67086\n","epoch 14 batch 0 loss = 31.262602\n","epoch 15 batch 0 loss = 31.532461\n","epoch 16 batch 0 loss = 31.177505\n","epoch 17 batch 0 loss = 31.365446\n","epoch 17 batch 1 loss = 31.11249\n","epoch 18 batch 0 loss = 31.413925\n","epoch 19 batch 0 loss = 31.305702\n","epoch 19 batch 1 loss = 31.382267\n","epoch 20 batch 0 loss = 31.054617\n","epoch 21 batch 0 loss = 31.348139\n","epoch 22 batch 0 loss = 31.03072\n","epoch 23 batch 0 loss = 31.306507\n","epoch 24 batch 0 loss = 30.993107\n","epoch 25 batch 0 loss = 31.277452\n","epoch 26 batch 0 loss = 31.309166\n","epoch 26 batch 1 loss = 31.651394\n","epoch 27 batch 0 loss = 30.92432\n","epoch 28 batch 0 loss = 31.27175\n","epoch 29 batch 0 loss = 30.921503\n","epoch 30 batch 0 loss = 31.27016\n","epoch 31 batch 0 loss = 30.913237\n","epoch 32 batch 0 loss = 31.17545\n","epoch 32 batch 1 loss = 30.920698\n","epoch 33 batch 0 loss = 31.254406\n","epoch 34 batch 0 loss = 31.158417\n","epoch 34 batch 1 loss = 31.365984\n","epoch 35 batch 0 loss = 31.15265\n","epoch 35 batch 1 loss = 31.201822\n","epoch 36 batch 0 loss = 30.93524\n","epoch 37 batch 0 loss = 31.22244\n","epoch 38 batch 0 loss = 30.927399\n","epoch 39 batch 0 loss = 31.187391\n","epoch 40 batch 0 loss = 30.93332\n","epoch 41 batch 0 loss = 31.438257\n","epoch 41 batch 1 loss = 30.904417\n","epoch 42 batch 0 loss = 31.502085\n","epoch 42 batch 1 loss = 30.943352\n","epoch 43 batch 0 loss = 31.160631\n","epoch 44 batch 0 loss = 30.930264\n","epoch 45 batch 0 loss = 31.187618\n","epoch 46 batch 0 loss = 30.926893\n","epoch 47 batch 0 loss = 31.136454\n","epoch 48 batch 0 loss = 30.934162\n","epoch 49 batch 0 loss = 31.150173\n","epoch 0 batch 0 loss = 34.4195\n","epoch 1 batch 0 loss = 33.407673\n","epoch 2 batch 0 loss = 32.342445\n","epoch 3 batch 0 loss = 31.12678\n","epoch 4 batch 0 loss = 29.707315\n","epoch 5 batch 0 loss = 28.064888\n","epoch 6 batch 0 loss = 26.247684\n","epoch 7 batch 0 loss = 24.39674\n","epoch 8 batch 0 loss = 22.581242\n","epoch 9 batch 0 loss = 20.932013\n","epoch 10 batch 0 loss = 19.500273\n","epoch 11 batch 0 loss = 18.164545\n","epoch 12 batch 0 loss = 16.975786\n","epoch 13 batch 0 loss = 15.954988\n","epoch 14 batch 0 loss = 15.215151\n","epoch 15 batch 0 loss = 14.719645\n","epoch 16 batch 0 loss = 14.355939\n","epoch 17 batch 0 loss = 14.135698\n","epoch 18 batch 0 loss = 13.688781\n","epoch 19 batch 0 loss = 13.5003195\n","epoch 20 batch 0 loss = 13.516672\n","epoch 21 batch 0 loss = 13.497543\n","epoch 22 batch 0 loss = 13.370124\n","epoch 23 batch 0 loss = 13.3147135\n","epoch 24 batch 0 loss = 13.255705\n","epoch 25 batch 0 loss = 13.178062\n","epoch 26 batch 0 loss = 13.177042\n","epoch 27 batch 0 loss = 13.06872\n","epoch 28 batch 0 loss = 13.077282\n","epoch 29 batch 0 loss = 12.988809\n","epoch 30 batch 0 loss = 12.992016\n","epoch 31 batch 0 loss = 12.8659115\n","epoch 32 batch 0 loss = 12.942664\n","epoch 33 batch 0 loss = 12.814261\n","epoch 34 batch 0 loss = 12.836206\n","epoch 35 batch 0 loss = 12.683843\n","epoch 36 batch 0 loss = 12.739628\n","epoch 37 batch 0 loss = 12.570646\n","epoch 38 batch 0 loss = 12.669325\n","epoch 39 batch 0 loss = 12.5079\n","epoch 40 batch 0 loss = 12.605688\n","epoch 41 batch 0 loss = 12.434463\n","epoch 42 batch 0 loss = 12.539139\n","epoch 43 batch 0 loss = 12.38589\n","epoch 44 batch 0 loss = 12.505696\n","epoch 45 batch 0 loss = 12.335825\n","epoch 46 batch 0 loss = 12.466376\n","epoch 47 batch 0 loss = 12.280803\n","epoch 48 batch 0 loss = 12.415073\n","epoch 49 batch 0 loss = 12.237885\n","epoch 0 batch 0 loss = 11.9931755\n","epoch 0 batch 1 loss = 12.822052\n","epoch 0 batch 2 loss = 12.33254\n","epoch 0 batch 3 loss = 12.705463\n","epoch 0 batch 4 loss = 12.702698\n","epoch 1 batch 0 loss = 11.954123\n","epoch 1 batch 1 loss = 12.572715\n","epoch 1 batch 2 loss = 12.137168\n","epoch 2 batch 0 loss = 12.194426\n","epoch 2 batch 1 loss = 11.97404\n","epoch 2 batch 2 loss = 12.037033\n","epoch 3 batch 0 loss = 11.908041\n","epoch 3 batch 1 loss = 11.607312\n","epoch 3 batch 2 loss = 12.299469\n","epoch 4 batch 0 loss = 12.010699\n","epoch 4 batch 1 loss = 11.610469\n","epoch 4 batch 2 loss = 11.7775955\n","epoch 4 batch 3 loss = 11.319444\n","epoch 4 batch 4 loss = 11.241836\n","epoch 5 batch 0 loss = 11.211212\n","epoch 5 batch 1 loss = 10.521007\n","epoch 5 batch 2 loss = 10.322716\n","epoch 6 batch 0 loss = 10.536292\n","epoch 6 batch 1 loss = 10.242618\n","epoch 7 batch 0 loss = 10.166689\n","epoch 7 batch 1 loss = 9.891171\n","epoch 7 batch 2 loss = 9.704295\n","epoch 7 batch 3 loss = 9.612405\n","epoch 8 batch 0 loss = 9.576268\n","epoch 8 batch 1 loss = 9.435773\n","epoch 8 batch 2 loss = 9.175184\n","epoch 9 batch 0 loss = 8.904037\n","epoch 9 batch 1 loss = 9.028407\n","epoch 9 batch 2 loss = 8.722823\n","epoch 10 batch 0 loss = 8.409458\n","epoch 10 batch 1 loss = 8.579406\n","epoch 10 batch 2 loss = 8.350607\n","epoch 10 batch 3 loss = 8.145989\n","epoch 11 batch 0 loss = 8.0456705\n","epoch 11 batch 1 loss = 7.5673714\n","epoch 11 batch 2 loss = 7.8730073\n","epoch 11 batch 3 loss = 7.3246403\n","epoch 12 batch 0 loss = 7.5104275\n","epoch 12 batch 1 loss = 7.328325\n","epoch 12 batch 2 loss = 7.241846\n","epoch 12 batch 3 loss = 7.195178\n","epoch 12 batch 4 loss = 7.0890336\n","epoch 13 batch 0 loss = 6.9047227\n","epoch 13 batch 1 loss = 6.7183447\n","epoch 13 batch 2 loss = 6.470312\n","epoch 13 batch 3 loss = 6.330542\n","epoch 13 batch 4 loss = 6.4431796\n","epoch 14 batch 0 loss = 6.353991\n","epoch 14 batch 1 loss = 6.0503516\n","epoch 15 batch 0 loss = 6.1275373\n","epoch 15 batch 1 loss = 5.9995713\n","epoch 15 batch 2 loss = 6.2491016\n","epoch 15 batch 3 loss = 5.9948745\n","epoch 16 batch 0 loss = 5.782593\n","epoch 16 batch 1 loss = 5.768174\n","epoch 16 batch 2 loss = 5.644787\n","epoch 17 batch 0 loss = 5.659761\n","epoch 17 batch 1 loss = 5.4574094\n","epoch 17 batch 2 loss = 5.511795\n","epoch 18 batch 0 loss = 5.5824814\n","epoch 18 batch 1 loss = 5.5040092\n","epoch 18 batch 2 loss = 5.2719326\n","epoch 18 batch 3 loss = 5.506611\n","epoch 19 batch 0 loss = 5.258115\n","epoch 19 batch 1 loss = 5.2057576\n","epoch 19 batch 2 loss = 5.096252\n","epoch 20 batch 0 loss = 5.059867\n","epoch 20 batch 1 loss = 5.169781\n","epoch 20 batch 2 loss = 5.1215854\n","epoch 21 batch 0 loss = 5.231742\n","epoch 21 batch 1 loss = 5.1711087\n","epoch 21 batch 2 loss = 5.2331834\n","epoch 22 batch 0 loss = 5.20377\n","epoch 22 batch 1 loss = 5.002598\n","epoch 22 batch 2 loss = 5.0542955\n","epoch 23 batch 0 loss = 5.0384746\n","epoch 23 batch 1 loss = 5.0385847\n","epoch 23 batch 2 loss = 4.9525533\n","epoch 24 batch 0 loss = 4.9770846\n","epoch 24 batch 1 loss = 5.0770135\n","epoch 24 batch 2 loss = 5.0995812\n","epoch 25 batch 0 loss = 4.992174\n","epoch 25 batch 1 loss = 5.108338\n","epoch 25 batch 2 loss = 5.0518417\n","epoch 25 batch 3 loss = 4.907951\n","epoch 25 batch 4 loss = 4.988346\n","epoch 25 batch 5 loss = 5.1670923\n","epoch 26 batch 0 loss = 5.0752554\n","epoch 26 batch 1 loss = 5.2317767\n","epoch 26 batch 2 loss = 5.0116878\n","epoch 27 batch 0 loss = 5.0131273\n","epoch 27 batch 1 loss = 4.8885155\n","epoch 27 batch 2 loss = 4.9821076\n","epoch 28 batch 0 loss = 4.8636427\n","epoch 28 batch 1 loss = 4.98516\n","epoch 28 batch 2 loss = 4.874603\n","epoch 29 batch 0 loss = 4.815898\n","epoch 29 batch 1 loss = 5.02102\n","epoch 29 batch 2 loss = 5.181536\n","epoch 29 batch 3 loss = 5.0845704\n","epoch 30 batch 0 loss = 5.0790634\n","epoch 30 batch 1 loss = 4.937243\n","epoch 30 batch 2 loss = 4.8677664\n","epoch 31 batch 0 loss = 4.9759583\n","epoch 31 batch 1 loss = 4.914274\n","epoch 31 batch 2 loss = 4.787791\n","epoch 31 batch 3 loss = 4.878887\n","epoch 31 batch 4 loss = 5.117494\n","epoch 32 batch 0 loss = 5.106647\n","epoch 32 batch 1 loss = 4.914758\n","epoch 32 batch 2 loss = 4.9941874\n","epoch 33 batch 0 loss = 5.2709804\n","epoch 33 batch 1 loss = 4.944517\n","epoch 33 batch 2 loss = 4.770169\n","epoch 34 batch 0 loss = 5.0324574\n","epoch 34 batch 1 loss = 4.962618\n","epoch 34 batch 2 loss = 4.9597325\n","epoch 34 batch 3 loss = 4.927379\n","epoch 34 batch 4 loss = 4.8195724\n","epoch 35 batch 0 loss = 4.8232083\n","epoch 35 batch 1 loss = 4.940632\n","epoch 35 batch 2 loss = 4.9858794\n","epoch 35 batch 3 loss = 5.013439\n","epoch 36 batch 0 loss = 5.1086373\n","epoch 36 batch 1 loss = 4.9075236\n","epoch 36 batch 2 loss = 4.823109\n","epoch 36 batch 3 loss = 4.9633517\n","epoch 36 batch 4 loss = 5.0144706\n","epoch 37 batch 0 loss = 4.926093\n","epoch 37 batch 1 loss = 5.0000534\n","epoch 37 batch 2 loss = 4.8707595\n","epoch 37 batch 3 loss = 4.962954\n","epoch 37 batch 4 loss = 5.048038\n","epoch 38 batch 0 loss = 4.9557347\n","epoch 38 batch 1 loss = 4.9309754\n","epoch 38 batch 2 loss = 4.8642597\n","epoch 39 batch 0 loss = 4.818155\n","epoch 39 batch 1 loss = 5.0947995\n","epoch 39 batch 2 loss = 4.825862\n","epoch 39 batch 3 loss = 4.8413506\n","epoch 40 batch 0 loss = 5.0156355\n","epoch 40 batch 1 loss = 4.8650084\n","epoch 40 batch 2 loss = 4.998099\n","epoch 40 batch 3 loss = 4.904983\n","epoch 41 batch 0 loss = 5.0100927\n","epoch 41 batch 1 loss = 5.017359\n","epoch 41 batch 2 loss = 5.1749372\n","epoch 42 batch 0 loss = 4.789866\n","epoch 42 batch 1 loss = 4.9824486\n","epoch 42 batch 2 loss = 5.003191\n","epoch 42 batch 3 loss = 5.0264053\n","epoch 43 batch 0 loss = 4.9543643\n","epoch 43 batch 1 loss = 4.9225454\n","epoch 43 batch 2 loss = 4.9933357\n","epoch 44 batch 0 loss = 5.011198\n","epoch 44 batch 1 loss = 5.001427\n","epoch 44 batch 2 loss = 4.975858\n","epoch 44 batch 3 loss = 4.894103\n","epoch 45 batch 0 loss = 4.9166713\n","epoch 45 batch 1 loss = 4.9709263\n","epoch 45 batch 2 loss = 5.176414\n","epoch 46 batch 0 loss = 4.930499\n","epoch 46 batch 1 loss = 5.049501\n","epoch 46 batch 2 loss = 4.8857317\n","epoch 46 batch 3 loss = 4.785433\n","epoch 47 batch 0 loss = 5.0496335\n","epoch 47 batch 1 loss = 4.9178677\n","epoch 47 batch 2 loss = 4.7740655\n","epoch 47 batch 3 loss = 4.825887\n","epoch 48 batch 0 loss = 4.997199\n","epoch 48 batch 1 loss = 5.044379\n","epoch 48 batch 2 loss = 4.930245\n","epoch 48 batch 3 loss = 4.8319173\n","epoch 49 batch 0 loss = 4.7912893\n","epoch 49 batch 1 loss = 5.026614\n","epoch 49 batch 2 loss = 4.9191213\n","epoch 0 batch 0 loss = 61.895084\n","epoch 0 batch 1 loss = 58.5544\n","epoch 1 batch 0 loss = 53.742805\n","epoch 2 batch 0 loss = 50.25222\n","epoch 2 batch 1 loss = 46.602962\n","epoch 3 batch 0 loss = 43.06307\n","epoch 3 batch 1 loss = 39.690556\n","epoch 4 batch 0 loss = 35.92016\n","epoch 5 batch 0 loss = 33.251118\n","epoch 5 batch 1 loss = 29.954058\n","epoch 6 batch 0 loss = 27.34686\n","epoch 6 batch 1 loss = 24.47364\n","epoch 7 batch 0 loss = 22.30074\n","epoch 7 batch 1 loss = 20.86377\n","epoch 8 batch 0 loss = 20.791313\n","epoch 8 batch 1 loss = 20.04023\n","epoch 9 batch 0 loss = 20.366438\n","epoch 9 batch 1 loss = 20.144316\n","epoch 10 batch 0 loss = 20.548723\n","epoch 10 batch 1 loss = 20.780758\n","epoch 11 batch 0 loss = 20.82021\n","epoch 11 batch 1 loss = 20.704187\n","epoch 12 batch 0 loss = 20.491331\n","epoch 12 batch 1 loss = 20.481577\n","epoch 13 batch 0 loss = 20.146563\n","epoch 13 batch 1 loss = 20.46442\n","epoch 14 batch 0 loss = 20.300121\n","epoch 15 batch 0 loss = 20.53762\n","epoch 15 batch 1 loss = 20.97946\n","epoch 16 batch 0 loss = 20.457235\n","epoch 17 batch 0 loss = 20.339651\n","epoch 17 batch 1 loss = 20.359701\n","epoch 18 batch 0 loss = 20.577488\n","epoch 18 batch 1 loss = 20.41867\n","epoch 19 batch 0 loss = 20.42898\n","epoch 19 batch 1 loss = 20.527328\n","epoch 20 batch 0 loss = 20.31131\n","epoch 20 batch 1 loss = 20.412554\n","epoch 21 batch 0 loss = 20.327587\n","epoch 21 batch 1 loss = 20.646276\n","epoch 22 batch 0 loss = 20.47047\n","epoch 22 batch 1 loss = 20.557514\n","epoch 23 batch 0 loss = 20.266712\n","epoch 24 batch 0 loss = 20.508175\n","epoch 24 batch 1 loss = 20.262207\n","epoch 25 batch 0 loss = 20.32258\n","epoch 25 batch 1 loss = 20.247646\n","epoch 26 batch 0 loss = 20.444086\n","epoch 26 batch 1 loss = 20.26546\n","epoch 27 batch 0 loss = 20.349546\n","epoch 27 batch 1 loss = 20.284508\n","epoch 28 batch 0 loss = 20.683544\n","epoch 28 batch 1 loss = 20.463654\n","epoch 29 batch 0 loss = 20.518587\n","epoch 29 batch 1 loss = 20.28291\n","epoch 30 batch 0 loss = 20.640715\n","epoch 30 batch 1 loss = 20.24743\n","epoch 31 batch 0 loss = 20.651794\n","epoch 31 batch 1 loss = 20.42773\n","epoch 32 batch 0 loss = 20.313696\n","epoch 33 batch 0 loss = 20.286993\n","epoch 33 batch 1 loss = 20.320642\n","epoch 34 batch 0 loss = 20.394978\n","epoch 34 batch 1 loss = 20.474737\n","epoch 35 batch 0 loss = 20.296736\n","epoch 35 batch 1 loss = 20.732775\n","epoch 36 batch 0 loss = 20.078218\n","epoch 36 batch 1 loss = 20.496298\n","epoch 37 batch 0 loss = 20.47681\n","epoch 37 batch 1 loss = 20.360056\n","epoch 38 batch 0 loss = 20.158268\n","epoch 38 batch 1 loss = 20.230173\n","epoch 39 batch 0 loss = 20.61555\n","epoch 39 batch 1 loss = 20.399532\n","epoch 40 batch 0 loss = 20.500132\n","epoch 40 batch 1 loss = 20.24712\n","epoch 41 batch 0 loss = 20.419735\n","epoch 41 batch 1 loss = 20.207094\n","epoch 42 batch 0 loss = 20.161411\n","epoch 43 batch 0 loss = 20.159744\n","epoch 44 batch 0 loss = 20.270792\n","epoch 44 batch 1 loss = 20.140497\n","epoch 45 batch 0 loss = 20.157536\n","epoch 45 batch 1 loss = 20.151924\n","epoch 46 batch 0 loss = 20.474688\n","epoch 46 batch 1 loss = 20.279533\n","epoch 47 batch 0 loss = 20.123344\n","epoch 47 batch 1 loss = 20.098375\n","epoch 48 batch 0 loss = 20.23335\n","epoch 48 batch 1 loss = 20.242964\n","epoch 49 batch 0 loss = 20.306278\n","epoch 49 batch 1 loss = 20.41396\n","epoch 0 batch 0 loss = 55.108498\n","epoch 1 batch 0 loss = 51.913956\n","epoch 2 batch 0 loss = 47.76496\n","epoch 3 batch 0 loss = 43.41928\n","epoch 4 batch 0 loss = 39.586002\n","epoch 5 batch 0 loss = 36.41106\n","epoch 6 batch 0 loss = 33.54165\n","epoch 7 batch 0 loss = 31.09891\n","epoch 8 batch 0 loss = 29.167603\n","epoch 9 batch 0 loss = 27.77475\n","epoch 10 batch 0 loss = 26.882057\n","epoch 11 batch 0 loss = 26.676537\n","epoch 12 batch 0 loss = 27.11129\n","epoch 13 batch 0 loss = 26.441957\n","epoch 14 batch 0 loss = 26.481207\n","epoch 15 batch 0 loss = 26.113264\n","epoch 16 batch 0 loss = 26.095106\n","epoch 17 batch 0 loss = 25.903975\n","epoch 18 batch 0 loss = 25.811634\n","epoch 19 batch 0 loss = 25.649506\n","epoch 20 batch 0 loss = 25.468336\n","epoch 21 batch 0 loss = 25.367373\n","epoch 22 batch 0 loss = 25.140566\n","epoch 23 batch 0 loss = 25.05888\n","epoch 24 batch 0 loss = 24.811705\n","epoch 25 batch 0 loss = 24.818602\n","epoch 26 batch 0 loss = 24.581818\n","epoch 27 batch 0 loss = 24.65154\n","epoch 28 batch 0 loss = 24.390795\n","epoch 29 batch 0 loss = 24.507914\n","epoch 30 batch 0 loss = 24.2303\n","epoch 31 batch 0 loss = 24.36705\n","epoch 32 batch 0 loss = 24.080578\n","epoch 33 batch 0 loss = 24.208843\n","epoch 34 batch 0 loss = 23.931696\n","epoch 35 batch 0 loss = 24.082487\n","epoch 36 batch 0 loss = 23.77046\n","epoch 37 batch 0 loss = 23.87671\n","epoch 38 batch 0 loss = 23.583454\n","epoch 39 batch 0 loss = 23.707628\n","epoch 40 batch 0 loss = 23.443245\n","epoch 41 batch 0 loss = 23.568842\n","epoch 42 batch 0 loss = 23.304794\n","epoch 43 batch 0 loss = 23.45372\n","epoch 44 batch 0 loss = 23.185593\n","epoch 45 batch 0 loss = 23.337448\n","epoch 46 batch 0 loss = 23.054565\n","epoch 47 batch 0 loss = 23.150177\n","epoch 48 batch 0 loss = 22.884075\n","epoch 49 batch 0 loss = 23.032864\n","epoch 0 batch 0 loss = 78.896736\n","epoch 1 batch 0 loss = 76.68741\n","epoch 2 batch 0 loss = 73.31661\n","epoch 3 batch 0 loss = 69.124695\n","epoch 4 batch 0 loss = 64.59066\n","epoch 5 batch 0 loss = 60.33529\n","epoch 6 batch 0 loss = 56.797745\n","epoch 7 batch 0 loss = 53.800053\n","epoch 8 batch 0 loss = 51.003887\n","epoch 9 batch 0 loss = 48.45321\n","epoch 10 batch 0 loss = 46.24808\n","epoch 11 batch 0 loss = 44.423172\n","epoch 12 batch 0 loss = 42.836823\n","epoch 13 batch 0 loss = 41.65722\n","epoch 14 batch 0 loss = 41.57929\n","epoch 15 batch 0 loss = 40.911926\n","epoch 16 batch 0 loss = 40.901524\n","epoch 17 batch 0 loss = 40.36044\n","epoch 18 batch 0 loss = 40.43796\n","epoch 19 batch 0 loss = 39.95561\n","epoch 20 batch 0 loss = 40.104336\n","epoch 21 batch 0 loss = 39.628815\n","epoch 22 batch 0 loss = 39.791965\n","epoch 23 batch 0 loss = 39.213524\n","epoch 24 batch 0 loss = 39.375496\n","epoch 25 batch 0 loss = 38.84189\n","epoch 26 batch 0 loss = 39.055294\n","epoch 27 batch 0 loss = 38.538136\n","epoch 28 batch 0 loss = 38.821\n","epoch 29 batch 0 loss = 38.3103\n","epoch 30 batch 0 loss = 38.59323\n","epoch 31 batch 0 loss = 38.082397\n","epoch 32 batch 0 loss = 38.433266\n","epoch 33 batch 0 loss = 37.864357\n","epoch 34 batch 0 loss = 38.22997\n","epoch 35 batch 0 loss = 37.687984\n","epoch 36 batch 0 loss = 38.122063\n","epoch 37 batch 0 loss = 37.56838\n","epoch 38 batch 0 loss = 38.042393\n","epoch 39 batch 0 loss = 37.47035\n","epoch 40 batch 0 loss = 37.957478\n","epoch 41 batch 0 loss = 37.34643\n","epoch 42 batch 0 loss = 37.761864\n","epoch 43 batch 0 loss = 37.07215\n","epoch 44 batch 0 loss = 37.51093\n","epoch 45 batch 0 loss = 36.864597\n","epoch 46 batch 0 loss = 37.300297\n","epoch 47 batch 0 loss = 36.711063\n","epoch 48 batch 0 loss = 37.216034\n","epoch 49 batch 0 loss = 36.61122\n","epoch 0 batch 0 loss = 35.23547\n","epoch 1 batch 0 loss = 34.498554\n","epoch 2 batch 0 loss = 33.687206\n","epoch 3 batch 0 loss = 32.895905\n","epoch 4 batch 0 loss = 32.058937\n","epoch 5 batch 0 loss = 31.139206\n","epoch 6 batch 0 loss = 30.127785\n","epoch 7 batch 0 loss = 28.854847\n","epoch 8 batch 0 loss = 27.339153\n","epoch 9 batch 0 loss = 25.522686\n","epoch 10 batch 0 loss = 23.69539\n","epoch 11 batch 0 loss = 22.004929\n","epoch 12 batch 0 loss = 20.566835\n","epoch 13 batch 0 loss = 19.307335\n","epoch 14 batch 0 loss = 18.31628\n","epoch 15 batch 0 loss = 17.507608\n","epoch 16 batch 0 loss = 16.867807\n","epoch 17 batch 0 loss = 16.445356\n","epoch 18 batch 0 loss = 15.985259\n","epoch 19 batch 0 loss = 15.7534485\n","epoch 20 batch 0 loss = 15.543258\n","epoch 21 batch 0 loss = 15.254602\n","epoch 22 batch 0 loss = 15.159829\n","epoch 23 batch 0 loss = 15.415496\n","epoch 24 batch 0 loss = 15.034653\n","epoch 25 batch 0 loss = 15.253536\n","epoch 26 batch 0 loss = 14.911158\n","epoch 27 batch 0 loss = 15.127264\n","epoch 28 batch 0 loss = 14.8053465\n","epoch 29 batch 0 loss = 15.000742\n","epoch 30 batch 0 loss = 14.691758\n","epoch 31 batch 0 loss = 14.929958\n","epoch 32 batch 0 loss = 14.595825\n","epoch 33 batch 0 loss = 14.746986\n","epoch 34 batch 0 loss = 14.489099\n","epoch 35 batch 0 loss = 14.649917\n","epoch 36 batch 0 loss = 14.403989\n","epoch 37 batch 0 loss = 14.567526\n","epoch 38 batch 0 loss = 14.32979\n","epoch 39 batch 0 loss = 14.460312\n","epoch 40 batch 0 loss = 14.236511\n","epoch 41 batch 0 loss = 14.407879\n","epoch 42 batch 0 loss = 14.211766\n","epoch 43 batch 0 loss = 14.37483\n","epoch 44 batch 0 loss = 14.147782\n","epoch 45 batch 0 loss = 14.340721\n","epoch 46 batch 0 loss = 14.083952\n","epoch 47 batch 0 loss = 14.27484\n","epoch 48 batch 0 loss = 13.984224\n","epoch 49 batch 0 loss = 14.162281\n","epoch 0 batch 0 loss = 368.8196\n","epoch 1 batch 0 loss = 346.37274\n","epoch 2 batch 0 loss = 320.30774\n","epoch 3 batch 0 loss = 292.93817\n","epoch 4 batch 0 loss = 265.6766\n","epoch 5 batch 0 loss = 239.18314\n","epoch 6 batch 0 loss = 221.24034\n","epoch 7 batch 0 loss = 218.14919\n","epoch 8 batch 0 loss = 221.07074\n","epoch 9 batch 0 loss = 218.0945\n","epoch 10 batch 0 loss = 221.11894\n","epoch 11 batch 0 loss = 218.03456\n","epoch 12 batch 0 loss = 221.13899\n","epoch 13 batch 0 loss = 217.87259\n","epoch 14 batch 0 loss = 221.15985\n","epoch 15 batch 0 loss = 217.76707\n","epoch 16 batch 0 loss = 221.23997\n","epoch 17 batch 0 loss = 217.7877\n","epoch 18 batch 0 loss = 221.30096\n","epoch 19 batch 0 loss = 217.70811\n","epoch 20 batch 0 loss = 221.29102\n","epoch 21 batch 0 loss = 217.63406\n","epoch 22 batch 0 loss = 221.39886\n","epoch 23 batch 0 loss = 217.51797\n","epoch 24 batch 0 loss = 221.38338\n","epoch 25 batch 0 loss = 217.47185\n","epoch 26 batch 0 loss = 221.44025\n","epoch 27 batch 0 loss = 217.46042\n","epoch 28 batch 0 loss = 221.47386\n","epoch 29 batch 0 loss = 217.36215\n","epoch 30 batch 0 loss = 221.51874\n","epoch 31 batch 0 loss = 217.32579\n","epoch 32 batch 0 loss = 221.57887\n","epoch 33 batch 0 loss = 217.25488\n","epoch 34 batch 0 loss = 221.64705\n","epoch 35 batch 0 loss = 217.16504\n","epoch 36 batch 0 loss = 221.59009\n","epoch 37 batch 0 loss = 217.12732\n","epoch 38 batch 0 loss = 221.6934\n","epoch 39 batch 0 loss = 217.0972\n","epoch 40 batch 0 loss = 221.66414\n","epoch 41 batch 0 loss = 217.00787\n","epoch 42 batch 0 loss = 221.74588\n","epoch 43 batch 0 loss = 216.95859\n","epoch 44 batch 0 loss = 221.74687\n","epoch 44 batch 1 loss = 216.94887\n","epoch 45 batch 0 loss = 221.79686\n","epoch 46 batch 0 loss = 216.83215\n","epoch 47 batch 0 loss = 221.84247\n","epoch 48 batch 0 loss = 216.80347\n","epoch 49 batch 0 loss = 221.92203\n","epoch 0 batch 0 loss = 39.614025\n","epoch 1 batch 0 loss = 38.49069\n","epoch 2 batch 0 loss = 37.59795\n","epoch 2 batch 1 loss = 36.254234\n","epoch 3 batch 0 loss = 34.382275\n","epoch 3 batch 1 loss = 32.98487\n","epoch 4 batch 0 loss = 31.26094\n","epoch 5 batch 0 loss = 29.45523\n","epoch 6 batch 0 loss = 27.574223\n","epoch 7 batch 0 loss = 25.865755\n","epoch 7 batch 1 loss = 23.901854\n","epoch 8 batch 0 loss = 21.994682\n","epoch 9 batch 0 loss = 20.547457\n","epoch 10 batch 0 loss = 19.458036\n","epoch 11 batch 0 loss = 18.635038\n","epoch 12 batch 0 loss = 18.001427\n","epoch 13 batch 0 loss = 17.497663\n","epoch 14 batch 0 loss = 17.355644\n","epoch 14 batch 1 loss = 17.092268\n","epoch 15 batch 0 loss = 17.312267\n","epoch 15 batch 1 loss = 16.876953\n","epoch 16 batch 0 loss = 16.806406\n","epoch 17 batch 0 loss = 16.7516\n","epoch 18 batch 0 loss = 16.73696\n","epoch 18 batch 1 loss = 16.651394\n","epoch 19 batch 0 loss = 16.804258\n","epoch 19 batch 1 loss = 16.60567\n","epoch 20 batch 0 loss = 16.577684\n","epoch 21 batch 0 loss = 16.542273\n","epoch 22 batch 0 loss = 16.541307\n","epoch 23 batch 0 loss = 16.429064\n","epoch 23 batch 1 loss = 16.507698\n","epoch 24 batch 0 loss = 16.484232\n","epoch 25 batch 0 loss = 16.484726\n","epoch 26 batch 0 loss = 16.48416\n","epoch 27 batch 0 loss = 16.481903\n","epoch 28 batch 0 loss = 16.462133\n","epoch 29 batch 0 loss = 16.458492\n","epoch 30 batch 0 loss = 16.46548\n","epoch 30 batch 1 loss = 16.630316\n","epoch 31 batch 0 loss = 16.63875\n","epoch 31 batch 1 loss = 16.555794\n","epoch 32 batch 0 loss = 16.446432\n","epoch 33 batch 0 loss = 16.60551\n","epoch 33 batch 1 loss = 16.438116\n","epoch 34 batch 0 loss = 16.360432\n","epoch 34 batch 1 loss = 16.268127\n","epoch 35 batch 0 loss = 16.63111\n","epoch 35 batch 1 loss = 16.448795\n","epoch 36 batch 0 loss = 16.430723\n","epoch 37 batch 0 loss = 16.446589\n","epoch 37 batch 1 loss = 16.529652\n","epoch 38 batch 0 loss = 16.4235\n","epoch 39 batch 0 loss = 16.41199\n","epoch 40 batch 0 loss = 16.59239\n","epoch 40 batch 1 loss = 16.424425\n","epoch 41 batch 0 loss = 16.42886\n","epoch 42 batch 0 loss = 16.421164\n","epoch 43 batch 0 loss = 16.416159\n","epoch 44 batch 0 loss = 16.416483\n","epoch 45 batch 0 loss = 16.410559\n","epoch 46 batch 0 loss = 16.416582\n","epoch 47 batch 0 loss = 16.482038\n","epoch 47 batch 1 loss = 16.409449\n","epoch 48 batch 0 loss = 16.59212\n","epoch 48 batch 1 loss = 16.416523\n","epoch 49 batch 0 loss = 16.593021\n","epoch 49 batch 1 loss = 16.542833\n","epoch 0 batch 0 loss = 26.889946\n","epoch 0 batch 1 loss = 25.182327\n","epoch 1 batch 0 loss = 24.714767\n","epoch 1 batch 1 loss = 23.527353\n","epoch 1 batch 2 loss = 22.361147\n","epoch 2 batch 0 loss = 20.2334\n","epoch 2 batch 1 loss = 19.047062\n","epoch 3 batch 0 loss = 17.890574\n","epoch 3 batch 1 loss = 17.045216\n","epoch 4 batch 0 loss = 15.8043585\n","epoch 4 batch 1 loss = 15.298961\n","epoch 4 batch 2 loss = 14.967499\n","epoch 4 batch 3 loss = 14.423216\n","epoch 5 batch 0 loss = 13.271405\n","epoch 5 batch 1 loss = 13.560618\n","epoch 6 batch 0 loss = 13.09094\n","epoch 6 batch 1 loss = 12.729066\n","epoch 6 batch 2 loss = 12.157849\n","epoch 7 batch 0 loss = 12.273923\n","epoch 7 batch 1 loss = 11.782004\n","epoch 7 batch 2 loss = 11.794964\n","epoch 8 batch 0 loss = 11.440006\n","epoch 8 batch 1 loss = 11.765814\n","epoch 8 batch 2 loss = 11.4970045\n","epoch 9 batch 0 loss = 11.515177\n","epoch 9 batch 1 loss = 11.792341\n","epoch 10 batch 0 loss = 11.293763\n","epoch 10 batch 1 loss = 11.875838\n","epoch 10 batch 2 loss = 11.348793\n","epoch 11 batch 0 loss = 11.324362\n","epoch 11 batch 1 loss = 11.362281\n","epoch 11 batch 2 loss = 11.3797\n","epoch 12 batch 0 loss = 11.71734\n","epoch 12 batch 1 loss = 11.159708\n","epoch 12 batch 2 loss = 11.338279\n","epoch 13 batch 0 loss = 11.323275\n","epoch 13 batch 1 loss = 11.5897875\n","epoch 13 batch 2 loss = 11.284258\n","epoch 14 batch 0 loss = 11.288003\n","epoch 14 batch 1 loss = 11.551273\n","epoch 14 batch 2 loss = 11.163576\n","epoch 15 batch 0 loss = 11.596713\n","epoch 15 batch 1 loss = 11.35273\n","epoch 15 batch 2 loss = 11.464723\n","epoch 16 batch 0 loss = 11.417404\n","epoch 16 batch 1 loss = 11.241318\n","epoch 16 batch 2 loss = 11.4572\n","epoch 17 batch 0 loss = 11.503275\n","epoch 17 batch 1 loss = 11.387907\n","epoch 18 batch 0 loss = 11.28063\n","epoch 18 batch 1 loss = 11.700547\n","epoch 19 batch 0 loss = 11.6460085\n","epoch 19 batch 1 loss = 11.770148\n","epoch 20 batch 0 loss = 11.51715\n","epoch 20 batch 1 loss = 11.268188\n","epoch 20 batch 2 loss = 11.581547\n","epoch 21 batch 0 loss = 11.508749\n","epoch 21 batch 1 loss = 11.401861\n","epoch 21 batch 2 loss = 11.656157\n","epoch 22 batch 0 loss = 11.674065\n","epoch 22 batch 1 loss = 11.616446\n","epoch 23 batch 0 loss = 11.290793\n","epoch 23 batch 1 loss = 11.330703\n","epoch 23 batch 2 loss = 11.454379\n","epoch 24 batch 0 loss = 11.417661\n","epoch 24 batch 1 loss = 11.597834\n","epoch 24 batch 2 loss = 11.623378\n","epoch 24 batch 3 loss = 11.55397\n","epoch 25 batch 0 loss = 11.355109\n","epoch 25 batch 1 loss = 11.509065\n","epoch 26 batch 0 loss = 11.25237\n","epoch 26 batch 1 loss = 11.460658\n","epoch 26 batch 2 loss = 11.398431\n","epoch 27 batch 0 loss = 11.491539\n","epoch 27 batch 1 loss = 11.28465\n","epoch 27 batch 2 loss = 11.320879\n","epoch 28 batch 0 loss = 11.633272\n","epoch 28 batch 1 loss = 11.410286\n","epoch 28 batch 2 loss = 11.51851\n","epoch 29 batch 0 loss = 11.669124\n","epoch 29 batch 1 loss = 11.433498\n","epoch 30 batch 0 loss = 11.080543\n","epoch 30 batch 1 loss = 11.424775\n","epoch 30 batch 2 loss = 11.657661\n","epoch 31 batch 0 loss = 11.517627\n","epoch 31 batch 1 loss = 11.404192\n","epoch 31 batch 2 loss = 11.445556\n","epoch 32 batch 0 loss = 11.360858\n","epoch 32 batch 1 loss = 11.610804\n","epoch 33 batch 0 loss = 11.581359\n","epoch 33 batch 1 loss = 11.588872\n","epoch 33 batch 2 loss = 11.106861\n","epoch 34 batch 0 loss = 11.368877\n","epoch 34 batch 1 loss = 11.623329\n","epoch 34 batch 2 loss = 11.332591\n","epoch 35 batch 0 loss = 11.764195\n","epoch 35 batch 1 loss = 11.36765\n","epoch 35 batch 2 loss = 11.427064\n","epoch 36 batch 0 loss = 11.451099\n","epoch 36 batch 1 loss = 11.613764\n","epoch 37 batch 0 loss = 11.360093\n","epoch 37 batch 1 loss = 11.540562\n","epoch 37 batch 2 loss = 11.810213\n","epoch 38 batch 0 loss = 11.992668\n","epoch 38 batch 1 loss = 11.382256\n","epoch 38 batch 2 loss = 11.456303\n","epoch 39 batch 0 loss = 11.443006\n","epoch 39 batch 1 loss = 11.604728\n","epoch 39 batch 2 loss = 11.212023\n","epoch 40 batch 0 loss = 11.67409\n","epoch 40 batch 1 loss = 11.562267\n","epoch 40 batch 2 loss = 11.365515\n","epoch 41 batch 0 loss = 11.542501\n","epoch 41 batch 1 loss = 11.423217\n","epoch 41 batch 2 loss = 12.01206\n","epoch 42 batch 0 loss = 11.476734\n","epoch 42 batch 1 loss = 11.385463\n","epoch 43 batch 0 loss = 11.671541\n","epoch 43 batch 1 loss = 11.4122\n","epoch 43 batch 2 loss = 11.691341\n","epoch 44 batch 0 loss = 11.355641\n","epoch 44 batch 1 loss = 11.562215\n","epoch 45 batch 0 loss = 11.721882\n","epoch 45 batch 1 loss = 11.439449\n","epoch 46 batch 0 loss = 11.4558935\n","epoch 46 batch 1 loss = 11.152105\n","epoch 47 batch 0 loss = 11.819039\n","epoch 47 batch 1 loss = 11.410946\n","epoch 47 batch 2 loss = 11.319019\n","epoch 48 batch 0 loss = 11.377011\n","epoch 48 batch 1 loss = 11.458457\n","epoch 49 batch 0 loss = 12.013993\n","epoch 49 batch 1 loss = 11.209164\n","epoch 49 batch 2 loss = 11.206435\n","epoch 49 batch 3 loss = 11.341238\n","epoch 49 batch 4 loss = 11.458554\n","epoch 0 batch 0 loss = 6.5406895\n","epoch 0 batch 1 loss = 6.99027\n","epoch 0 batch 2 loss = 6.7524056\n","epoch 0 batch 3 loss = 7.115331\n","epoch 0 batch 4 loss = 6.836669\n","epoch 0 batch 5 loss = 6.843852\n","epoch 0 batch 6 loss = 6.7738333\n","epoch 0 batch 7 loss = 7.110262\n","epoch 0 batch 8 loss = 6.7126164\n","epoch 1 batch 0 loss = 6.563848\n","epoch 1 batch 1 loss = 6.619157\n","epoch 1 batch 2 loss = 6.7003884\n","epoch 1 batch 3 loss = 6.7597375\n","epoch 1 batch 4 loss = 6.5689597\n","epoch 1 batch 5 loss = 6.671508\n","epoch 1 batch 6 loss = 6.8055344\n","epoch 1 batch 7 loss = 6.6758285\n","epoch 1 batch 8 loss = 6.455517\n","epoch 1 batch 9 loss = 6.8327913\n","epoch 1 batch 10 loss = 6.307974\n","epoch 1 batch 11 loss = 6.3836236\n","epoch 1 batch 12 loss = 6.4089775\n","epoch 2 batch 0 loss = 6.3808093\n","epoch 2 batch 1 loss = 6.688905\n","epoch 2 batch 2 loss = 6.514462\n","epoch 2 batch 3 loss = 6.451138\n","epoch 2 batch 4 loss = 6.354757\n","epoch 2 batch 5 loss = 6.62518\n","epoch 2 batch 6 loss = 6.649017\n","epoch 2 batch 7 loss = 6.511777\n","epoch 2 batch 8 loss = 5.994933\n","epoch 2 batch 9 loss = 6.6171107\n","epoch 2 batch 10 loss = 6.160237\n","epoch 2 batch 11 loss = 6.267741\n","epoch 2 batch 12 loss = 6.066877\n","epoch 3 batch 0 loss = 6.4681764\n","epoch 3 batch 1 loss = 6.0662327\n","epoch 3 batch 2 loss = 6.408759\n","epoch 3 batch 3 loss = 6.3278747\n","epoch 3 batch 4 loss = 6.601672\n","epoch 3 batch 5 loss = 6.053842\n","epoch 3 batch 6 loss = 6.3518066\n","epoch 3 batch 7 loss = 6.1085496\n","epoch 3 batch 8 loss = 5.951892\n","epoch 3 batch 9 loss = 5.8269563\n","epoch 3 batch 10 loss = 5.855591\n","epoch 3 batch 11 loss = 5.9511905\n","epoch 3 batch 12 loss = 5.9150953\n","epoch 3 batch 13 loss = 5.99172\n","epoch 3 batch 14 loss = 5.871193\n","epoch 3 batch 15 loss = 5.953978\n","epoch 3 batch 16 loss = 5.9787383\n","epoch 4 batch 0 loss = 5.983936\n","epoch 4 batch 1 loss = 5.7877464\n","epoch 4 batch 2 loss = 5.480783\n","epoch 4 batch 3 loss = 5.7364516\n","epoch 4 batch 4 loss = 5.6235704\n","epoch 4 batch 5 loss = 5.511947\n","epoch 4 batch 6 loss = 5.911552\n","epoch 4 batch 7 loss = 5.4638953\n","epoch 4 batch 8 loss = 5.295021\n","epoch 4 batch 9 loss = 5.559017\n","epoch 4 batch 10 loss = 5.4042664\n","epoch 5 batch 0 loss = 5.4584675\n","epoch 5 batch 1 loss = 5.4185214\n","epoch 5 batch 2 loss = 5.506639\n","epoch 5 batch 3 loss = 5.392045\n","epoch 5 batch 4 loss = 5.221688\n","epoch 5 batch 5 loss = 5.453313\n","epoch 5 batch 6 loss = 4.9030814\n","epoch 5 batch 7 loss = 5.2435346\n","epoch 5 batch 8 loss = 5.2429037\n","epoch 5 batch 9 loss = 5.1832423\n","epoch 5 batch 10 loss = 5.04116\n","epoch 6 batch 0 loss = 5.1946917\n","epoch 6 batch 1 loss = 4.7903595\n","epoch 6 batch 2 loss = 5.0605626\n","epoch 6 batch 3 loss = 5.202602\n","epoch 6 batch 4 loss = 4.9600887\n","epoch 6 batch 5 loss = 4.710033\n","epoch 6 batch 6 loss = 4.8338404\n","epoch 6 batch 7 loss = 4.8887367\n","epoch 6 batch 8 loss = 4.902296\n","epoch 6 batch 9 loss = 4.850832\n","epoch 6 batch 10 loss = 4.709959\n","epoch 7 batch 0 loss = 4.6557956\n","epoch 7 batch 1 loss = 4.62366\n","epoch 7 batch 2 loss = 4.8781476\n","epoch 7 batch 3 loss = 4.7298985\n","epoch 7 batch 4 loss = 4.709795\n","epoch 7 batch 5 loss = 4.5561337\n","epoch 7 batch 6 loss = 4.4812856\n","epoch 7 batch 7 loss = 4.485428\n","epoch 7 batch 8 loss = 4.4346576\n","epoch 7 batch 9 loss = 4.422723\n","epoch 8 batch 0 loss = 4.34472\n","epoch 8 batch 1 loss = 4.550726\n","epoch 8 batch 2 loss = 4.3992233\n","epoch 8 batch 3 loss = 4.380091\n","epoch 8 batch 4 loss = 4.3984747\n","epoch 8 batch 5 loss = 4.0770426\n","epoch 8 batch 6 loss = 4.482819\n","epoch 8 batch 7 loss = 4.329766\n","epoch 9 batch 0 loss = 4.065063\n","epoch 9 batch 1 loss = 4.2297378\n","epoch 9 batch 2 loss = 4.393846\n","epoch 9 batch 3 loss = 4.131997\n","epoch 9 batch 4 loss = 4.113613\n","epoch 9 batch 5 loss = 3.9370368\n","epoch 9 batch 6 loss = 4.1300664\n","epoch 9 batch 7 loss = 4.06806\n","epoch 9 batch 8 loss = 4.1155553\n","epoch 9 batch 9 loss = 4.0970464\n","epoch 9 batch 10 loss = 4.047669\n","epoch 10 batch 0 loss = 3.919648\n","epoch 10 batch 1 loss = 3.8384962\n","epoch 10 batch 2 loss = 3.8878782\n","epoch 10 batch 3 loss = 3.948272\n","epoch 10 batch 4 loss = 3.6762967\n","epoch 10 batch 5 loss = 3.6939023\n","epoch 10 batch 6 loss = 3.7105963\n","epoch 10 batch 7 loss = 3.8305676\n","epoch 10 batch 8 loss = 3.8546553\n","epoch 11 batch 0 loss = 3.7616713\n","epoch 11 batch 1 loss = 3.5895061\n","epoch 11 batch 2 loss = 3.8157516\n","epoch 11 batch 3 loss = 3.7173245\n","epoch 11 batch 4 loss = 3.6810076\n","epoch 11 batch 5 loss = 3.6607528\n","epoch 11 batch 6 loss = 3.3186636\n","epoch 11 batch 7 loss = 3.409933\n","epoch 11 batch 8 loss = 3.4544857\n","epoch 11 batch 9 loss = 3.6859705\n","epoch 11 batch 10 loss = 3.3527718\n","epoch 11 batch 11 loss = 3.558236\n","epoch 11 batch 12 loss = 3.3383696\n","epoch 12 batch 0 loss = 3.3841603\n","epoch 12 batch 1 loss = 3.3717232\n","epoch 12 batch 2 loss = 3.2487044\n","epoch 12 batch 3 loss = 3.4133575\n","epoch 12 batch 4 loss = 3.2054033\n","epoch 12 batch 5 loss = 3.406262\n","epoch 12 batch 6 loss = 3.4076686\n","epoch 12 batch 7 loss = 3.3826156\n","epoch 12 batch 8 loss = 3.2006476\n","epoch 12 batch 9 loss = 3.2788656\n","epoch 13 batch 0 loss = 3.3301833\n","epoch 13 batch 1 loss = 3.1570866\n","epoch 13 batch 2 loss = 3.4488175\n","epoch 13 batch 3 loss = 3.2127104\n","epoch 13 batch 4 loss = 3.3451784\n","epoch 13 batch 5 loss = 3.3215897\n","epoch 13 batch 6 loss = 3.0997827\n","epoch 13 batch 7 loss = 3.206898\n","epoch 13 batch 8 loss = 3.2118006\n","epoch 13 batch 9 loss = 3.2076445\n","epoch 13 batch 10 loss = 3.0924392\n","epoch 13 batch 11 loss = 3.086973\n","epoch 13 batch 12 loss = 3.2024062\n","epoch 14 batch 0 loss = 2.9933777\n","epoch 14 batch 1 loss = 3.1642363\n","epoch 14 batch 2 loss = 3.1281283\n","epoch 14 batch 3 loss = 3.113809\n","epoch 14 batch 4 loss = 3.0865068\n","epoch 14 batch 5 loss = 3.0824337\n","epoch 14 batch 6 loss = 2.9568303\n","epoch 14 batch 7 loss = 3.1236138\n","epoch 14 batch 8 loss = 3.2814386\n","epoch 14 batch 9 loss = 3.094812\n","epoch 14 batch 10 loss = 3.299108\n","epoch 14 batch 11 loss = 2.8762627\n","epoch 15 batch 0 loss = 3.0958557\n","epoch 15 batch 1 loss = 3.1174345\n","epoch 15 batch 2 loss = 2.9872522\n","epoch 15 batch 3 loss = 3.0066338\n","epoch 15 batch 4 loss = 2.933838\n","epoch 15 batch 5 loss = 3.1478407\n","epoch 15 batch 6 loss = 2.8640325\n","epoch 15 batch 7 loss = 3.0823643\n","epoch 15 batch 8 loss = 2.9568448\n","epoch 15 batch 9 loss = 3.1951334\n","epoch 16 batch 0 loss = 3.068963\n","epoch 16 batch 1 loss = 3.150521\n","epoch 16 batch 2 loss = 2.9744115\n","epoch 16 batch 3 loss = 3.0033677\n","epoch 16 batch 4 loss = 3.1336365\n","epoch 16 batch 5 loss = 2.770664\n","epoch 16 batch 6 loss = 2.9184875\n","epoch 16 batch 7 loss = 3.0253637\n","epoch 16 batch 8 loss = 2.9903648\n","epoch 16 batch 9 loss = 3.1308222\n","epoch 16 batch 10 loss = 3.0484502\n","epoch 16 batch 11 loss = 3.029827\n","epoch 17 batch 0 loss = 3.0566816\n","epoch 17 batch 1 loss = 3.0669534\n","epoch 17 batch 2 loss = 2.981758\n","epoch 17 batch 3 loss = 3.068773\n","epoch 17 batch 4 loss = 3.0777972\n","epoch 17 batch 5 loss = 3.06392\n","epoch 17 batch 6 loss = 3.1568143\n","epoch 17 batch 7 loss = 3.0507298\n","epoch 17 batch 8 loss = 3.148997\n","epoch 17 batch 9 loss = 2.9357862\n","epoch 17 batch 10 loss = 3.0460207\n","epoch 18 batch 0 loss = 2.9737535\n","epoch 18 batch 1 loss = 3.0960436\n","epoch 18 batch 2 loss = 2.9619966\n","epoch 18 batch 3 loss = 3.0387852\n","epoch 18 batch 4 loss = 3.137738\n","epoch 18 batch 5 loss = 3.0355573\n","epoch 18 batch 6 loss = 3.1475391\n","epoch 18 batch 7 loss = 2.9723172\n","epoch 18 batch 8 loss = 3.0975003\n","epoch 18 batch 9 loss = 2.9116302\n","epoch 18 batch 10 loss = 3.0401316\n","epoch 19 batch 0 loss = 2.9138334\n","epoch 19 batch 1 loss = 3.173923\n","epoch 19 batch 2 loss = 3.057023\n","epoch 19 batch 3 loss = 3.0095599\n","epoch 19 batch 4 loss = 3.0198174\n","epoch 19 batch 5 loss = 3.0489511\n","epoch 19 batch 6 loss = 2.9916391\n","epoch 19 batch 7 loss = 3.0286179\n","epoch 20 batch 0 loss = 2.9220083\n","epoch 20 batch 1 loss = 2.8440125\n","epoch 20 batch 2 loss = 3.1296427\n","epoch 20 batch 3 loss = 3.0189557\n","epoch 20 batch 4 loss = 2.9256408\n","epoch 20 batch 5 loss = 2.9768174\n","epoch 20 batch 6 loss = 3.0016713\n","epoch 20 batch 7 loss = 3.0054417\n","epoch 20 batch 8 loss = 3.0505679\n","epoch 20 batch 9 loss = 3.0660553\n","epoch 21 batch 0 loss = 2.8964615\n","epoch 21 batch 1 loss = 2.9272463\n","epoch 21 batch 2 loss = 3.0446057\n","epoch 21 batch 3 loss = 2.9619293\n","epoch 21 batch 4 loss = 2.895037\n","epoch 21 batch 5 loss = 2.983774\n","epoch 21 batch 6 loss = 3.0171561\n","epoch 21 batch 7 loss = 2.910182\n","epoch 21 batch 8 loss = 2.8304875\n","epoch 22 batch 0 loss = 2.9987934\n","epoch 22 batch 1 loss = 3.019942\n","epoch 22 batch 2 loss = 2.8598342\n","epoch 22 batch 3 loss = 3.0131705\n","epoch 22 batch 4 loss = 3.069688\n","epoch 22 batch 5 loss = 2.9253638\n","epoch 22 batch 6 loss = 2.9561284\n","epoch 22 batch 7 loss = 3.065379\n","epoch 22 batch 8 loss = 2.8565397\n","epoch 22 batch 9 loss = 2.8565814\n","epoch 22 batch 10 loss = 2.752892\n","epoch 22 batch 11 loss = 3.0498402\n","epoch 23 batch 0 loss = 3.049013\n","epoch 23 batch 1 loss = 2.9974332\n","epoch 23 batch 2 loss = 2.925115\n","epoch 23 batch 3 loss = 2.9879658\n","epoch 23 batch 4 loss = 3.0200233\n","epoch 23 batch 5 loss = 3.0385613\n","epoch 23 batch 6 loss = 3.0064976\n","epoch 23 batch 7 loss = 2.8410532\n","epoch 23 batch 8 loss = 2.9406426\n","epoch 23 batch 9 loss = 2.6238167\n","epoch 23 batch 10 loss = 3.02881\n","epoch 23 batch 11 loss = 2.6618226\n","epoch 23 batch 12 loss = 2.9254303\n","epoch 23 batch 13 loss = 2.8356285\n","epoch 23 batch 14 loss = 2.8486526\n","epoch 24 batch 0 loss = 3.0514474\n","epoch 24 batch 1 loss = 2.94056\n","epoch 24 batch 2 loss = 3.002558\n","epoch 24 batch 3 loss = 2.960012\n","epoch 24 batch 4 loss = 2.9440224\n","epoch 24 batch 5 loss = 2.9124603\n","epoch 24 batch 6 loss = 2.8354342\n","epoch 24 batch 7 loss = 2.9743922\n","epoch 24 batch 8 loss = 2.9712074\n","epoch 24 batch 9 loss = 3.0402558\n","epoch 24 batch 10 loss = 2.859038\n","epoch 24 batch 11 loss = 2.980662\n","epoch 24 batch 12 loss = 2.97254\n","epoch 25 batch 0 loss = 3.027118\n","epoch 25 batch 1 loss = 2.888218\n","epoch 25 batch 2 loss = 2.886041\n","epoch 25 batch 3 loss = 3.0356836\n","epoch 25 batch 4 loss = 2.8602405\n","epoch 25 batch 5 loss = 2.9285958\n","epoch 25 batch 6 loss = 2.877028\n","epoch 25 batch 7 loss = 2.9210396\n","epoch 25 batch 8 loss = 2.8791492\n","epoch 26 batch 0 loss = 2.910437\n","epoch 26 batch 1 loss = 2.9311614\n","epoch 26 batch 2 loss = 2.9774075\n","epoch 26 batch 3 loss = 2.9613554\n","epoch 26 batch 4 loss = 2.8594394\n","epoch 26 batch 5 loss = 3.0385711\n","epoch 26 batch 6 loss = 3.0364068\n","epoch 26 batch 7 loss = 3.0095363\n","epoch 27 batch 0 loss = 2.893737\n","epoch 27 batch 1 loss = 3.046753\n","epoch 27 batch 2 loss = 3.0200467\n","epoch 27 batch 3 loss = 3.0247025\n","epoch 27 batch 4 loss = 3.0306385\n","epoch 27 batch 5 loss = 2.9696918\n","epoch 27 batch 6 loss = 2.9797823\n","epoch 27 batch 7 loss = 2.8182478\n","epoch 27 batch 8 loss = 2.944491\n","epoch 27 batch 9 loss = 2.9616303\n","epoch 27 batch 10 loss = 2.8293247\n","epoch 28 batch 0 loss = 3.0639932\n","epoch 28 batch 1 loss = 2.7619283\n","epoch 28 batch 2 loss = 2.8580556\n","epoch 28 batch 3 loss = 3.0052161\n","epoch 28 batch 4 loss = 2.9960954\n","epoch 28 batch 5 loss = 3.0220666\n","epoch 28 batch 6 loss = 3.0748634\n","epoch 28 batch 7 loss = 2.8713183\n","epoch 28 batch 8 loss = 2.9576967\n","epoch 28 batch 9 loss = 2.9942353\n","epoch 28 batch 10 loss = 2.7956295\n","epoch 29 batch 0 loss = 2.948361\n","epoch 29 batch 1 loss = 2.9175546\n","epoch 29 batch 2 loss = 3.0704408\n","epoch 29 batch 3 loss = 2.9914794\n","epoch 29 batch 4 loss = 2.9230928\n","epoch 29 batch 5 loss = 2.8560643\n","epoch 29 batch 6 loss = 2.9992445\n","epoch 29 batch 7 loss = 3.0248718\n","epoch 29 batch 8 loss = 2.8815668\n","epoch 29 batch 9 loss = 2.8337457\n","epoch 29 batch 10 loss = 2.9319572\n","epoch 29 batch 11 loss = 3.063048\n","epoch 29 batch 12 loss = 2.818405\n","epoch 29 batch 13 loss = 2.9134643\n","epoch 29 batch 14 loss = 2.900474\n","epoch 30 batch 0 loss = 2.7631357\n","epoch 30 batch 1 loss = 3.1401234\n","epoch 30 batch 2 loss = 3.0270312\n","epoch 30 batch 3 loss = 2.921288\n","epoch 30 batch 4 loss = 2.8984468\n","epoch 30 batch 5 loss = 2.7801716\n","epoch 30 batch 6 loss = 3.0154815\n","epoch 30 batch 7 loss = 2.9607632\n","epoch 31 batch 0 loss = 2.8602765\n","epoch 31 batch 1 loss = 3.054911\n","epoch 31 batch 2 loss = 2.8883152\n","epoch 31 batch 3 loss = 2.9323435\n","epoch 31 batch 4 loss = 2.918474\n","epoch 31 batch 5 loss = 2.889942\n","epoch 31 batch 6 loss = 2.9992127\n","epoch 31 batch 7 loss = 2.9521568\n","epoch 31 batch 8 loss = 2.6590557\n","epoch 31 batch 9 loss = 3.0290468\n","epoch 31 batch 10 loss = 2.8956866\n","epoch 32 batch 0 loss = 3.1256945\n","epoch 32 batch 1 loss = 2.9185076\n","epoch 32 batch 2 loss = 2.8851597\n","epoch 32 batch 3 loss = 2.9673507\n","epoch 32 batch 4 loss = 3.0209548\n","epoch 32 batch 5 loss = 2.9207625\n","epoch 32 batch 6 loss = 2.682571\n","epoch 32 batch 7 loss = 2.9157376\n","epoch 32 batch 8 loss = 2.887651\n","epoch 32 batch 9 loss = 3.0568666\n","epoch 32 batch 10 loss = 2.9605927\n","epoch 32 batch 11 loss = 2.9320643\n","epoch 32 batch 12 loss = 2.7837732\n","epoch 32 batch 13 loss = 2.8723426\n","epoch 32 batch 14 loss = 2.93998\n","epoch 33 batch 0 loss = 2.843774\n","epoch 33 batch 1 loss = 2.9360104\n","epoch 33 batch 2 loss = 3.147097\n","epoch 33 batch 3 loss = 2.8474727\n","epoch 33 batch 4 loss = 3.076344\n","epoch 33 batch 5 loss = 2.807994\n","epoch 33 batch 6 loss = 2.8219442\n","epoch 33 batch 7 loss = 2.9901917\n","epoch 33 batch 8 loss = 2.916934\n","epoch 33 batch 9 loss = 2.8519323\n","epoch 33 batch 10 loss = 3.0019696\n","epoch 33 batch 11 loss = 2.94527\n","epoch 33 batch 12 loss = 2.940297\n","epoch 33 batch 13 loss = 2.8148315\n","epoch 34 batch 0 loss = 2.7197511\n","epoch 34 batch 1 loss = 2.7380908\n","epoch 34 batch 2 loss = 2.8765895\n","epoch 34 batch 3 loss = 2.8593853\n","epoch 34 batch 4 loss = 3.0596836\n","epoch 34 batch 5 loss = 2.97408\n","epoch 34 batch 6 loss = 2.921457\n","epoch 35 batch 0 loss = 2.9914896\n","epoch 35 batch 1 loss = 2.9401152\n","epoch 35 batch 2 loss = 2.819124\n","epoch 35 batch 3 loss = 2.9266746\n","epoch 35 batch 4 loss = 2.9668703\n","epoch 35 batch 5 loss = 2.807731\n","epoch 35 batch 6 loss = 3.0258136\n","epoch 35 batch 7 loss = 3.0513277\n","epoch 35 batch 8 loss = 2.8798358\n","epoch 35 batch 9 loss = 2.8973618\n","epoch 35 batch 10 loss = 2.9764159\n","epoch 35 batch 11 loss = 2.7855225\n","epoch 35 batch 12 loss = 2.909257\n","epoch 36 batch 0 loss = 2.7858515\n","epoch 36 batch 1 loss = 3.0481827\n","epoch 36 batch 2 loss = 2.9287906\n","epoch 36 batch 3 loss = 3.0393212\n","epoch 36 batch 4 loss = 2.847157\n","epoch 36 batch 5 loss = 2.9232788\n","epoch 36 batch 6 loss = 2.9033535\n","epoch 36 batch 7 loss = 2.7929537\n","epoch 36 batch 8 loss = 2.81789\n","epoch 36 batch 9 loss = 2.861877\n","epoch 36 batch 10 loss = 2.8978057\n","epoch 37 batch 0 loss = 2.9204612\n","epoch 37 batch 1 loss = 2.8717916\n","epoch 37 batch 2 loss = 2.9551682\n","epoch 37 batch 3 loss = 2.9915955\n","epoch 37 batch 4 loss = 2.6887646\n","epoch 37 batch 5 loss = 3.0873394\n","epoch 37 batch 6 loss = 2.8254478\n","epoch 37 batch 7 loss = 2.9916782\n","epoch 37 batch 8 loss = 2.9229162\n","epoch 37 batch 9 loss = 2.9697685\n","epoch 37 batch 10 loss = 2.88312\n","epoch 37 batch 11 loss = 2.8375816\n","epoch 37 batch 12 loss = 2.9440086\n","epoch 37 batch 13 loss = 2.8482013\n","epoch 38 batch 0 loss = 3.1138043\n","epoch 38 batch 1 loss = 2.7432797\n","epoch 38 batch 2 loss = 2.8459105\n","epoch 38 batch 3 loss = 3.015493\n","epoch 38 batch 4 loss = 3.0054116\n","epoch 38 batch 5 loss = 2.8744555\n","epoch 38 batch 6 loss = 2.862221\n","epoch 38 batch 7 loss = 2.8917484\n","epoch 38 batch 8 loss = 2.8654387\n","epoch 38 batch 9 loss = 2.7757704\n","epoch 38 batch 10 loss = 2.898186\n","epoch 38 batch 11 loss = 2.863818\n","epoch 39 batch 0 loss = 3.0117142\n","epoch 39 batch 1 loss = 3.0105598\n","epoch 39 batch 2 loss = 2.8028784\n","epoch 39 batch 3 loss = 2.8702204\n","epoch 39 batch 4 loss = 2.7214215\n","epoch 39 batch 5 loss = 2.9043546\n","epoch 39 batch 6 loss = 2.9545138\n","epoch 39 batch 7 loss = 2.984363\n","epoch 39 batch 8 loss = 2.847402\n","epoch 40 batch 0 loss = 2.9837198\n","epoch 40 batch 1 loss = 2.900988\n","epoch 40 batch 2 loss = 2.9752843\n","epoch 40 batch 3 loss = 2.7348564\n","epoch 40 batch 4 loss = 2.8297055\n","epoch 40 batch 5 loss = 2.7752228\n","epoch 40 batch 6 loss = 2.9968202\n","epoch 40 batch 7 loss = 2.7507212\n","epoch 40 batch 8 loss = 3.02891\n","epoch 40 batch 9 loss = 2.57994\n","epoch 41 batch 0 loss = 2.8300307\n","epoch 41 batch 1 loss = 2.9250135\n","epoch 41 batch 2 loss = 3.050022\n","epoch 41 batch 3 loss = 2.8820007\n","epoch 41 batch 4 loss = 2.9002962\n","epoch 41 batch 5 loss = 2.8547711\n","epoch 41 batch 6 loss = 2.9470131\n","epoch 41 batch 7 loss = 2.925988\n","epoch 41 batch 8 loss = 2.8362286\n","epoch 41 batch 9 loss = 3.0440612\n","epoch 41 batch 10 loss = 2.7981586\n","epoch 42 batch 0 loss = 2.8275876\n","epoch 42 batch 1 loss = 2.9025257\n","epoch 42 batch 2 loss = 2.8758893\n","epoch 42 batch 3 loss = 2.8980143\n","epoch 42 batch 4 loss = 2.7984638\n","epoch 42 batch 5 loss = 3.0203593\n","epoch 42 batch 6 loss = 2.8703256\n","epoch 42 batch 7 loss = 2.715681\n","epoch 42 batch 8 loss = 3.0031142\n","epoch 42 batch 9 loss = 2.897791\n","epoch 42 batch 10 loss = 2.9972935\n","epoch 42 batch 11 loss = 2.8686512\n","epoch 42 batch 12 loss = 2.7293437\n","epoch 43 batch 0 loss = 2.8879342\n","epoch 43 batch 1 loss = 2.9231026\n","epoch 43 batch 2 loss = 2.9335682\n","epoch 43 batch 3 loss = 2.8037105\n","epoch 43 batch 4 loss = 2.9063776\n","epoch 43 batch 5 loss = 2.914032\n","epoch 43 batch 6 loss = 2.9329312\n","epoch 43 batch 7 loss = 3.0103781\n","epoch 43 batch 8 loss = 2.879079\n","epoch 43 batch 9 loss = 2.9023526\n","epoch 43 batch 10 loss = 2.8084667\n","epoch 43 batch 11 loss = 2.8477094\n","epoch 44 batch 0 loss = 2.862278\n","epoch 44 batch 1 loss = 2.9054632\n","epoch 44 batch 2 loss = 2.4477828\n","epoch 44 batch 3 loss = 2.9222078\n","epoch 44 batch 4 loss = 2.99621\n","epoch 44 batch 5 loss = 2.9571662\n","epoch 44 batch 6 loss = 2.717907\n","epoch 44 batch 7 loss = 3.0294726\n","epoch 44 batch 8 loss = 2.9528897\n","epoch 45 batch 0 loss = 2.8345864\n","epoch 45 batch 1 loss = 2.966876\n","epoch 45 batch 2 loss = 2.8896198\n","epoch 45 batch 3 loss = 3.03379\n","epoch 45 batch 4 loss = 2.901309\n","epoch 45 batch 5 loss = 2.8393338\n","epoch 45 batch 6 loss = 2.9286287\n","epoch 45 batch 7 loss = 2.9382524\n","epoch 45 batch 8 loss = 2.9140973\n","epoch 45 batch 9 loss = 2.7398956\n","epoch 46 batch 0 loss = 2.902818\n","epoch 46 batch 1 loss = 2.7543476\n","epoch 46 batch 2 loss = 2.8805134\n","epoch 46 batch 3 loss = 2.889057\n","epoch 46 batch 4 loss = 2.8036027\n","epoch 46 batch 5 loss = 2.8750238\n","epoch 46 batch 6 loss = 2.8812692\n","epoch 46 batch 7 loss = 2.8039057\n","epoch 46 batch 8 loss = 2.9640784\n","epoch 47 batch 0 loss = 2.7995648\n","epoch 47 batch 1 loss = 2.8035767\n","epoch 47 batch 2 loss = 3.0307264\n","epoch 47 batch 3 loss = 2.9690588\n","epoch 47 batch 4 loss = 2.937019\n","epoch 47 batch 5 loss = 3.03158\n","epoch 47 batch 6 loss = 2.880492\n","epoch 47 batch 7 loss = 2.95981\n","epoch 47 batch 8 loss = 2.8548918\n","epoch 47 batch 9 loss = 2.8578904\n","epoch 47 batch 10 loss = 2.9121482\n","epoch 47 batch 11 loss = 2.8411846\n","epoch 47 batch 12 loss = 2.813266\n","epoch 48 batch 0 loss = 2.9937413\n","epoch 48 batch 1 loss = 2.8349447\n","epoch 48 batch 2 loss = 2.7568862\n","epoch 48 batch 3 loss = 2.843091\n","epoch 48 batch 4 loss = 2.9669971\n","epoch 48 batch 5 loss = 2.968227\n","epoch 48 batch 6 loss = 2.8740995\n","epoch 48 batch 7 loss = 2.813475\n","epoch 48 batch 8 loss = 2.9334385\n","epoch 48 batch 9 loss = 2.814829\n","epoch 48 batch 10 loss = 2.9236736\n","epoch 49 batch 0 loss = 2.8556414\n","epoch 49 batch 1 loss = 2.913746\n","epoch 49 batch 2 loss = 2.7611427\n","epoch 49 batch 3 loss = 2.8261876\n","epoch 49 batch 4 loss = 2.7289789\n","epoch 49 batch 5 loss = 2.9146242\n","epoch 49 batch 6 loss = 2.9853978\n","epoch 49 batch 7 loss = 2.921625\n","epoch 49 batch 8 loss = 2.9639435\n","epoch 0 batch 0 loss = 14.536257\n","epoch 0 batch 1 loss = 15.105239\n","epoch 0 batch 2 loss = 14.987532\n","epoch 1 batch 0 loss = 14.762245\n","epoch 1 batch 1 loss = 14.242084\n","epoch 1 batch 2 loss = 14.148017\n","epoch 2 batch 0 loss = 13.862671\n","epoch 2 batch 1 loss = 14.205114\n","epoch 2 batch 2 loss = 13.974251\n","epoch 3 batch 0 loss = 13.81404\n","epoch 3 batch 1 loss = 13.840667\n","epoch 3 batch 2 loss = 14.154658\n","epoch 3 batch 3 loss = 13.858294\n","epoch 3 batch 4 loss = 13.605079\n","epoch 4 batch 0 loss = 13.321334\n","epoch 4 batch 1 loss = 13.170466\n","epoch 4 batch 2 loss = 13.176488\n","epoch 4 batch 3 loss = 13.936194\n","epoch 5 batch 0 loss = 13.210482\n","epoch 5 batch 1 loss = 12.97252\n","epoch 5 batch 2 loss = 12.773476\n","epoch 5 batch 3 loss = 12.990782\n","epoch 6 batch 0 loss = 12.787857\n","epoch 6 batch 1 loss = 12.638741\n","epoch 6 batch 2 loss = 12.623128\n","epoch 7 batch 0 loss = 12.525277\n","epoch 7 batch 1 loss = 11.9702835\n","epoch 7 batch 2 loss = 12.710616\n","epoch 7 batch 3 loss = 12.600542\n","epoch 7 batch 4 loss = 12.359875\n","epoch 7 batch 5 loss = 11.898096\n","epoch 8 batch 0 loss = 11.905247\n","epoch 8 batch 1 loss = 11.763847\n","epoch 9 batch 0 loss = 12.120943\n","epoch 9 batch 1 loss = 11.852699\n","epoch 9 batch 2 loss = 11.64317\n","epoch 10 batch 0 loss = 11.984871\n","epoch 10 batch 1 loss = 11.165567\n","epoch 10 batch 2 loss = 11.1520405\n","epoch 11 batch 0 loss = 10.625858\n","epoch 11 batch 1 loss = 11.114747\n","epoch 11 batch 2 loss = 10.870541\n","epoch 12 batch 0 loss = 11.18611\n","epoch 12 batch 1 loss = 10.984811\n","epoch 12 batch 2 loss = 10.817477\n","epoch 13 batch 0 loss = 10.54852\n","epoch 13 batch 1 loss = 10.053119\n","epoch 13 batch 2 loss = 10.054668\n","epoch 14 batch 0 loss = 10.42746\n","epoch 14 batch 1 loss = 10.24645\n","epoch 14 batch 2 loss = 9.981058\n","epoch 15 batch 0 loss = 9.759735\n","epoch 15 batch 1 loss = 9.69759\n","epoch 15 batch 2 loss = 9.832822\n","epoch 15 batch 3 loss = 9.685483\n","epoch 16 batch 0 loss = 9.907506\n","epoch 16 batch 1 loss = 9.947088\n","epoch 16 batch 2 loss = 9.826987\n","epoch 17 batch 0 loss = 9.379323\n","epoch 17 batch 1 loss = 9.40846\n","epoch 17 batch 2 loss = 9.751105\n","epoch 17 batch 3 loss = 9.485898\n","epoch 18 batch 0 loss = 9.295063\n","epoch 18 batch 1 loss = 9.140081\n","epoch 18 batch 2 loss = 9.453645\n","epoch 19 batch 0 loss = 9.080716\n","epoch 19 batch 1 loss = 8.771444\n","epoch 19 batch 2 loss = 8.970861\n","epoch 20 batch 0 loss = 8.898556\n","epoch 20 batch 1 loss = 8.949256\n","epoch 20 batch 2 loss = 8.642395\n","epoch 21 batch 0 loss = 8.79296\n","epoch 21 batch 1 loss = 8.778053\n","epoch 21 batch 2 loss = 8.699694\n","epoch 22 batch 0 loss = 8.802623\n","epoch 22 batch 1 loss = 8.829277\n","epoch 22 batch 2 loss = 8.560974\n","epoch 23 batch 0 loss = 8.403194\n","epoch 23 batch 1 loss = 8.531497\n","epoch 23 batch 2 loss = 8.559159\n","epoch 23 batch 3 loss = 8.379814\n","epoch 24 batch 0 loss = 8.586884\n","epoch 24 batch 1 loss = 8.299048\n","epoch 24 batch 2 loss = 8.336754\n","epoch 24 batch 3 loss = 8.248575\n","epoch 25 batch 0 loss = 8.209187\n","epoch 25 batch 1 loss = 8.1172905\n","epoch 25 batch 2 loss = 7.929517\n","epoch 26 batch 0 loss = 7.955525\n","epoch 26 batch 1 loss = 7.9769006\n","epoch 26 batch 2 loss = 7.932101\n","epoch 26 batch 3 loss = 8.248724\n","epoch 27 batch 0 loss = 8.008612\n","epoch 27 batch 1 loss = 8.082173\n","epoch 27 batch 2 loss = 7.933533\n","epoch 28 batch 0 loss = 8.005289\n","epoch 28 batch 1 loss = 7.971524\n","epoch 28 batch 2 loss = 8.102415\n","epoch 29 batch 0 loss = 7.7350492\n","epoch 29 batch 1 loss = 8.116345\n","epoch 29 batch 2 loss = 8.114804\n","epoch 29 batch 3 loss = 8.021285\n","epoch 30 batch 0 loss = 8.072846\n","epoch 30 batch 1 loss = 7.978194\n","epoch 30 batch 2 loss = 7.6987324\n","epoch 31 batch 0 loss = 7.9356785\n","epoch 31 batch 1 loss = 7.891326\n","epoch 31 batch 2 loss = 7.741509\n","epoch 32 batch 0 loss = 8.114695\n","epoch 32 batch 1 loss = 7.92442\n","epoch 32 batch 2 loss = 7.987207\n","epoch 33 batch 0 loss = 7.936957\n","epoch 33 batch 1 loss = 7.843483\n","epoch 33 batch 2 loss = 7.7967567\n","epoch 34 batch 0 loss = 7.812053\n","epoch 34 batch 1 loss = 7.560561\n","epoch 34 batch 2 loss = 7.5605254\n","epoch 35 batch 0 loss = 7.604216\n","epoch 35 batch 1 loss = 7.712374\n","epoch 35 batch 2 loss = 7.9306498\n","epoch 35 batch 3 loss = 7.707278\n","epoch 36 batch 0 loss = 7.6436205\n","epoch 36 batch 1 loss = 7.6824026\n","epoch 37 batch 0 loss = 7.931048\n","epoch 37 batch 1 loss = 7.8495574\n","epoch 37 batch 2 loss = 7.775446\n","epoch 38 batch 0 loss = 7.761425\n","epoch 38 batch 1 loss = 7.810324\n","epoch 38 batch 2 loss = 7.5730267\n","epoch 39 batch 0 loss = 7.8359013\n","epoch 39 batch 1 loss = 7.7752895\n","epoch 39 batch 2 loss = 7.85184\n","epoch 39 batch 3 loss = 7.5529995\n","epoch 40 batch 0 loss = 7.5741186\n","epoch 40 batch 1 loss = 7.667472\n","epoch 41 batch 0 loss = 7.756527\n","epoch 41 batch 1 loss = 8.017824\n","epoch 41 batch 2 loss = 7.6507616\n","epoch 42 batch 0 loss = 7.6908693\n","epoch 42 batch 1 loss = 7.645293\n","epoch 42 batch 2 loss = 7.713356\n","epoch 43 batch 0 loss = 7.721835\n","epoch 43 batch 1 loss = 7.843126\n","epoch 43 batch 2 loss = 7.4595213\n","epoch 44 batch 0 loss = 7.912704\n","epoch 44 batch 1 loss = 7.7919908\n","epoch 44 batch 2 loss = 7.62446\n","epoch 44 batch 3 loss = 7.8557568\n","epoch 45 batch 0 loss = 7.5775933\n","epoch 45 batch 1 loss = 7.641188\n","epoch 45 batch 2 loss = 7.7129216\n","epoch 46 batch 0 loss = 7.5948453\n","epoch 46 batch 1 loss = 7.71281\n","epoch 46 batch 2 loss = 7.7626724\n","epoch 47 batch 0 loss = 7.5636764\n","epoch 47 batch 1 loss = 7.825823\n","epoch 48 batch 0 loss = 7.634426\n","epoch 48 batch 1 loss = 7.918011\n","epoch 48 batch 2 loss = 7.9364924\n","epoch 49 batch 0 loss = 7.7779226\n","epoch 49 batch 1 loss = 7.694339\n","epoch 49 batch 2 loss = 7.645491\n","epoch 49 batch 3 loss = 7.7387114\n","epoch 49 batch 4 loss = 7.891317\n","epoch 0 batch 0 loss = 45.78324\n","epoch 1 batch 0 loss = 43.921787\n","epoch 2 batch 0 loss = 42.03383\n","epoch 3 batch 0 loss = 40.108833\n","epoch 4 batch 0 loss = 38.215088\n","epoch 5 batch 0 loss = 36.300995\n","epoch 6 batch 0 loss = 34.3129\n","epoch 7 batch 0 loss = 32.31599\n","epoch 8 batch 0 loss = 30.326916\n","epoch 9 batch 0 loss = 28.385105\n","epoch 10 batch 0 loss = 26.529472\n","epoch 11 batch 0 loss = 24.771353\n","epoch 12 batch 0 loss = 23.192938\n","epoch 13 batch 0 loss = 21.881205\n","epoch 14 batch 0 loss = 20.927252\n","epoch 15 batch 0 loss = 20.111944\n","epoch 16 batch 0 loss = 19.59221\n","epoch 17 batch 0 loss = 19.402218\n","epoch 18 batch 0 loss = 19.23774\n","epoch 19 batch 0 loss = 18.697765\n","epoch 20 batch 0 loss = 18.541143\n","epoch 21 batch 0 loss = 18.496733\n","epoch 22 batch 0 loss = 18.491146\n","epoch 23 batch 0 loss = 18.489962\n","epoch 24 batch 0 loss = 18.479942\n","epoch 25 batch 0 loss = 18.48103\n","epoch 26 batch 0 loss = 18.477306\n","epoch 27 batch 0 loss = 18.473442\n","epoch 28 batch 0 loss = 18.480461\n","epoch 29 batch 0 loss = 18.463095\n","epoch 30 batch 0 loss = 18.477081\n","epoch 31 batch 0 loss = 18.479216\n","epoch 32 batch 0 loss = 18.473108\n","epoch 33 batch 0 loss = 18.469017\n","epoch 34 batch 0 loss = 18.454704\n","epoch 35 batch 0 loss = 18.47232\n","epoch 36 batch 0 loss = 18.456009\n","epoch 37 batch 0 loss = 18.469093\n","epoch 38 batch 0 loss = 18.445494\n","epoch 39 batch 0 loss = 18.452501\n","epoch 40 batch 0 loss = 18.457455\n","epoch 41 batch 0 loss = 18.460417\n","epoch 42 batch 0 loss = 18.449009\n","epoch 43 batch 0 loss = 18.461523\n","epoch 44 batch 0 loss = 18.454714\n","epoch 45 batch 0 loss = 18.469471\n","epoch 46 batch 0 loss = 18.460901\n","epoch 47 batch 0 loss = 18.447392\n","epoch 48 batch 0 loss = 18.461433\n","epoch 49 batch 0 loss = 18.463928\n","epoch 0 batch 0 loss = 6.6253285\n","epoch 0 batch 1 loss = 6.780608\n","epoch 0 batch 2 loss = 6.2716923\n","epoch 0 batch 3 loss = 6.2520285\n","epoch 0 batch 4 loss = 5.8839498\n","epoch 0 batch 5 loss = 6.013573\n","epoch 0 batch 6 loss = 5.551212\n","epoch 1 batch 0 loss = 5.945653\n","epoch 1 batch 1 loss = 5.71274\n","epoch 1 batch 2 loss = 5.326088\n","epoch 1 batch 3 loss = 5.521778\n","epoch 1 batch 4 loss = 5.2276936\n","epoch 1 batch 5 loss = 5.3760104\n","epoch 1 batch 6 loss = 5.4802475\n","epoch 2 batch 0 loss = 5.1597276\n","epoch 2 batch 1 loss = 4.922675\n","epoch 2 batch 2 loss = 4.819462\n","epoch 2 batch 3 loss = 5.0484433\n","epoch 2 batch 4 loss = 5.0496273\n","epoch 2 batch 5 loss = 4.8196807\n","epoch 2 batch 6 loss = 4.6464477\n","epoch 3 batch 0 loss = 4.7586217\n","epoch 3 batch 1 loss = 4.453518\n","epoch 3 batch 2 loss = 4.5427375\n","epoch 3 batch 3 loss = 4.4821987\n","epoch 3 batch 4 loss = 4.610922\n","epoch 3 batch 5 loss = 4.4785924\n","epoch 3 batch 6 loss = 3.9402413\n","epoch 3 batch 7 loss = 4.261346\n","epoch 4 batch 0 loss = 4.384071\n","epoch 4 batch 1 loss = 3.9385188\n","epoch 4 batch 2 loss = 4.2779217\n","epoch 4 batch 3 loss = 4.0233088\n","epoch 4 batch 4 loss = 3.833151\n","epoch 4 batch 5 loss = 3.8242164\n","epoch 4 batch 6 loss = 3.8433893\n","epoch 4 batch 7 loss = 4.1046014\n","epoch 5 batch 0 loss = 3.8046873\n","epoch 5 batch 1 loss = 3.860138\n","epoch 5 batch 2 loss = 3.864291\n","epoch 5 batch 3 loss = 3.6435456\n","epoch 5 batch 4 loss = 3.834747\n","epoch 5 batch 5 loss = 3.7291098\n","epoch 5 batch 6 loss = 3.738487\n","epoch 6 batch 0 loss = 3.50213\n","epoch 6 batch 1 loss = 3.6750007\n","epoch 6 batch 2 loss = 3.7583811\n","epoch 6 batch 3 loss = 3.5859396\n","epoch 6 batch 4 loss = 3.6315305\n","epoch 6 batch 5 loss = 3.6311889\n","epoch 6 batch 6 loss = 3.5350554\n","epoch 6 batch 7 loss = 3.5277913\n","epoch 7 batch 0 loss = 3.6924953\n","epoch 7 batch 1 loss = 3.6281412\n","epoch 7 batch 2 loss = 3.102304\n","epoch 7 batch 3 loss = 3.563478\n","epoch 7 batch 4 loss = 3.2010226\n","epoch 7 batch 5 loss = 3.3306012\n","epoch 7 batch 6 loss = 3.4150264\n","epoch 7 batch 7 loss = 3.2383366\n","epoch 8 batch 0 loss = 3.4132261\n","epoch 8 batch 1 loss = 3.5014489\n","epoch 8 batch 2 loss = 3.2578235\n","epoch 8 batch 3 loss = 3.3749428\n","epoch 8 batch 4 loss = 3.3895235\n","epoch 8 batch 5 loss = 3.6078188\n","epoch 8 batch 6 loss = 3.4524155\n","epoch 8 batch 7 loss = 3.7217093\n","epoch 8 batch 8 loss = 3.416935\n","epoch 8 batch 9 loss = 3.4724019\n","epoch 9 batch 0 loss = 3.3352046\n","epoch 9 batch 1 loss = 3.3875406\n","epoch 9 batch 2 loss = 3.4057477\n","epoch 9 batch 3 loss = 3.5009878\n","epoch 9 batch 4 loss = 3.7559142\n","epoch 9 batch 5 loss = 3.4622602\n","epoch 9 batch 6 loss = 3.0720613\n","epoch 9 batch 7 loss = 3.3427253\n","epoch 9 batch 8 loss = 3.4443326\n","epoch 10 batch 0 loss = 3.3249035\n","epoch 10 batch 1 loss = 3.4503217\n","epoch 10 batch 2 loss = 3.5472527\n","epoch 10 batch 3 loss = 3.4754395\n","epoch 10 batch 4 loss = 3.2626805\n","epoch 10 batch 5 loss = 3.5976264\n","epoch 10 batch 6 loss = 3.703556\n","epoch 10 batch 7 loss = 3.340556\n","epoch 11 batch 0 loss = 3.5225048\n","epoch 11 batch 1 loss = 3.4065435\n","epoch 11 batch 2 loss = 3.110408\n","epoch 11 batch 3 loss = 3.3722885\n","epoch 11 batch 4 loss = 3.2596135\n","epoch 11 batch 5 loss = 3.519488\n","epoch 11 batch 6 loss = 3.4152436\n","epoch 12 batch 0 loss = 3.459294\n","epoch 12 batch 1 loss = 3.3937683\n","epoch 12 batch 2 loss = 3.4827151\n","epoch 12 batch 3 loss = 3.4039762\n","epoch 12 batch 4 loss = 3.3556476\n","epoch 12 batch 5 loss = 3.3843198\n","epoch 12 batch 6 loss = 3.5346327\n","epoch 13 batch 0 loss = 3.5167255\n","epoch 13 batch 1 loss = 3.2530596\n","epoch 13 batch 2 loss = 3.6502488\n","epoch 13 batch 3 loss = 3.2924235\n","epoch 13 batch 4 loss = 3.3581092\n","epoch 13 batch 5 loss = 3.4019299\n","epoch 13 batch 6 loss = 3.4876306\n","epoch 14 batch 0 loss = 3.1138754\n","epoch 14 batch 1 loss = 3.3614826\n","epoch 14 batch 2 loss = 3.414976\n","epoch 14 batch 3 loss = 3.289265\n","epoch 14 batch 4 loss = 3.3544004\n","epoch 14 batch 5 loss = 3.456058\n","epoch 14 batch 6 loss = 3.2986407\n","epoch 15 batch 0 loss = 3.4682577\n","epoch 15 batch 1 loss = 3.2802143\n","epoch 15 batch 2 loss = 3.5074599\n","epoch 15 batch 3 loss = 3.2565722\n","epoch 15 batch 4 loss = 3.3179493\n","epoch 15 batch 5 loss = 3.445316\n","epoch 15 batch 6 loss = 3.2822266\n","epoch 16 batch 0 loss = 3.309154\n","epoch 16 batch 1 loss = 3.528058\n","epoch 16 batch 2 loss = 3.4052088\n","epoch 16 batch 3 loss = 3.3056455\n","epoch 16 batch 4 loss = 3.3755352\n","epoch 16 batch 5 loss = 3.173872\n","epoch 16 batch 6 loss = 3.4350684\n","epoch 16 batch 7 loss = 3.2425888\n","epoch 16 batch 8 loss = 3.2214289\n","epoch 17 batch 0 loss = 3.1257586\n","epoch 17 batch 1 loss = 3.5085456\n","epoch 17 batch 2 loss = 3.2438715\n","epoch 17 batch 3 loss = 3.488833\n","epoch 17 batch 4 loss = 3.3698206\n","epoch 17 batch 5 loss = 3.5067916\n","epoch 17 batch 6 loss = 3.4331932\n","epoch 17 batch 7 loss = 3.362558\n","epoch 17 batch 8 loss = 3.6158946\n","epoch 18 batch 0 loss = 3.498313\n","epoch 18 batch 1 loss = 3.3486469\n","epoch 18 batch 2 loss = 3.6331306\n","epoch 18 batch 3 loss = 3.330704\n","epoch 18 batch 4 loss = 3.3614016\n","epoch 18 batch 5 loss = 3.3783927\n","epoch 18 batch 6 loss = 3.4386373\n","epoch 19 batch 0 loss = 3.2815685\n","epoch 19 batch 1 loss = 3.4022942\n","epoch 19 batch 2 loss = 3.3762684\n","epoch 19 batch 3 loss = 3.498988\n","epoch 19 batch 4 loss = 3.5377111\n","epoch 19 batch 5 loss = 3.5875165\n","epoch 19 batch 6 loss = 3.6017396\n","epoch 20 batch 0 loss = 3.144194\n","epoch 20 batch 1 loss = 3.135041\n","epoch 20 batch 2 loss = 3.2959578\n","epoch 20 batch 3 loss = 3.3971593\n","epoch 20 batch 4 loss = 3.6887717\n","epoch 20 batch 5 loss = 3.3261507\n","epoch 20 batch 6 loss = 3.540534\n","epoch 20 batch 7 loss = 3.5256143\n","epoch 20 batch 8 loss = 3.2893884\n","epoch 20 batch 9 loss = 3.4092138\n","epoch 20 batch 10 loss = 3.4933705\n","epoch 21 batch 0 loss = 3.5544517\n","epoch 21 batch 1 loss = 3.2854528\n","epoch 21 batch 2 loss = 3.4854536\n","epoch 21 batch 3 loss = 3.2606473\n","epoch 21 batch 4 loss = 3.3697884\n","epoch 21 batch 5 loss = 3.2859347\n","epoch 22 batch 0 loss = 3.6980903\n","epoch 22 batch 1 loss = 3.5046453\n","epoch 22 batch 2 loss = 3.4670606\n","epoch 22 batch 3 loss = 3.4886925\n","epoch 22 batch 4 loss = 3.4621668\n","epoch 22 batch 5 loss = 3.1860936\n","epoch 22 batch 6 loss = 3.1343987\n","epoch 23 batch 0 loss = 3.4864519\n","epoch 23 batch 1 loss = 3.6568537\n","epoch 23 batch 2 loss = 3.1968915\n","epoch 23 batch 3 loss = 3.157912\n","epoch 23 batch 4 loss = 3.4011827\n","epoch 23 batch 5 loss = 3.4928095\n","epoch 23 batch 6 loss = 3.5342944\n","epoch 24 batch 0 loss = 3.5426805\n","epoch 24 batch 1 loss = 3.5381186\n","epoch 24 batch 2 loss = 3.5457563\n","epoch 24 batch 3 loss = 3.444584\n","epoch 24 batch 4 loss = 3.6651433\n","epoch 24 batch 5 loss = 3.1418562\n","epoch 24 batch 6 loss = 3.3365839\n","epoch 25 batch 0 loss = 3.1079316\n","epoch 25 batch 1 loss = 3.255623\n","epoch 25 batch 2 loss = 3.3324952\n","epoch 25 batch 3 loss = 3.4821196\n","epoch 25 batch 4 loss = 3.4797354\n","epoch 25 batch 5 loss = 3.2645402\n","epoch 26 batch 0 loss = 3.3242996\n","epoch 26 batch 1 loss = 3.5246396\n","epoch 26 batch 2 loss = 3.389021\n","epoch 26 batch 3 loss = 3.3149345\n","epoch 26 batch 4 loss = 3.439398\n","epoch 26 batch 5 loss = 3.5349448\n","epoch 26 batch 6 loss = 3.3263338\n","epoch 27 batch 0 loss = 3.2865434\n","epoch 27 batch 1 loss = 3.2149115\n","epoch 27 batch 2 loss = 3.266883\n","epoch 27 batch 3 loss = 3.2848804\n","epoch 27 batch 4 loss = 3.507475\n","epoch 27 batch 5 loss = 3.3147097\n","epoch 27 batch 6 loss = 3.5410252\n","epoch 27 batch 7 loss = 3.2210555\n","epoch 28 batch 0 loss = 3.4434054\n","epoch 28 batch 1 loss = 3.4655924\n","epoch 28 batch 2 loss = 3.398982\n","epoch 28 batch 3 loss = 3.447508\n","epoch 28 batch 4 loss = 3.5330515\n","epoch 28 batch 5 loss = 3.3465965\n","epoch 28 batch 6 loss = 3.6076913\n","epoch 29 batch 0 loss = 3.5442648\n","epoch 29 batch 1 loss = 3.4641478\n","epoch 29 batch 2 loss = 3.3761053\n","epoch 29 batch 3 loss = 3.532229\n","epoch 29 batch 4 loss = 3.4642158\n","epoch 29 batch 5 loss = 3.450846\n","epoch 30 batch 0 loss = 3.208612\n","epoch 30 batch 1 loss = 3.432374\n","epoch 30 batch 2 loss = 3.5615947\n","epoch 30 batch 3 loss = 3.4898267\n","epoch 30 batch 4 loss = 3.5947769\n","epoch 30 batch 5 loss = 3.385982\n","epoch 30 batch 6 loss = 3.4560962\n","epoch 30 batch 7 loss = 3.3425622\n","epoch 30 batch 8 loss = 3.342368\n","epoch 30 batch 9 loss = 3.2132354\n","epoch 31 batch 0 loss = 3.04397\n","epoch 31 batch 1 loss = 3.1644804\n","epoch 31 batch 2 loss = 3.384878\n","epoch 31 batch 3 loss = 3.5442429\n","epoch 31 batch 4 loss = 3.2947161\n","epoch 31 batch 5 loss = 3.141287\n","epoch 31 batch 6 loss = 3.4651315\n","epoch 31 batch 7 loss = 3.595442\n","epoch 31 batch 8 loss = 3.4355986\n","epoch 32 batch 0 loss = 3.4747748\n","epoch 32 batch 1 loss = 3.2129645\n","epoch 32 batch 2 loss = 3.4340773\n","epoch 32 batch 3 loss = 3.158746\n","epoch 32 batch 4 loss = 3.568866\n","epoch 32 batch 5 loss = 3.397085\n","epoch 32 batch 6 loss = 3.3568301\n","epoch 32 batch 7 loss = 3.3106444\n","epoch 33 batch 0 loss = 3.2392428\n","epoch 33 batch 1 loss = 3.215181\n","epoch 33 batch 2 loss = 3.589374\n","epoch 33 batch 3 loss = 3.1624696\n","epoch 33 batch 4 loss = 3.0594509\n","epoch 33 batch 5 loss = 3.3380873\n","epoch 33 batch 6 loss = 3.6072633\n","epoch 33 batch 7 loss = 3.4869802\n","epoch 34 batch 0 loss = 3.1874144\n","epoch 34 batch 1 loss = 3.2077148\n","epoch 34 batch 2 loss = 3.0538468\n","epoch 34 batch 3 loss = 3.5052612\n","epoch 34 batch 4 loss = 3.2443743\n","epoch 34 batch 5 loss = 3.7096014\n","epoch 34 batch 6 loss = 3.601147\n","epoch 34 batch 7 loss = 3.2845645\n","epoch 34 batch 8 loss = 3.491086\n","epoch 35 batch 0 loss = 3.4975753\n","epoch 35 batch 1 loss = 3.3603516\n","epoch 35 batch 2 loss = 3.3172314\n","epoch 35 batch 3 loss = 3.2391906\n","epoch 35 batch 4 loss = 3.4294305\n","epoch 35 batch 5 loss = 3.3296785\n","epoch 35 batch 6 loss = 3.3522751\n","epoch 36 batch 0 loss = 3.4627655\n","epoch 36 batch 1 loss = 3.3348963\n","epoch 36 batch 2 loss = 3.3140934\n","epoch 36 batch 3 loss = 3.4163227\n","epoch 36 batch 4 loss = 3.7565873\n","epoch 36 batch 5 loss = 3.2418423\n","epoch 36 batch 6 loss = 3.3450296\n","epoch 36 batch 7 loss = 3.5249395\n","epoch 37 batch 0 loss = 3.53909\n","epoch 37 batch 1 loss = 3.3302755\n","epoch 37 batch 2 loss = 3.2030504\n","epoch 37 batch 3 loss = 3.4256053\n","epoch 37 batch 4 loss = 3.314093\n","epoch 38 batch 0 loss = 3.3675292\n","epoch 38 batch 1 loss = 3.5472486\n","epoch 38 batch 2 loss = 3.5594225\n","epoch 38 batch 3 loss = 3.3051536\n","epoch 38 batch 4 loss = 3.5862916\n","epoch 38 batch 5 loss = 3.4769626\n","epoch 38 batch 6 loss = 3.4036632\n","epoch 39 batch 0 loss = 3.4597623\n","epoch 39 batch 1 loss = 3.425473\n","epoch 39 batch 2 loss = 3.4917185\n","epoch 39 batch 3 loss = 3.3422174\n","epoch 39 batch 4 loss = 3.0907528\n","epoch 39 batch 5 loss = 3.2157009\n","epoch 39 batch 6 loss = 3.4860935\n","epoch 39 batch 7 loss = 3.675778\n","epoch 40 batch 0 loss = 3.442886\n","epoch 40 batch 1 loss = 3.4435592\n","epoch 40 batch 2 loss = 3.2077627\n","epoch 40 batch 3 loss = 3.319234\n","epoch 40 batch 4 loss = 3.3485537\n","epoch 41 batch 0 loss = 3.5158927\n","epoch 41 batch 1 loss = 3.12586\n","epoch 41 batch 2 loss = 3.7497783\n","epoch 41 batch 3 loss = 3.510091\n","epoch 41 batch 4 loss = 3.1747394\n","epoch 41 batch 5 loss = 3.3686678\n","epoch 41 batch 6 loss = 3.0258105\n","epoch 41 batch 7 loss = 3.6524115\n","epoch 42 batch 0 loss = 3.1874523\n","epoch 42 batch 1 loss = 3.5026362\n","epoch 42 batch 2 loss = 3.0809126\n","epoch 42 batch 3 loss = 3.5125935\n","epoch 42 batch 4 loss = 3.6175\n","epoch 42 batch 5 loss = 3.477487\n","epoch 42 batch 6 loss = 3.3326743\n","epoch 42 batch 7 loss = 3.3237922\n","epoch 43 batch 0 loss = 3.087823\n","epoch 43 batch 1 loss = 3.2883327\n","epoch 43 batch 2 loss = 3.5989325\n","epoch 43 batch 3 loss = 3.393597\n","epoch 43 batch 4 loss = 3.5219412\n","epoch 43 batch 5 loss = 3.1501677\n","epoch 43 batch 6 loss = 3.5136862\n","epoch 43 batch 7 loss = 3.245992\n","epoch 43 batch 8 loss = 3.1971126\n","epoch 44 batch 0 loss = 3.0100756\n","epoch 44 batch 1 loss = 3.3683329\n","epoch 44 batch 2 loss = 3.530918\n","epoch 44 batch 3 loss = 3.2754095\n","epoch 44 batch 4 loss = 3.1575234\n","epoch 44 batch 5 loss = 3.42261\n","epoch 44 batch 6 loss = 3.219221\n","epoch 44 batch 7 loss = 3.3249307\n","epoch 44 batch 8 loss = 3.3568203\n","epoch 45 batch 0 loss = 3.608476\n","epoch 45 batch 1 loss = 3.3134356\n","epoch 45 batch 2 loss = 2.9410548\n","epoch 45 batch 3 loss = 3.2990556\n","epoch 45 batch 4 loss = 3.3967745\n","epoch 45 batch 5 loss = 3.22947\n","epoch 45 batch 6 loss = 3.3186076\n","epoch 45 batch 7 loss = 3.3281903\n","epoch 46 batch 0 loss = 3.2045197\n","epoch 46 batch 1 loss = 3.261774\n","epoch 46 batch 2 loss = 3.1234589\n","epoch 46 batch 3 loss = 3.3700418\n","epoch 46 batch 4 loss = 3.4002144\n","epoch 46 batch 5 loss = 3.5848675\n","epoch 46 batch 6 loss = 3.3506954\n","epoch 47 batch 0 loss = 3.227729\n","epoch 47 batch 1 loss = 3.356777\n","epoch 47 batch 2 loss = 3.231008\n","epoch 47 batch 3 loss = 3.2493942\n","epoch 47 batch 4 loss = 3.2485657\n","epoch 47 batch 5 loss = 3.3450744\n","epoch 47 batch 6 loss = 3.3766923\n","epoch 48 batch 0 loss = 3.187981\n","epoch 48 batch 1 loss = 3.3494098\n","epoch 48 batch 2 loss = 3.5301998\n","epoch 48 batch 3 loss = 3.2309241\n","epoch 48 batch 4 loss = 3.2725797\n","epoch 49 batch 0 loss = 3.592957\n","epoch 49 batch 1 loss = 3.6456704\n","epoch 49 batch 2 loss = 3.3212059\n","epoch 49 batch 3 loss = 2.9733407\n","epoch 49 batch 4 loss = 3.3155918\n","epoch 49 batch 5 loss = 3.39405\n","epoch 49 batch 6 loss = 3.5029528\n","epoch 49 batch 7 loss = 3.4270542\n","epoch 0 batch 0 loss = 357.4639\n","epoch 1 batch 0 loss = 333.24188\n","epoch 2 batch 0 loss = 308.11996\n","epoch 3 batch 0 loss = 280.20837\n","epoch 4 batch 0 loss = 251.09723\n","epoch 5 batch 0 loss = 222.58548\n","epoch 6 batch 0 loss = 199.27484\n","epoch 7 batch 0 loss = 187.40668\n","epoch 8 batch 0 loss = 186.73865\n","epoch 9 batch 0 loss = 189.95415\n","epoch 10 batch 0 loss = 187.80296\n","epoch 11 batch 0 loss = 189.83002\n","epoch 12 batch 0 loss = 187.75279\n","epoch 13 batch 0 loss = 189.9297\n","epoch 14 batch 0 loss = 187.59584\n","epoch 15 batch 0 loss = 189.89073\n","epoch 16 batch 0 loss = 187.48119\n","epoch 17 batch 0 loss = 189.97217\n","epoch 18 batch 0 loss = 187.39551\n","epoch 19 batch 0 loss = 189.97752\n","epoch 20 batch 0 loss = 187.34866\n","epoch 21 batch 0 loss = 190.04672\n","epoch 22 batch 0 loss = 187.17336\n","epoch 23 batch 0 loss = 190.12871\n","epoch 24 batch 0 loss = 187.14542\n","epoch 25 batch 0 loss = 190.17416\n","epoch 26 batch 0 loss = 187.05565\n","epoch 27 batch 0 loss = 190.17603\n","epoch 28 batch 0 loss = 186.9669\n","epoch 29 batch 0 loss = 190.29303\n","epoch 30 batch 0 loss = 186.94508\n","epoch 31 batch 0 loss = 190.26212\n","epoch 32 batch 0 loss = 186.83119\n","epoch 33 batch 0 loss = 190.33841\n","epoch 34 batch 0 loss = 186.74243\n","epoch 35 batch 0 loss = 190.35562\n","epoch 36 batch 0 loss = 186.72134\n","epoch 37 batch 0 loss = 190.47609\n","epoch 38 batch 0 loss = 186.61794\n","epoch 39 batch 0 loss = 190.47957\n","epoch 40 batch 0 loss = 186.57213\n","epoch 41 batch 0 loss = 190.49298\n","epoch 42 batch 0 loss = 186.48189\n","epoch 43 batch 0 loss = 190.53795\n","epoch 44 batch 0 loss = 186.44495\n","epoch 45 batch 0 loss = 190.5866\n","epoch 46 batch 0 loss = 186.36838\n","epoch 47 batch 0 loss = 190.62582\n","epoch 48 batch 0 loss = 186.30923\n","epoch 49 batch 0 loss = 190.64159\n","epoch 0 batch 0 loss = 8.301788\n","epoch 0 batch 1 loss = 8.026925\n","epoch 0 batch 2 loss = 8.130913\n","epoch 1 batch 0 loss = 8.239327\n","epoch 1 batch 1 loss = 7.9904585\n","epoch 1 batch 2 loss = 8.004065\n","epoch 2 batch 0 loss = 7.9020805\n","epoch 2 batch 1 loss = 8.036201\n","epoch 2 batch 2 loss = 7.795716\n","epoch 3 batch 0 loss = 7.5668035\n","epoch 3 batch 1 loss = 7.6611643\n","epoch 3 batch 2 loss = 7.603633\n","epoch 4 batch 0 loss = 7.4945564\n","epoch 4 batch 1 loss = 7.1019926\n","epoch 4 batch 2 loss = 7.3431177\n","epoch 5 batch 0 loss = 7.2846823\n","epoch 5 batch 1 loss = 6.8694105\n","epoch 5 batch 2 loss = 6.8292174\n","epoch 6 batch 0 loss = 7.0931597\n","epoch 6 batch 1 loss = 6.769554\n","epoch 6 batch 2 loss = 6.9298897\n","epoch 7 batch 0 loss = 6.463627\n","epoch 7 batch 1 loss = 6.4864664\n","epoch 7 batch 2 loss = 6.14029\n","epoch 7 batch 3 loss = 6.445086\n","epoch 8 batch 0 loss = 6.5010366\n","epoch 8 batch 1 loss = 6.136897\n","epoch 8 batch 2 loss = 6.1005025\n","epoch 8 batch 3 loss = 5.974102\n","epoch 9 batch 0 loss = 5.851711\n","epoch 9 batch 1 loss = 5.9418054\n","epoch 9 batch 2 loss = 5.9847455\n","epoch 10 batch 0 loss = 5.6740885\n","epoch 10 batch 1 loss = 5.8006716\n","epoch 10 batch 2 loss = 5.7154455\n","epoch 10 batch 3 loss = 5.5988016\n","epoch 11 batch 0 loss = 5.320767\n","epoch 11 batch 1 loss = 5.611265\n","epoch 11 batch 2 loss = 5.7123466\n","epoch 11 batch 3 loss = 5.5132027\n","epoch 12 batch 0 loss = 5.4972034\n","epoch 12 batch 1 loss = 5.287415\n","epoch 12 batch 2 loss = 5.6037555\n","epoch 12 batch 3 loss = 5.1525335\n","epoch 13 batch 0 loss = 5.1909156\n","epoch 13 batch 1 loss = 5.352677\n","epoch 13 batch 2 loss = 5.0155134\n","epoch 14 batch 0 loss = 5.2751136\n","epoch 14 batch 1 loss = 4.886062\n","epoch 15 batch 0 loss = 4.9513726\n","epoch 15 batch 1 loss = 5.173649\n","epoch 15 batch 2 loss = 4.9341497\n","epoch 15 batch 3 loss = 4.9745584\n","epoch 16 batch 0 loss = 5.0516872\n","epoch 16 batch 1 loss = 4.695144\n","epoch 16 batch 2 loss = 4.8533506\n","epoch 17 batch 0 loss = 4.7590656\n","epoch 17 batch 1 loss = 4.6262064\n","epoch 17 batch 2 loss = 4.83567\n","epoch 18 batch 0 loss = 4.6564093\n","epoch 18 batch 1 loss = 4.5941143\n","epoch 18 batch 2 loss = 4.4893446\n","epoch 19 batch 0 loss = 4.5067754\n","epoch 19 batch 1 loss = 4.521731\n","epoch 19 batch 2 loss = 4.4491277\n","epoch 20 batch 0 loss = 4.411549\n","epoch 20 batch 1 loss = 4.5490785\n","epoch 20 batch 2 loss = 4.221532\n","epoch 20 batch 3 loss = 4.4506307\n","epoch 21 batch 0 loss = 4.3672366\n","epoch 21 batch 1 loss = 4.255461\n","epoch 21 batch 2 loss = 4.358358\n","epoch 21 batch 3 loss = 4.1258063\n","epoch 22 batch 0 loss = 4.3120728\n","epoch 22 batch 1 loss = 4.079558\n","epoch 22 batch 2 loss = 4.2085934\n","epoch 23 batch 0 loss = 4.157478\n","epoch 23 batch 1 loss = 4.142002\n","epoch 23 batch 2 loss = 4.065388\n","epoch 24 batch 0 loss = 3.9948492\n","epoch 24 batch 1 loss = 4.0643063\n","epoch 24 batch 2 loss = 4.152273\n","epoch 25 batch 0 loss = 3.9043868\n","epoch 25 batch 1 loss = 3.9464722\n","epoch 25 batch 2 loss = 3.96813\n","epoch 26 batch 0 loss = 3.963349\n","epoch 26 batch 1 loss = 4.0115156\n","epoch 26 batch 2 loss = 4.0966372\n","epoch 27 batch 0 loss = 4.013936\n","epoch 27 batch 1 loss = 3.871777\n","epoch 27 batch 2 loss = 3.8398585\n","epoch 28 batch 0 loss = 3.7979362\n","epoch 28 batch 1 loss = 3.9695323\n","epoch 28 batch 2 loss = 3.9285636\n","epoch 29 batch 0 loss = 3.855545\n","epoch 29 batch 1 loss = 3.9281807\n","epoch 29 batch 2 loss = 3.872792\n","epoch 29 batch 3 loss = 3.810271\n","epoch 30 batch 0 loss = 3.8916376\n","epoch 30 batch 1 loss = 3.8748581\n","epoch 30 batch 2 loss = 3.7469778\n","epoch 30 batch 3 loss = 3.7781892\n","epoch 30 batch 4 loss = 3.8363724\n","epoch 31 batch 0 loss = 3.738772\n","epoch 31 batch 1 loss = 3.630964\n","epoch 31 batch 2 loss = 3.8235433\n","epoch 31 batch 3 loss = 3.7961388\n","epoch 32 batch 0 loss = 3.7370594\n","epoch 32 batch 1 loss = 3.7480445\n","epoch 32 batch 2 loss = 3.704494\n","epoch 32 batch 3 loss = 3.7445323\n","epoch 33 batch 0 loss = 3.7839954\n","epoch 33 batch 1 loss = 3.7932942\n","epoch 33 batch 2 loss = 3.711922\n","epoch 34 batch 0 loss = 3.680393\n","epoch 34 batch 1 loss = 3.7571301\n","epoch 34 batch 2 loss = 3.6472948\n","epoch 35 batch 0 loss = 3.668988\n","epoch 35 batch 1 loss = 3.8120346\n","epoch 35 batch 2 loss = 3.5952609\n","epoch 36 batch 0 loss = 3.7905781\n","epoch 36 batch 1 loss = 3.6487434\n","epoch 36 batch 2 loss = 3.6479874\n","epoch 37 batch 0 loss = 3.5992885\n","epoch 37 batch 1 loss = 3.6262584\n","epoch 37 batch 2 loss = 3.5908608\n","epoch 38 batch 0 loss = 3.6650136\n","epoch 38 batch 1 loss = 3.6802151\n","epoch 38 batch 2 loss = 3.7697067\n","epoch 39 batch 0 loss = 3.6259944\n","epoch 39 batch 1 loss = 3.7286673\n","epoch 39 batch 2 loss = 3.5564377\n","epoch 39 batch 3 loss = 3.706806\n","epoch 40 batch 0 loss = 3.7039902\n","epoch 40 batch 1 loss = 3.5986636\n","epoch 40 batch 2 loss = 3.6737401\n","epoch 41 batch 0 loss = 3.5065703\n","epoch 41 batch 1 loss = 3.7661767\n","epoch 41 batch 2 loss = 3.6651757\n","epoch 42 batch 0 loss = 3.6713994\n","epoch 42 batch 1 loss = 3.6021194\n","epoch 42 batch 2 loss = 3.561114\n","epoch 43 batch 0 loss = 3.6182122\n","epoch 43 batch 1 loss = 3.6134355\n","epoch 43 batch 2 loss = 3.7272613\n","epoch 44 batch 0 loss = 3.5750842\n","epoch 44 batch 1 loss = 3.5888894\n","epoch 44 batch 2 loss = 3.621649\n","epoch 44 batch 3 loss = 3.7634234\n","epoch 44 batch 4 loss = 3.5342581\n","epoch 45 batch 0 loss = 3.48903\n","epoch 45 batch 1 loss = 3.6221237\n","epoch 45 batch 2 loss = 3.6547296\n","epoch 46 batch 0 loss = 3.7380738\n","epoch 46 batch 1 loss = 3.593639\n","epoch 46 batch 2 loss = 3.6579506\n","epoch 47 batch 0 loss = 3.714392\n","epoch 47 batch 1 loss = 3.6964657\n","epoch 47 batch 2 loss = 3.7026434\n","epoch 47 batch 3 loss = 3.7022307\n","epoch 48 batch 0 loss = 3.6019967\n","epoch 48 batch 1 loss = 3.654728\n","epoch 48 batch 2 loss = 3.8660965\n","epoch 49 batch 0 loss = 3.5784009\n","epoch 49 batch 1 loss = 3.667429\n","epoch 49 batch 2 loss = 3.6500945\n","epoch 49 batch 3 loss = 3.714363\n","epoch 49 batch 4 loss = 3.6910524\n","epoch 0 batch 0 loss = 95.61089\n","epoch 1 batch 0 loss = 91.52834\n","epoch 2 batch 0 loss = 87.688385\n","epoch 2 batch 1 loss = 83.71305\n","epoch 3 batch 0 loss = 79.9323\n","epoch 4 batch 0 loss = 76.286064\n","epoch 5 batch 0 loss = 72.799774\n","epoch 6 batch 0 loss = 69.43158\n","epoch 7 batch 0 loss = 66.234116\n","epoch 8 batch 0 loss = 63.198116\n","epoch 9 batch 0 loss = 60.358746\n","epoch 10 batch 0 loss = 57.803734\n","epoch 11 batch 0 loss = 55.54479\n","epoch 12 batch 0 loss = 53.575912\n","epoch 13 batch 0 loss = 51.996582\n","epoch 14 batch 0 loss = 50.78111\n","epoch 15 batch 0 loss = 49.61219\n","epoch 16 batch 0 loss = 48.718838\n","epoch 17 batch 0 loss = 48.28127\n","epoch 18 batch 0 loss = 47.327667\n","epoch 19 batch 0 loss = 47.337666\n","epoch 19 batch 1 loss = 46.506344\n","epoch 20 batch 0 loss = 46.500454\n","epoch 21 batch 0 loss = 45.57032\n","epoch 21 batch 1 loss = 46.501892\n","epoch 22 batch 0 loss = 46.51309\n","epoch 23 batch 0 loss = 46.468296\n","epoch 24 batch 0 loss = 46.49325\n","epoch 25 batch 0 loss = 46.501312\n","epoch 26 batch 0 loss = 46.47006\n","epoch 27 batch 0 loss = 46.483906\n","epoch 28 batch 0 loss = 46.487347\n","epoch 29 batch 0 loss = 46.52611\n","epoch 30 batch 0 loss = 46.481956\n","epoch 31 batch 0 loss = 46.502384\n","epoch 32 batch 0 loss = 47.107853\n","epoch 32 batch 1 loss = 46.47828\n","epoch 33 batch 0 loss = 46.51349\n","epoch 34 batch 0 loss = 46.497776\n","epoch 35 batch 0 loss = 46.507328\n","epoch 36 batch 0 loss = 46.49926\n","epoch 37 batch 0 loss = 46.47797\n","epoch 38 batch 0 loss = 46.477283\n","epoch 39 batch 0 loss = 46.502018\n","epoch 40 batch 0 loss = 46.471012\n","epoch 41 batch 0 loss = 46.47401\n","epoch 42 batch 0 loss = 46.490124\n","epoch 43 batch 0 loss = 46.503906\n","epoch 44 batch 0 loss = 46.485565\n","epoch 45 batch 0 loss = 46.459686\n","epoch 46 batch 0 loss = 46.49594\n","epoch 47 batch 0 loss = 46.482857\n","epoch 48 batch 0 loss = 46.77194\n","epoch 48 batch 1 loss = 46.477154\n","epoch 49 batch 0 loss = 46.50284\n","epoch 0 batch 0 loss = 41.060402\n","epoch 1 batch 0 loss = 39.85315\n","epoch 2 batch 0 loss = 38.742928\n","epoch 2 batch 1 loss = 37.572193\n","epoch 3 batch 0 loss = 36.054417\n","epoch 4 batch 0 loss = 34.743385\n","epoch 5 batch 0 loss = 32.911655\n","epoch 5 batch 1 loss = 31.851353\n","epoch 6 batch 0 loss = 30.470798\n","epoch 6 batch 1 loss = 28.563599\n","epoch 7 batch 0 loss = 26.85658\n","epoch 8 batch 0 loss = 25.10782\n","epoch 9 batch 0 loss = 23.41888\n","epoch 10 batch 0 loss = 21.89881\n","epoch 11 batch 0 loss = 20.999826\n","epoch 11 batch 1 loss = 19.737518\n","epoch 12 batch 0 loss = 19.0814\n","epoch 12 batch 1 loss = 18.444054\n","epoch 13 batch 0 loss = 18.169577\n","epoch 14 batch 0 loss = 18.014935\n","epoch 15 batch 0 loss = 17.909208\n","epoch 16 batch 0 loss = 17.8358\n","epoch 17 batch 0 loss = 17.780338\n","epoch 18 batch 0 loss = 17.89402\n","epoch 18 batch 1 loss = 17.69926\n","epoch 19 batch 0 loss = 17.661745\n","epoch 20 batch 0 loss = 17.64221\n","epoch 21 batch 0 loss = 17.839832\n","epoch 21 batch 1 loss = 17.601397\n","epoch 22 batch 0 loss = 17.597923\n","epoch 23 batch 0 loss = 17.58093\n","epoch 24 batch 0 loss = 17.703075\n","epoch 24 batch 1 loss = 17.56097\n","epoch 25 batch 0 loss = 17.658016\n","epoch 25 batch 1 loss = 17.536507\n","epoch 26 batch 0 loss = 17.714191\n","epoch 26 batch 1 loss = 17.62934\n","epoch 27 batch 0 loss = 17.534723\n","epoch 28 batch 0 loss = 17.53005\n","epoch 29 batch 0 loss = 17.530441\n","epoch 30 batch 0 loss = 17.673197\n","epoch 30 batch 1 loss = 17.53147\n","epoch 31 batch 0 loss = 17.67388\n","epoch 31 batch 1 loss = 17.602154\n","epoch 32 batch 0 loss = 17.519285\n","epoch 33 batch 0 loss = 17.502293\n","epoch 34 batch 0 loss = 17.527243\n","epoch 34 batch 1 loss = 17.486471\n","epoch 35 batch 0 loss = 17.733568\n","epoch 35 batch 1 loss = 17.689363\n","epoch 36 batch 0 loss = 17.487354\n","epoch 37 batch 0 loss = 17.492846\n","epoch 38 batch 0 loss = 17.496891\n","epoch 39 batch 0 loss = 17.452908\n","epoch 39 batch 1 loss = 17.495037\n","epoch 40 batch 0 loss = 17.492104\n","epoch 41 batch 0 loss = 17.499104\n","epoch 42 batch 0 loss = 17.498608\n","epoch 43 batch 0 loss = 17.476635\n","epoch 44 batch 0 loss = 17.498394\n","epoch 45 batch 0 loss = 17.497644\n","epoch 46 batch 0 loss = 17.471\n","epoch 47 batch 0 loss = 17.493164\n","epoch 48 batch 0 loss = 17.502207\n","epoch 49 batch 0 loss = 17.496685\n","epoch 0 batch 0 loss = 119.84097\n","epoch 1 batch 0 loss = 115.38353\n","epoch 2 batch 0 loss = 109.09308\n","epoch 3 batch 0 loss = 101.070786\n","epoch 4 batch 0 loss = 92.04396\n","epoch 5 batch 0 loss = 82.758446\n","epoch 6 batch 0 loss = 74.51088\n","epoch 7 batch 0 loss = 68.16117\n","epoch 8 batch 0 loss = 63.173004\n","epoch 9 batch 0 loss = 59.927227\n","epoch 10 batch 0 loss = 57.877975\n","epoch 11 batch 0 loss = 58.283863\n","epoch 12 batch 0 loss = 58.22736\n","epoch 13 batch 0 loss = 57.56951\n","epoch 14 batch 0 loss = 57.605896\n","epoch 15 batch 0 loss = 57.04591\n","epoch 16 batch 0 loss = 57.28171\n","epoch 17 batch 0 loss = 56.697983\n","epoch 18 batch 0 loss = 57.00954\n","epoch 19 batch 0 loss = 56.416435\n","epoch 20 batch 0 loss = 56.755207\n","epoch 21 batch 0 loss = 56.13529\n","epoch 22 batch 0 loss = 56.502792\n","epoch 23 batch 0 loss = 55.883427\n","epoch 24 batch 0 loss = 56.229828\n","epoch 25 batch 0 loss = 55.614983\n","epoch 26 batch 0 loss = 55.983418\n","epoch 27 batch 0 loss = 55.026222\n","epoch 28 batch 0 loss = 55.40999\n","epoch 29 batch 0 loss = 54.605938\n","epoch 30 batch 0 loss = 55.063473\n","epoch 31 batch 0 loss = 54.317043\n","epoch 32 batch 0 loss = 54.776394\n","epoch 33 batch 0 loss = 54.08005\n","epoch 34 batch 0 loss = 54.511955\n","epoch 35 batch 0 loss = 53.789814\n","epoch 36 batch 0 loss = 54.085648\n","epoch 37 batch 0 loss = 52.949474\n","epoch 38 batch 0 loss = 53.110634\n","epoch 39 batch 0 loss = 52.381317\n","epoch 40 batch 0 loss = 52.6939\n","epoch 41 batch 0 loss = 52.09315\n","epoch 42 batch 0 loss = 52.41798\n","epoch 43 batch 0 loss = 52.03723\n","epoch 44 batch 0 loss = 52.089626\n","epoch 45 batch 0 loss = 51.81821\n","epoch 46 batch 0 loss = 51.883434\n","epoch 47 batch 0 loss = 51.61676\n","epoch 48 batch 0 loss = 51.68091\n","epoch 49 batch 0 loss = 51.54178\n","epoch 0 batch 0 loss = 5.72515\n","epoch 0 batch 1 loss = 5.65342\n","epoch 0 batch 2 loss = 5.4569745\n","epoch 0 batch 3 loss = 5.7660513\n","epoch 0 batch 4 loss = 5.5717316\n","epoch 0 batch 5 loss = 5.838288\n","epoch 0 batch 6 loss = 5.31929\n","epoch 0 batch 7 loss = 5.868354\n","epoch 0 batch 8 loss = 5.556682\n","epoch 0 batch 9 loss = 5.9089756\n","epoch 0 batch 10 loss = 5.510647\n","epoch 0 batch 11 loss = 5.354358\n","epoch 0 batch 12 loss = 5.4046783\n","epoch 1 batch 0 loss = 5.430043\n","epoch 1 batch 1 loss = 5.496958\n","epoch 1 batch 2 loss = 5.213826\n","epoch 1 batch 3 loss = 5.165979\n","epoch 1 batch 4 loss = 5.1987877\n","epoch 1 batch 5 loss = 5.148209\n","epoch 1 batch 6 loss = 5.426107\n","epoch 1 batch 7 loss = 5.2606945\n","epoch 1 batch 8 loss = 5.228767\n","epoch 1 batch 9 loss = 5.12757\n","epoch 1 batch 10 loss = 5.0448923\n","epoch 1 batch 11 loss = 5.2223697\n","epoch 1 batch 12 loss = 5.2069273\n","epoch 1 batch 13 loss = 4.94701\n","epoch 1 batch 14 loss = 4.940247\n","epoch 2 batch 0 loss = 4.875273\n","epoch 2 batch 1 loss = 5.5225663\n","epoch 2 batch 2 loss = 4.926757\n","epoch 2 batch 3 loss = 4.5373635\n","epoch 2 batch 4 loss = 4.927313\n","epoch 2 batch 5 loss = 4.804187\n","epoch 2 batch 6 loss = 5.017588\n","epoch 2 batch 7 loss = 4.640546\n","epoch 2 batch 8 loss = 4.8741126\n","epoch 2 batch 9 loss = 4.9875712\n","epoch 2 batch 10 loss = 5.0336967\n","epoch 2 batch 11 loss = 4.354458\n","epoch 2 batch 12 loss = 4.814076\n","epoch 2 batch 13 loss = 4.721859\n","epoch 2 batch 14 loss = 5.0370893\n","epoch 2 batch 15 loss = 4.413559\n","epoch 2 batch 16 loss = 4.8995686\n","epoch 3 batch 0 loss = 4.715127\n","epoch 3 batch 1 loss = 4.7524977\n","epoch 3 batch 2 loss = 4.5868015\n","epoch 3 batch 3 loss = 4.4469566\n","epoch 3 batch 4 loss = 4.561231\n","epoch 3 batch 5 loss = 4.3968205\n","epoch 3 batch 6 loss = 4.536498\n","epoch 3 batch 7 loss = 4.8153358\n","epoch 3 batch 8 loss = 4.2871346\n","epoch 3 batch 9 loss = 4.3944263\n","epoch 3 batch 10 loss = 4.447907\n","epoch 3 batch 11 loss = 4.4480605\n","epoch 3 batch 12 loss = 4.7105827\n","epoch 3 batch 13 loss = 4.3600836\n","epoch 3 batch 14 loss = 4.0721717\n","epoch 3 batch 15 loss = 4.406577\n","epoch 3 batch 16 loss = 4.147474\n","epoch 3 batch 17 loss = 4.3000846\n","epoch 3 batch 18 loss = 4.4037566\n","epoch 3 batch 19 loss = 4.4834385\n","epoch 4 batch 0 loss = 4.5568056\n","epoch 4 batch 1 loss = 3.9795067\n","epoch 4 batch 2 loss = 4.3878756\n","epoch 4 batch 3 loss = 4.1552973\n","epoch 4 batch 4 loss = 3.7053776\n","epoch 4 batch 5 loss = 4.23209\n","epoch 4 batch 6 loss = 4.2856264\n","epoch 4 batch 7 loss = 4.08935\n","epoch 4 batch 8 loss = 4.0112376\n","epoch 4 batch 9 loss = 4.00208\n","epoch 4 batch 10 loss = 3.8160539\n","epoch 4 batch 11 loss = 3.7831588\n","epoch 4 batch 12 loss = 4.056309\n","epoch 4 batch 13 loss = 3.8576167\n","epoch 4 batch 14 loss = 4.073222\n","epoch 4 batch 15 loss = 4.21995\n","epoch 5 batch 0 loss = 4.0385265\n","epoch 5 batch 1 loss = 4.1505075\n","epoch 5 batch 2 loss = 3.6671228\n","epoch 5 batch 3 loss = 3.9513083\n","epoch 5 batch 4 loss = 3.853816\n","epoch 5 batch 5 loss = 3.97264\n","epoch 5 batch 6 loss = 3.728662\n","epoch 5 batch 7 loss = 3.8044953\n","epoch 5 batch 8 loss = 3.7077966\n","epoch 5 batch 9 loss = 3.8995492\n","epoch 5 batch 10 loss = 3.8954177\n","epoch 5 batch 11 loss = 3.6730516\n","epoch 5 batch 12 loss = 3.7459974\n","epoch 5 batch 13 loss = 3.7027538\n","epoch 5 batch 14 loss = 3.7335503\n","epoch 5 batch 15 loss = 3.6220994\n","epoch 5 batch 16 loss = 3.6617591\n","epoch 5 batch 17 loss = 3.5707877\n","epoch 5 batch 18 loss = 3.4916718\n","epoch 6 batch 0 loss = 3.4007668\n","epoch 6 batch 1 loss = 3.6642249\n","epoch 6 batch 2 loss = 3.4600508\n","epoch 6 batch 3 loss = 3.370101\n","epoch 6 batch 4 loss = 3.6573622\n","epoch 6 batch 5 loss = 3.7825174\n","epoch 6 batch 6 loss = 3.4224477\n","epoch 6 batch 7 loss = 3.1737835\n","epoch 6 batch 8 loss = 3.3916311\n","epoch 6 batch 9 loss = 3.3880608\n","epoch 6 batch 10 loss = 3.3807766\n","epoch 6 batch 11 loss = 3.5240703\n","epoch 6 batch 12 loss = 3.4677777\n","epoch 6 batch 13 loss = 3.523609\n","epoch 7 batch 0 loss = 3.4361787\n","epoch 7 batch 1 loss = 3.1923425\n","epoch 7 batch 2 loss = 3.4991646\n","epoch 7 batch 3 loss = 3.5426357\n","epoch 7 batch 4 loss = 3.4556167\n","epoch 7 batch 5 loss = 3.3708036\n","epoch 7 batch 6 loss = 3.4521844\n","epoch 7 batch 7 loss = 3.442757\n","epoch 7 batch 8 loss = 3.3396842\n","epoch 7 batch 9 loss = 3.2413268\n","epoch 7 batch 10 loss = 3.2786126\n","epoch 7 batch 11 loss = 3.3107786\n","epoch 7 batch 12 loss = 3.284329\n","epoch 7 batch 13 loss = 3.4811006\n","epoch 7 batch 14 loss = 3.2942712\n","epoch 7 batch 15 loss = 3.3788567\n","epoch 7 batch 16 loss = 3.0140994\n","epoch 7 batch 17 loss = 3.338955\n","epoch 7 batch 18 loss = 3.3623805\n","epoch 8 batch 0 loss = 3.0780864\n","epoch 8 batch 1 loss = 3.30898\n","epoch 8 batch 2 loss = 3.1146698\n","epoch 8 batch 3 loss = 3.1740818\n","epoch 8 batch 4 loss = 3.264783\n","epoch 8 batch 5 loss = 3.2789936\n","epoch 8 batch 6 loss = 3.3113303\n","epoch 8 batch 7 loss = 3.2500727\n","epoch 8 batch 8 loss = 3.2520912\n","epoch 8 batch 9 loss = 3.3244438\n","epoch 8 batch 10 loss = 3.1851215\n","epoch 8 batch 11 loss = 3.43076\n","epoch 8 batch 12 loss = 2.9722939\n","epoch 8 batch 13 loss = 3.1461163\n","epoch 8 batch 14 loss = 2.984707\n","epoch 8 batch 15 loss = 2.891723\n","epoch 8 batch 16 loss = 3.4267075\n","epoch 8 batch 17 loss = 3.07416\n","epoch 9 batch 0 loss = 3.0506563\n","epoch 9 batch 1 loss = 3.0537434\n","epoch 9 batch 2 loss = 2.6334496\n","epoch 9 batch 3 loss = 3.3175187\n","epoch 9 batch 4 loss = 3.1190102\n","epoch 9 batch 5 loss = 3.1672907\n","epoch 9 batch 6 loss = 2.9813018\n","epoch 9 batch 7 loss = 2.9930596\n","epoch 9 batch 8 loss = 3.367312\n","epoch 9 batch 9 loss = 3.0522828\n","epoch 9 batch 10 loss = 2.9798012\n","epoch 9 batch 11 loss = 3.173586\n","epoch 9 batch 12 loss = 2.927789\n","epoch 9 batch 13 loss = 3.0221179\n","epoch 9 batch 14 loss = 2.9809206\n","epoch 10 batch 0 loss = 3.1735113\n","epoch 10 batch 1 loss = 3.0510247\n","epoch 10 batch 2 loss = 3.0083191\n","epoch 10 batch 3 loss = 3.168538\n","epoch 10 batch 4 loss = 2.9336817\n","epoch 10 batch 5 loss = 3.26265\n","epoch 10 batch 6 loss = 3.138586\n","epoch 10 batch 7 loss = 2.9344833\n","epoch 10 batch 8 loss = 3.0309935\n","epoch 10 batch 9 loss = 3.2497106\n","epoch 10 batch 10 loss = 3.0356371\n","epoch 10 batch 11 loss = 3.0928485\n","epoch 10 batch 12 loss = 3.17207\n","epoch 10 batch 13 loss = 3.0725396\n","epoch 10 batch 14 loss = 3.0229332\n","epoch 10 batch 15 loss = 2.8614144\n","epoch 10 batch 16 loss = 3.0942247\n","epoch 10 batch 17 loss = 3.076487\n","epoch 11 batch 0 loss = 2.906468\n","epoch 11 batch 1 loss = 2.7604742\n","epoch 11 batch 2 loss = 3.066638\n","epoch 11 batch 3 loss = 3.1071403\n","epoch 11 batch 4 loss = 2.8499646\n","epoch 11 batch 5 loss = 2.982794\n","epoch 11 batch 6 loss = 3.212545\n","epoch 11 batch 7 loss = 2.9728203\n","epoch 11 batch 8 loss = 2.9982336\n","epoch 11 batch 9 loss = 2.9813225\n","epoch 11 batch 10 loss = 3.0426226\n","epoch 11 batch 11 loss = 3.0065203\n","epoch 11 batch 12 loss = 3.167029\n","epoch 11 batch 13 loss = 3.0820322\n","epoch 11 batch 14 loss = 2.7659583\n","epoch 11 batch 15 loss = 2.9951406\n","epoch 11 batch 16 loss = 2.869678\n","epoch 11 batch 17 loss = 3.227315\n","epoch 11 batch 18 loss = 3.0217702\n","epoch 12 batch 0 loss = 2.949007\n","epoch 12 batch 1 loss = 2.9655757\n","epoch 12 batch 2 loss = 3.0178654\n","epoch 12 batch 3 loss = 3.0680134\n","epoch 12 batch 4 loss = 3.2088692\n","epoch 12 batch 5 loss = 3.2177024\n","epoch 12 batch 6 loss = 3.2418396\n","epoch 12 batch 7 loss = 2.9332316\n","epoch 12 batch 8 loss = 3.0280674\n","epoch 12 batch 9 loss = 2.9622982\n","epoch 12 batch 10 loss = 2.9940295\n","epoch 12 batch 11 loss = 3.026447\n","epoch 12 batch 12 loss = 3.0587673\n","epoch 12 batch 13 loss = 2.8126683\n","epoch 12 batch 14 loss = 2.9649205\n","epoch 13 batch 0 loss = 3.0053096\n","epoch 13 batch 1 loss = 2.9814548\n","epoch 13 batch 2 loss = 3.049533\n","epoch 13 batch 3 loss = 2.9691398\n","epoch 13 batch 4 loss = 3.068732\n","epoch 13 batch 5 loss = 3.1291132\n","epoch 13 batch 6 loss = 2.8863342\n","epoch 13 batch 7 loss = 2.9838297\n","epoch 13 batch 8 loss = 2.913973\n","epoch 13 batch 9 loss = 2.7159762\n","epoch 13 batch 10 loss = 2.9592319\n","epoch 13 batch 11 loss = 2.8745425\n","epoch 13 batch 12 loss = 3.0554585\n","epoch 13 batch 13 loss = 3.0775247\n","epoch 13 batch 14 loss = 2.897511\n","epoch 13 batch 15 loss = 2.9381437\n","epoch 13 batch 16 loss = 2.7983944\n","epoch 14 batch 0 loss = 2.7743614\n","epoch 14 batch 1 loss = 3.1221051\n","epoch 14 batch 2 loss = 2.9753294\n","epoch 14 batch 3 loss = 3.0450625\n","epoch 14 batch 4 loss = 2.9426744\n","epoch 14 batch 5 loss = 2.9892542\n","epoch 14 batch 6 loss = 2.733881\n","epoch 14 batch 7 loss = 2.9937105\n","epoch 14 batch 8 loss = 2.9527204\n","epoch 14 batch 9 loss = 3.0396113\n","epoch 14 batch 10 loss = 2.9715955\n","epoch 14 batch 11 loss = 2.8630981\n","epoch 14 batch 12 loss = 3.0258305\n","epoch 14 batch 13 loss = 2.952031\n","epoch 14 batch 14 loss = 2.9097989\n","epoch 14 batch 15 loss = 2.782401\n","epoch 14 batch 16 loss = 2.9460354\n","epoch 15 batch 0 loss = 2.8740587\n","epoch 15 batch 1 loss = 2.9145606\n","epoch 15 batch 2 loss = 2.8991854\n","epoch 15 batch 3 loss = 2.8544462\n","epoch 15 batch 4 loss = 3.0917706\n","epoch 15 batch 5 loss = 3.083029\n","epoch 15 batch 6 loss = 2.8915858\n","epoch 15 batch 7 loss = 3.0884323\n","epoch 15 batch 8 loss = 3.0540266\n","epoch 15 batch 9 loss = 2.8257124\n","epoch 15 batch 10 loss = 2.976652\n","epoch 15 batch 11 loss = 2.793736\n","epoch 15 batch 12 loss = 2.7608237\n","epoch 15 batch 13 loss = 3.107542\n","epoch 15 batch 14 loss = 2.893812\n","epoch 15 batch 15 loss = 2.9374545\n","epoch 15 batch 16 loss = 2.7193818\n","epoch 15 batch 17 loss = 2.9422562\n","epoch 15 batch 18 loss = 2.9363205\n","epoch 16 batch 0 loss = 3.010661\n","epoch 16 batch 1 loss = 2.7764006\n","epoch 16 batch 2 loss = 2.8728213\n","epoch 16 batch 3 loss = 2.7269793\n","epoch 16 batch 4 loss = 2.8540828\n","epoch 16 batch 5 loss = 2.998602\n","epoch 16 batch 6 loss = 3.00496\n","epoch 16 batch 7 loss = 2.9781654\n","epoch 16 batch 8 loss = 3.0445344\n","epoch 16 batch 9 loss = 2.8464584\n","epoch 16 batch 10 loss = 2.9709861\n","epoch 16 batch 11 loss = 2.859835\n","epoch 16 batch 12 loss = 2.9101706\n","epoch 17 batch 0 loss = 3.0561388\n","epoch 17 batch 1 loss = 2.8991897\n","epoch 17 batch 2 loss = 2.9813228\n","epoch 17 batch 3 loss = 3.10297\n","epoch 17 batch 4 loss = 2.9912267\n","epoch 17 batch 5 loss = 2.9196074\n","epoch 17 batch 6 loss = 2.9705858\n","epoch 17 batch 7 loss = 3.1461186\n","epoch 17 batch 8 loss = 2.8651688\n","epoch 17 batch 9 loss = 2.8549647\n","epoch 17 batch 10 loss = 3.0303729\n","epoch 17 batch 11 loss = 2.7993472\n","epoch 17 batch 12 loss = 3.1077063\n","epoch 17 batch 13 loss = 2.9509075\n","epoch 18 batch 0 loss = 2.9464574\n","epoch 18 batch 1 loss = 2.8382568\n","epoch 18 batch 2 loss = 2.8121107\n","epoch 18 batch 3 loss = 2.960487\n","epoch 18 batch 4 loss = 3.0274572\n","epoch 18 batch 5 loss = 3.0592866\n","epoch 18 batch 6 loss = 2.8443527\n","epoch 18 batch 7 loss = 2.9400537\n","epoch 18 batch 8 loss = 2.9483597\n","epoch 18 batch 9 loss = 2.9483786\n","epoch 18 batch 10 loss = 2.8034391\n","epoch 18 batch 11 loss = 3.0211916\n","epoch 18 batch 12 loss = 2.7760296\n","epoch 18 batch 13 loss = 2.819502\n","epoch 18 batch 14 loss = 2.8371382\n","epoch 19 batch 0 loss = 2.9681168\n","epoch 19 batch 1 loss = 2.7517757\n","epoch 19 batch 2 loss = 2.9811163\n","epoch 19 batch 3 loss = 2.9037247\n","epoch 19 batch 4 loss = 2.7926497\n","epoch 19 batch 5 loss = 2.8401415\n","epoch 19 batch 6 loss = 3.0864768\n","epoch 19 batch 7 loss = 2.9539554\n","epoch 19 batch 8 loss = 2.96582\n","epoch 19 batch 9 loss = 2.7747602\n","epoch 19 batch 10 loss = 2.8549895\n","epoch 19 batch 11 loss = 2.7109294\n","epoch 19 batch 12 loss = 2.875162\n","epoch 19 batch 13 loss = 2.9478755\n","epoch 19 batch 14 loss = 2.9024613\n","epoch 19 batch 15 loss = 2.8830113\n","epoch 19 batch 16 loss = 2.9158359\n","epoch 19 batch 17 loss = 2.91314\n","epoch 20 batch 0 loss = 2.8104289\n","epoch 20 batch 1 loss = 3.0164332\n","epoch 20 batch 2 loss = 2.8510106\n","epoch 20 batch 3 loss = 2.9421954\n","epoch 20 batch 4 loss = 2.7305589\n","epoch 20 batch 5 loss = 2.7360423\n","epoch 20 batch 6 loss = 2.9860978\n","epoch 20 batch 7 loss = 2.6456046\n","epoch 20 batch 8 loss = 2.9633112\n","epoch 20 batch 9 loss = 2.9384682\n","epoch 20 batch 10 loss = 3.1030958\n","epoch 20 batch 11 loss = 2.7601664\n","epoch 20 batch 12 loss = 2.9747663\n","epoch 20 batch 13 loss = 3.2492409\n","epoch 20 batch 14 loss = 3.0937152\n","epoch 20 batch 15 loss = 2.7249403\n","epoch 20 batch 16 loss = 2.9558938\n","epoch 20 batch 17 loss = 2.8817458\n","epoch 20 batch 18 loss = 2.9924088\n","epoch 21 batch 0 loss = 3.0656781\n","epoch 21 batch 1 loss = 3.1122193\n","epoch 21 batch 2 loss = 2.966561\n","epoch 21 batch 3 loss = 3.04048\n","epoch 21 batch 4 loss = 2.9038322\n","epoch 21 batch 5 loss = 2.9285054\n","epoch 21 batch 6 loss = 2.908034\n","epoch 21 batch 7 loss = 2.6714609\n","epoch 21 batch 8 loss = 2.9809082\n","epoch 21 batch 9 loss = 2.9249885\n","epoch 21 batch 10 loss = 3.0392802\n","epoch 21 batch 11 loss = 2.804608\n","epoch 21 batch 12 loss = 2.8570566\n","epoch 21 batch 13 loss = 2.8123026\n","epoch 21 batch 14 loss = 3.0579286\n","epoch 22 batch 0 loss = 2.93832\n","epoch 22 batch 1 loss = 3.0033147\n","epoch 22 batch 2 loss = 3.004813\n","epoch 22 batch 3 loss = 2.7946782\n","epoch 22 batch 4 loss = 3.021298\n","epoch 22 batch 5 loss = 2.9703703\n","epoch 22 batch 6 loss = 3.061879\n","epoch 22 batch 7 loss = 2.856035\n","epoch 22 batch 8 loss = 3.0865953\n","epoch 22 batch 9 loss = 2.789372\n","epoch 22 batch 10 loss = 2.852281\n","epoch 22 batch 11 loss = 2.503792\n","epoch 22 batch 12 loss = 3.106559\n","epoch 22 batch 13 loss = 2.8581238\n","epoch 22 batch 14 loss = 2.8036065\n","epoch 23 batch 0 loss = 2.948144\n","epoch 23 batch 1 loss = 3.0052848\n","epoch 23 batch 2 loss = 2.964222\n","epoch 23 batch 3 loss = 2.6976857\n","epoch 23 batch 4 loss = 2.8958812\n","epoch 23 batch 5 loss = 2.9506557\n","epoch 23 batch 6 loss = 2.8232737\n","epoch 23 batch 7 loss = 2.9385111\n","epoch 23 batch 8 loss = 2.9078135\n","epoch 23 batch 9 loss = 2.9675565\n","epoch 23 batch 10 loss = 2.5534778\n","epoch 23 batch 11 loss = 2.8925729\n","epoch 23 batch 12 loss = 3.016548\n","epoch 23 batch 13 loss = 2.760813\n","epoch 23 batch 14 loss = 2.982019\n","epoch 23 batch 15 loss = 2.9900742\n","epoch 24 batch 0 loss = 2.900535\n","epoch 24 batch 1 loss = 2.894128\n","epoch 24 batch 2 loss = 2.8602269\n","epoch 24 batch 3 loss = 3.0706522\n","epoch 24 batch 4 loss = 2.8604732\n","epoch 24 batch 5 loss = 2.8868032\n","epoch 24 batch 6 loss = 2.966987\n","epoch 24 batch 7 loss = 2.8835912\n","epoch 24 batch 8 loss = 2.7104802\n","epoch 24 batch 9 loss = 2.706433\n","epoch 24 batch 10 loss = 2.7666764\n","epoch 24 batch 11 loss = 2.950428\n","epoch 24 batch 12 loss = 2.9374995\n","epoch 24 batch 13 loss = 2.727501\n","epoch 24 batch 14 loss = 2.7723238\n","epoch 24 batch 15 loss = 2.8355453\n","epoch 24 batch 16 loss = 3.0704966\n","epoch 24 batch 17 loss = 2.8410628\n","epoch 24 batch 18 loss = 2.8808615\n","epoch 24 batch 19 loss = 2.619402\n","epoch 24 batch 20 loss = 2.861488\n","epoch 24 batch 21 loss = 2.8897815\n","epoch 24 batch 22 loss = 3.0034132\n","epoch 24 batch 23 loss = 2.9672062\n","epoch 25 batch 0 loss = 3.0112484\n","epoch 25 batch 1 loss = 2.9839802\n","epoch 25 batch 2 loss = 2.7727516\n","epoch 25 batch 3 loss = 2.8015182\n","epoch 25 batch 4 loss = 2.9639766\n","epoch 25 batch 5 loss = 2.6894958\n","epoch 25 batch 6 loss = 2.9544647\n","epoch 25 batch 7 loss = 2.916524\n","epoch 25 batch 8 loss = 3.1153548\n","epoch 25 batch 9 loss = 2.997479\n","epoch 25 batch 10 loss = 2.7788224\n","epoch 25 batch 11 loss = 2.9099936\n","epoch 25 batch 12 loss = 2.8124459\n","epoch 25 batch 13 loss = 2.8374465\n","epoch 25 batch 14 loss = 2.9648445\n","epoch 25 batch 15 loss = 2.9384906\n","epoch 25 batch 16 loss = 2.7634084\n","epoch 25 batch 17 loss = 2.754609\n","epoch 26 batch 0 loss = 2.8008893\n","epoch 26 batch 1 loss = 2.8909807\n","epoch 26 batch 2 loss = 2.8286622\n","epoch 26 batch 3 loss = 2.8537126\n","epoch 26 batch 4 loss = 3.0497565\n","epoch 26 batch 5 loss = 2.9259877\n","epoch 26 batch 6 loss = 2.928927\n","epoch 26 batch 7 loss = 2.9498935\n","epoch 26 batch 8 loss = 3.0786197\n","epoch 26 batch 9 loss = 2.8443117\n","epoch 26 batch 10 loss = 2.9666266\n","epoch 26 batch 11 loss = 2.7111797\n","epoch 26 batch 12 loss = 2.8680265\n","epoch 26 batch 13 loss = 2.9219189\n","epoch 26 batch 14 loss = 2.8907037\n","epoch 26 batch 15 loss = 2.63207\n","epoch 26 batch 16 loss = 2.9341202\n","epoch 27 batch 0 loss = 2.743082\n","epoch 27 batch 1 loss = 2.823204\n","epoch 27 batch 2 loss = 2.8887217\n","epoch 27 batch 3 loss = 3.1544538\n","epoch 27 batch 4 loss = 2.9294198\n","epoch 27 batch 5 loss = 2.952372\n","epoch 27 batch 6 loss = 3.0393345\n","epoch 27 batch 7 loss = 2.7116032\n","epoch 27 batch 8 loss = 2.792466\n","epoch 27 batch 9 loss = 2.9040327\n","epoch 27 batch 10 loss = 2.914128\n","epoch 27 batch 11 loss = 3.035981\n","epoch 27 batch 12 loss = 3.0136955\n","epoch 27 batch 13 loss = 3.001612\n","epoch 27 batch 14 loss = 2.967086\n","epoch 27 batch 15 loss = 3.0447993\n","epoch 27 batch 16 loss = 2.6182666\n","epoch 27 batch 17 loss = 2.830381\n","epoch 28 batch 0 loss = 2.9646678\n","epoch 28 batch 1 loss = 2.907617\n","epoch 28 batch 2 loss = 2.749662\n","epoch 28 batch 3 loss = 3.0641878\n","epoch 28 batch 4 loss = 2.8730364\n","epoch 28 batch 5 loss = 2.7665818\n","epoch 28 batch 6 loss = 2.8606207\n","epoch 28 batch 7 loss = 2.8865893\n","epoch 28 batch 8 loss = 3.0004144\n","epoch 28 batch 9 loss = 2.8806324\n","epoch 28 batch 10 loss = 2.9459383\n","epoch 28 batch 11 loss = 2.8728557\n","epoch 28 batch 12 loss = 3.0653787\n","epoch 28 batch 13 loss = 2.9483445\n","epoch 28 batch 14 loss = 2.947748\n","epoch 28 batch 15 loss = 2.6734753\n","epoch 28 batch 16 loss = 2.8945446\n","epoch 28 batch 17 loss = 2.879809\n","epoch 28 batch 18 loss = 2.8997822\n","epoch 29 batch 0 loss = 2.7787924\n","epoch 29 batch 1 loss = 2.8968952\n","epoch 29 batch 2 loss = 2.6147065\n","epoch 29 batch 3 loss = 3.0082\n","epoch 29 batch 4 loss = 3.078482\n","epoch 29 batch 5 loss = 2.8643703\n","epoch 29 batch 6 loss = 2.6939309\n","epoch 29 batch 7 loss = 2.8411324\n","epoch 29 batch 8 loss = 3.1139128\n","epoch 29 batch 9 loss = 2.8576827\n","epoch 29 batch 10 loss = 2.8303967\n","epoch 29 batch 11 loss = 2.912502\n","epoch 29 batch 12 loss = 2.9534092\n","epoch 29 batch 13 loss = 2.9450092\n","epoch 29 batch 14 loss = 2.9176884\n","epoch 29 batch 15 loss = 2.9374049\n","epoch 30 batch 0 loss = 3.0619898\n","epoch 30 batch 1 loss = 2.9483864\n","epoch 30 batch 2 loss = 3.1138113\n","epoch 30 batch 3 loss = 3.02791\n","epoch 30 batch 4 loss = 2.8270624\n","epoch 30 batch 5 loss = 2.9471908\n","epoch 30 batch 6 loss = 3.0765133\n","epoch 30 batch 7 loss = 2.8050973\n","epoch 30 batch 8 loss = 2.817023\n","epoch 30 batch 9 loss = 2.7234797\n","epoch 30 batch 10 loss = 2.9637382\n","epoch 30 batch 11 loss = 2.879394\n","epoch 30 batch 12 loss = 2.9189465\n","epoch 30 batch 13 loss = 2.7465494\n","epoch 30 batch 14 loss = 2.8893743\n","epoch 30 batch 15 loss = 2.8302784\n","epoch 30 batch 16 loss = 2.850335\n","epoch 30 batch 17 loss = 2.857167\n","epoch 30 batch 18 loss = 3.0104647\n","epoch 30 batch 19 loss = 2.7338722\n","epoch 30 batch 20 loss = 3.0328755\n","epoch 31 batch 0 loss = 2.9295483\n","epoch 31 batch 1 loss = 2.9421542\n","epoch 31 batch 2 loss = 2.7976346\n","epoch 31 batch 3 loss = 2.7947032\n","epoch 31 batch 4 loss = 2.9690416\n","epoch 31 batch 5 loss = 2.925385\n","epoch 31 batch 6 loss = 2.9650886\n","epoch 31 batch 7 loss = 2.863252\n","epoch 31 batch 8 loss = 2.776907\n","epoch 31 batch 9 loss = 2.8581152\n","epoch 31 batch 10 loss = 2.8285735\n","epoch 31 batch 11 loss = 2.8096666\n","epoch 31 batch 12 loss = 2.8845515\n","epoch 31 batch 13 loss = 2.7674513\n","epoch 31 batch 14 loss = 3.08456\n","epoch 31 batch 15 loss = 2.8963542\n","epoch 31 batch 16 loss = 2.9521232\n","epoch 31 batch 17 loss = 2.8452117\n","epoch 32 batch 0 loss = 3.0779338\n","epoch 32 batch 1 loss = 2.962037\n","epoch 32 batch 2 loss = 2.818817\n","epoch 32 batch 3 loss = 2.8869765\n","epoch 32 batch 4 loss = 2.7997785\n","epoch 32 batch 5 loss = 2.8889103\n","epoch 32 batch 6 loss = 3.026282\n","epoch 32 batch 7 loss = 2.8764625\n","epoch 32 batch 8 loss = 3.0421472\n","epoch 32 batch 9 loss = 2.8502321\n","epoch 32 batch 10 loss = 2.9330063\n","epoch 32 batch 11 loss = 3.0201373\n","epoch 32 batch 12 loss = 2.7526014\n","epoch 32 batch 13 loss = 2.6887589\n","epoch 33 batch 0 loss = 2.912702\n","epoch 33 batch 1 loss = 2.6137154\n","epoch 33 batch 2 loss = 2.9341471\n","epoch 33 batch 3 loss = 2.8745558\n","epoch 33 batch 4 loss = 2.8748589\n","epoch 33 batch 5 loss = 2.846071\n","epoch 33 batch 6 loss = 2.851744\n","epoch 33 batch 7 loss = 3.0886025\n","epoch 33 batch 8 loss = 2.8319466\n","epoch 33 batch 9 loss = 2.8555026\n","epoch 33 batch 10 loss = 2.8632119\n","epoch 33 batch 11 loss = 2.8585234\n","epoch 33 batch 12 loss = 3.0169365\n","epoch 33 batch 13 loss = 2.919676\n","epoch 33 batch 14 loss = 2.9339275\n","epoch 33 batch 15 loss = 3.1284885\n","epoch 33 batch 16 loss = 2.8576484\n","epoch 33 batch 17 loss = 2.9597325\n","epoch 33 batch 18 loss = 2.8981552\n","epoch 33 batch 19 loss = 2.9382646\n","epoch 34 batch 0 loss = 2.9539568\n","epoch 34 batch 1 loss = 2.852817\n","epoch 34 batch 2 loss = 2.9539835\n","epoch 34 batch 3 loss = 2.9594097\n","epoch 34 batch 4 loss = 3.1003244\n","epoch 34 batch 5 loss = 2.987682\n","epoch 34 batch 6 loss = 2.960935\n","epoch 34 batch 7 loss = 2.8422682\n","epoch 34 batch 8 loss = 3.0226257\n","epoch 34 batch 9 loss = 2.994453\n","epoch 34 batch 10 loss = 2.9197254\n","epoch 34 batch 11 loss = 2.915733\n","epoch 34 batch 12 loss = 2.9545133\n","epoch 35 batch 0 loss = 2.807007\n","epoch 35 batch 1 loss = 2.9586298\n","epoch 35 batch 2 loss = 2.801931\n","epoch 35 batch 3 loss = 2.7132592\n","epoch 35 batch 4 loss = 2.6729975\n","epoch 35 batch 5 loss = 3.2605536\n","epoch 35 batch 6 loss = 2.7270784\n","epoch 35 batch 7 loss = 2.7483554\n","epoch 35 batch 8 loss = 2.9005568\n","epoch 35 batch 9 loss = 2.8133268\n","epoch 35 batch 10 loss = 2.9362304\n","epoch 35 batch 11 loss = 2.7568944\n","epoch 35 batch 12 loss = 3.1455727\n","epoch 35 batch 13 loss = 2.8768802\n","epoch 36 batch 0 loss = 2.8016982\n","epoch 36 batch 1 loss = 2.755583\n","epoch 36 batch 2 loss = 3.0174403\n","epoch 36 batch 3 loss = 2.9450185\n","epoch 36 batch 4 loss = 2.9707377\n","epoch 36 batch 5 loss = 2.7612276\n","epoch 36 batch 6 loss = 2.861033\n","epoch 36 batch 7 loss = 2.866887\n","epoch 36 batch 8 loss = 2.7576585\n","epoch 36 batch 9 loss = 2.9506133\n","epoch 36 batch 10 loss = 3.0316567\n","epoch 36 batch 11 loss = 3.0419886\n","epoch 36 batch 12 loss = 2.9585924\n","epoch 36 batch 13 loss = 2.8479655\n","epoch 36 batch 14 loss = 2.9858983\n","epoch 37 batch 0 loss = 3.0053515\n","epoch 37 batch 1 loss = 2.8237228\n","epoch 37 batch 2 loss = 2.9618993\n","epoch 37 batch 3 loss = 2.9561949\n","epoch 37 batch 4 loss = 3.0511692\n","epoch 37 batch 5 loss = 3.0079107\n","epoch 37 batch 6 loss = 2.7618954\n","epoch 37 batch 7 loss = 2.9486697\n","epoch 37 batch 8 loss = 2.8903513\n","epoch 37 batch 9 loss = 3.0169337\n","epoch 37 batch 10 loss = 2.9522293\n","epoch 37 batch 11 loss = 3.052905\n","epoch 37 batch 12 loss = 2.8565617\n","epoch 37 batch 13 loss = 2.653013\n","epoch 37 batch 14 loss = 3.0275946\n","epoch 37 batch 15 loss = 2.96287\n","epoch 37 batch 16 loss = 3.0568187\n","epoch 37 batch 17 loss = 2.832563\n","epoch 37 batch 18 loss = 2.8675785\n","epoch 38 batch 0 loss = 2.8935091\n","epoch 38 batch 1 loss = 2.787018\n","epoch 38 batch 2 loss = 2.8783846\n","epoch 38 batch 3 loss = 3.0434666\n","epoch 38 batch 4 loss = 2.9692476\n","epoch 38 batch 5 loss = 2.7326565\n","epoch 38 batch 6 loss = 2.765192\n","epoch 38 batch 7 loss = 2.8836806\n","epoch 38 batch 8 loss = 2.6049058\n","epoch 38 batch 9 loss = 2.749859\n","epoch 38 batch 10 loss = 2.7743328\n","epoch 38 batch 11 loss = 2.9939725\n","epoch 38 batch 12 loss = 3.0458915\n","epoch 38 batch 13 loss = 2.8195798\n","epoch 39 batch 0 loss = 2.7392812\n","epoch 39 batch 1 loss = 3.0754287\n","epoch 39 batch 2 loss = 2.902727\n","epoch 39 batch 3 loss = 2.8284302\n","epoch 39 batch 4 loss = 2.902439\n","epoch 39 batch 5 loss = 2.7979078\n","epoch 39 batch 6 loss = 2.9713395\n","epoch 39 batch 7 loss = 2.9915333\n","epoch 39 batch 8 loss = 3.0184255\n","epoch 39 batch 9 loss = 2.7944505\n","epoch 39 batch 10 loss = 2.998254\n","epoch 39 batch 11 loss = 3.077485\n","epoch 39 batch 12 loss = 2.900881\n","epoch 39 batch 13 loss = 3.0428557\n","epoch 39 batch 14 loss = 3.0946667\n","epoch 39 batch 15 loss = 2.7360554\n","epoch 39 batch 16 loss = 2.487704\n","epoch 39 batch 17 loss = 2.850669\n","epoch 39 batch 18 loss = 2.8326142\n","epoch 39 batch 19 loss = 3.0248678\n","epoch 39 batch 20 loss = 2.8192189\n","epoch 39 batch 21 loss = 2.8674042\n","epoch 40 batch 0 loss = 2.8451138\n","epoch 40 batch 1 loss = 2.7731454\n","epoch 40 batch 2 loss = 2.939235\n","epoch 40 batch 3 loss = 3.0571876\n","epoch 40 batch 4 loss = 3.0645971\n","epoch 40 batch 5 loss = 2.8962796\n","epoch 40 batch 6 loss = 2.9218106\n","epoch 40 batch 7 loss = 2.9573615\n","epoch 40 batch 8 loss = 2.9568758\n","epoch 40 batch 9 loss = 2.9183981\n","epoch 40 batch 10 loss = 2.851155\n","epoch 40 batch 11 loss = 2.9868793\n","epoch 40 batch 12 loss = 2.8390355\n","epoch 40 batch 13 loss = 2.911502\n","epoch 40 batch 14 loss = 2.781325\n","epoch 40 batch 15 loss = 2.8259718\n","epoch 40 batch 16 loss = 2.9028893\n","epoch 40 batch 17 loss = 3.0177438\n","epoch 41 batch 0 loss = 2.797827\n","epoch 41 batch 1 loss = 2.976159\n","epoch 41 batch 2 loss = 2.891002\n","epoch 41 batch 3 loss = 2.8311226\n","epoch 41 batch 4 loss = 2.992598\n","epoch 41 batch 5 loss = 2.7017019\n","epoch 41 batch 6 loss = 2.84958\n","epoch 41 batch 7 loss = 2.9790514\n","epoch 41 batch 8 loss = 2.806314\n","epoch 41 batch 9 loss = 2.7606878\n","epoch 41 batch 10 loss = 2.8334079\n","epoch 41 batch 11 loss = 2.8732312\n","epoch 41 batch 12 loss = 3.0535574\n","epoch 41 batch 13 loss = 2.7801394\n","epoch 41 batch 14 loss = 2.750662\n","epoch 41 batch 15 loss = 2.9720128\n","epoch 42 batch 0 loss = 2.9517756\n","epoch 42 batch 1 loss = 2.9987903\n","epoch 42 batch 2 loss = 3.0392082\n","epoch 42 batch 3 loss = 2.8903015\n","epoch 42 batch 4 loss = 2.9321504\n","epoch 42 batch 5 loss = 2.8929524\n","epoch 42 batch 6 loss = 2.8249125\n","epoch 42 batch 7 loss = 2.9339182\n","epoch 42 batch 8 loss = 3.107095\n","epoch 42 batch 9 loss = 2.7515392\n","epoch 42 batch 10 loss = 2.8621597\n","epoch 42 batch 11 loss = 2.932049\n","epoch 42 batch 12 loss = 3.0157225\n","epoch 42 batch 13 loss = 2.8023784\n","epoch 43 batch 0 loss = 2.9392405\n","epoch 43 batch 1 loss = 2.8524249\n","epoch 43 batch 2 loss = 2.959282\n","epoch 43 batch 3 loss = 2.797461\n","epoch 43 batch 4 loss = 2.8517714\n","epoch 43 batch 5 loss = 3.1114295\n","epoch 43 batch 6 loss = 2.6259737\n","epoch 43 batch 7 loss = 2.7241569\n","epoch 43 batch 8 loss = 2.9392545\n","epoch 43 batch 9 loss = 2.7769842\n","epoch 43 batch 10 loss = 2.9559774\n","epoch 43 batch 11 loss = 2.8425622\n","epoch 43 batch 12 loss = 2.8579087\n","epoch 43 batch 13 loss = 2.977563\n","epoch 43 batch 14 loss = 2.8059998\n","epoch 43 batch 15 loss = 2.9422388\n","epoch 44 batch 0 loss = 2.9649894\n","epoch 44 batch 1 loss = 2.9078145\n","epoch 44 batch 2 loss = 2.951257\n","epoch 44 batch 3 loss = 2.8464808\n","epoch 44 batch 4 loss = 2.9250348\n","epoch 44 batch 5 loss = 3.0121772\n","epoch 44 batch 6 loss = 2.7300344\n","epoch 44 batch 7 loss = 2.7640023\n","epoch 44 batch 8 loss = 2.8766334\n","epoch 44 batch 9 loss = 2.8361146\n","epoch 44 batch 10 loss = 2.7078738\n","epoch 44 batch 11 loss = 2.968015\n","epoch 44 batch 12 loss = 2.9713404\n","epoch 44 batch 13 loss = 3.042831\n","epoch 44 batch 14 loss = 3.0201383\n","epoch 44 batch 15 loss = 3.030953\n","epoch 45 batch 0 loss = 2.8448906\n","epoch 45 batch 1 loss = 2.8962924\n","epoch 45 batch 2 loss = 2.9274864\n","epoch 45 batch 3 loss = 2.8485568\n","epoch 45 batch 4 loss = 2.9563377\n","epoch 45 batch 5 loss = 2.8423135\n","epoch 45 batch 6 loss = 2.8188732\n","epoch 45 batch 7 loss = 2.9116359\n","epoch 45 batch 8 loss = 2.8703256\n","epoch 45 batch 9 loss = 2.7448266\n","epoch 45 batch 10 loss = 2.9951828\n","epoch 45 batch 11 loss = 2.8127198\n","epoch 45 batch 12 loss = 2.9061995\n","epoch 45 batch 13 loss = 2.9793763\n","epoch 45 batch 14 loss = 2.9164073\n","epoch 45 batch 15 loss = 2.9622498\n","epoch 46 batch 0 loss = 2.8355691\n","epoch 46 batch 1 loss = 2.8281093\n","epoch 46 batch 2 loss = 2.9003131\n","epoch 46 batch 3 loss = 2.967997\n","epoch 46 batch 4 loss = 2.922834\n","epoch 46 batch 5 loss = 2.7755466\n","epoch 46 batch 6 loss = 2.8735108\n","epoch 46 batch 7 loss = 2.7318234\n","epoch 46 batch 8 loss = 2.9811313\n","epoch 46 batch 9 loss = 2.81566\n","epoch 46 batch 10 loss = 2.689186\n","epoch 46 batch 11 loss = 2.8295321\n","epoch 46 batch 12 loss = 2.8134987\n","epoch 46 batch 13 loss = 2.8379574\n","epoch 46 batch 14 loss = 2.8698564\n","epoch 47 batch 0 loss = 2.9877038\n","epoch 47 batch 1 loss = 2.90009\n","epoch 47 batch 2 loss = 2.8056767\n","epoch 47 batch 3 loss = 2.8420613\n","epoch 47 batch 4 loss = 2.929764\n","epoch 47 batch 5 loss = 3.0392735\n","epoch 47 batch 6 loss = 2.959204\n","epoch 47 batch 7 loss = 2.8749166\n","epoch 47 batch 8 loss = 2.762829\n","epoch 47 batch 9 loss = 2.623823\n","epoch 47 batch 10 loss = 3.0645952\n","epoch 47 batch 11 loss = 2.972528\n","epoch 47 batch 12 loss = 2.8980753\n","epoch 47 batch 13 loss = 2.8239005\n","epoch 47 batch 14 loss = 2.932396\n","epoch 48 batch 0 loss = 2.635992\n","epoch 48 batch 1 loss = 2.8620415\n","epoch 48 batch 2 loss = 2.9290695\n","epoch 48 batch 3 loss = 3.014749\n","epoch 48 batch 4 loss = 2.912427\n","epoch 48 batch 5 loss = 2.595877\n","epoch 48 batch 6 loss = 2.8576849\n","epoch 48 batch 7 loss = 3.0385594\n","epoch 48 batch 8 loss = 2.9973536\n","epoch 48 batch 9 loss = 2.8564153\n","epoch 48 batch 10 loss = 2.816615\n","epoch 48 batch 11 loss = 2.8658104\n","epoch 48 batch 12 loss = 2.741973\n","epoch 49 batch 0 loss = 2.7911816\n","epoch 49 batch 1 loss = 2.881209\n","epoch 49 batch 2 loss = 2.8289185\n","epoch 49 batch 3 loss = 2.885146\n","epoch 49 batch 4 loss = 2.8105102\n","epoch 49 batch 5 loss = 2.8406126\n","epoch 49 batch 6 loss = 2.9825125\n","epoch 49 batch 7 loss = 3.0111694\n","epoch 49 batch 8 loss = 2.801882\n","epoch 49 batch 9 loss = 2.7909858\n","epoch 49 batch 10 loss = 2.8453705\n","epoch 49 batch 11 loss = 2.9225929\n","epoch 49 batch 12 loss = 2.9265645\n","epoch 49 batch 13 loss = 2.7936718\n","epoch 49 batch 14 loss = 2.8198633\n","epoch 49 batch 15 loss = 2.8276155\n","epoch 49 batch 16 loss = 2.8113456\n","epoch 0 batch 0 loss = 23.60143\n","epoch 0 batch 1 loss = 22.728643\n","epoch 0 batch 2 loss = 21.579376\n","epoch 0 batch 3 loss = 20.562468\n","epoch 1 batch 0 loss = 19.97117\n","epoch 1 batch 1 loss = 18.574478\n","epoch 1 batch 2 loss = 18.079145\n","epoch 1 batch 3 loss = 17.958693\n","epoch 2 batch 0 loss = 16.505247\n","epoch 2 batch 1 loss = 16.615316\n","epoch 2 batch 2 loss = 15.197669\n","epoch 3 batch 0 loss = 14.420469\n","epoch 3 batch 1 loss = 14.203509\n","epoch 3 batch 2 loss = 13.669184\n","epoch 3 batch 3 loss = 12.97383\n","epoch 4 batch 0 loss = 13.2179575\n","epoch 4 batch 1 loss = 12.590339\n","epoch 4 batch 2 loss = 12.735471\n","epoch 5 batch 0 loss = 12.316184\n","epoch 5 batch 1 loss = 12.44145\n","epoch 6 batch 0 loss = 11.854769\n","epoch 6 batch 1 loss = 12.115563\n","epoch 6 batch 2 loss = 12.194637\n","epoch 6 batch 3 loss = 11.796725\n","epoch 7 batch 0 loss = 12.290944\n","epoch 7 batch 1 loss = 12.411961\n","epoch 7 batch 2 loss = 11.753937\n","epoch 7 batch 3 loss = 11.929513\n","epoch 8 batch 0 loss = 12.13982\n","epoch 8 batch 1 loss = 12.283647\n","epoch 8 batch 2 loss = 12.091006\n","epoch 8 batch 3 loss = 11.805727\n","epoch 9 batch 0 loss = 12.103432\n","epoch 9 batch 1 loss = 11.618987\n","epoch 9 batch 2 loss = 12.110283\n","epoch 10 batch 0 loss = 11.439291\n","epoch 10 batch 1 loss = 12.044837\n","epoch 10 batch 2 loss = 12.235989\n","epoch 11 batch 0 loss = 12.168689\n","epoch 11 batch 1 loss = 11.992259\n","epoch 11 batch 2 loss = 12.110074\n","epoch 12 batch 0 loss = 11.937874\n","epoch 12 batch 1 loss = 11.782436\n","epoch 12 batch 2 loss = 11.665087\n","epoch 13 batch 0 loss = 12.124284\n","epoch 13 batch 1 loss = 11.880435\n","epoch 13 batch 2 loss = 12.066713\n","epoch 14 batch 0 loss = 11.6703205\n","epoch 14 batch 1 loss = 11.638924\n","epoch 14 batch 2 loss = 11.541859\n","epoch 15 batch 0 loss = 11.773722\n","epoch 15 batch 1 loss = 12.221272\n","epoch 15 batch 2 loss = 11.708635\n","epoch 16 batch 0 loss = 11.873979\n","epoch 16 batch 1 loss = 12.248344\n","epoch 16 batch 2 loss = 11.601675\n","epoch 17 batch 0 loss = 11.459207\n","epoch 17 batch 1 loss = 12.219509\n","epoch 17 batch 2 loss = 11.45775\n","epoch 18 batch 0 loss = 11.920593\n","epoch 18 batch 1 loss = 11.7222185\n","epoch 18 batch 2 loss = 11.828703\n","epoch 19 batch 0 loss = 11.715341\n","epoch 19 batch 1 loss = 12.3113165\n","epoch 19 batch 2 loss = 11.876375\n","epoch 20 batch 0 loss = 12.017942\n","epoch 20 batch 1 loss = 11.855733\n","epoch 21 batch 0 loss = 11.137863\n","epoch 21 batch 1 loss = 11.416238\n","epoch 21 batch 2 loss = 11.976187\n","epoch 21 batch 3 loss = 12.142209\n","epoch 22 batch 0 loss = 11.750712\n","epoch 22 batch 1 loss = 11.747576\n","epoch 22 batch 2 loss = 12.017864\n","epoch 23 batch 0 loss = 11.772651\n","epoch 23 batch 1 loss = 12.214924\n","epoch 23 batch 2 loss = 11.6969385\n","epoch 24 batch 0 loss = 12.066244\n","epoch 24 batch 1 loss = 11.771834\n","epoch 25 batch 0 loss = 11.813866\n","epoch 25 batch 1 loss = 11.225713\n","epoch 25 batch 2 loss = 11.673479\n","epoch 25 batch 3 loss = 12.239543\n","epoch 26 batch 0 loss = 11.560705\n","epoch 26 batch 1 loss = 11.790616\n","epoch 26 batch 2 loss = 11.975762\n","epoch 27 batch 0 loss = 11.625628\n","epoch 27 batch 1 loss = 11.8279505\n","epoch 27 batch 2 loss = 11.930303\n","epoch 28 batch 0 loss = 12.431307\n","epoch 28 batch 1 loss = 11.26188\n","epoch 28 batch 2 loss = 11.953391\n","epoch 29 batch 0 loss = 11.6526165\n","epoch 29 batch 1 loss = 11.827137\n","epoch 29 batch 2 loss = 11.876331\n","epoch 30 batch 0 loss = 11.966645\n","epoch 30 batch 1 loss = 11.556864\n","epoch 30 batch 2 loss = 11.763627\n","epoch 31 batch 0 loss = 11.89527\n","epoch 31 batch 1 loss = 12.0036545\n","epoch 31 batch 2 loss = 11.994726\n","epoch 31 batch 3 loss = 11.869383\n","epoch 32 batch 0 loss = 12.145847\n","epoch 32 batch 1 loss = 12.05984\n","epoch 32 batch 2 loss = 12.188846\n","epoch 32 batch 3 loss = 11.614841\n","epoch 33 batch 0 loss = 12.014622\n","epoch 33 batch 1 loss = 12.21436\n","epoch 33 batch 2 loss = 11.739876\n","epoch 34 batch 0 loss = 11.891042\n","epoch 34 batch 1 loss = 11.657101\n","epoch 34 batch 2 loss = 11.714617\n","epoch 34 batch 3 loss = 11.881083\n","epoch 35 batch 0 loss = 11.7279005\n","epoch 35 batch 1 loss = 11.878953\n","epoch 35 batch 2 loss = 11.926883\n","epoch 36 batch 0 loss = 11.538246\n","epoch 36 batch 1 loss = 11.913419\n","epoch 36 batch 2 loss = 11.931839\n","epoch 37 batch 0 loss = 12.153091\n","epoch 37 batch 1 loss = 12.015432\n","epoch 37 batch 2 loss = 11.756033\n","epoch 38 batch 0 loss = 11.832364\n","epoch 38 batch 1 loss = 11.835718\n","epoch 38 batch 2 loss = 12.058749\n","epoch 39 batch 0 loss = 11.599351\n","epoch 39 batch 1 loss = 11.831773\n","epoch 39 batch 2 loss = 11.714209\n","epoch 40 batch 0 loss = 11.619703\n","epoch 40 batch 1 loss = 11.963944\n","epoch 40 batch 2 loss = 11.436211\n","epoch 41 batch 0 loss = 11.8359165\n","epoch 41 batch 1 loss = 11.392867\n","epoch 41 batch 2 loss = 11.908736\n","epoch 42 batch 0 loss = 11.288567\n","epoch 42 batch 1 loss = 11.694589\n","epoch 42 batch 2 loss = 11.87776\n","epoch 43 batch 0 loss = 11.721502\n","epoch 43 batch 1 loss = 12.042365\n","epoch 43 batch 2 loss = 11.847223\n","epoch 44 batch 0 loss = 11.847049\n","epoch 44 batch 1 loss = 11.816572\n","epoch 44 batch 2 loss = 12.043173\n","epoch 44 batch 3 loss = 12.247764\n","epoch 45 batch 0 loss = 11.63266\n","epoch 45 batch 1 loss = 12.0566025\n","epoch 45 batch 2 loss = 11.943121\n","epoch 46 batch 0 loss = 11.357178\n","epoch 46 batch 1 loss = 12.309176\n","epoch 46 batch 2 loss = 11.974705\n","epoch 47 batch 0 loss = 12.102938\n","epoch 47 batch 1 loss = 11.988145\n","epoch 47 batch 2 loss = 12.157186\n","epoch 47 batch 3 loss = 11.693677\n","epoch 48 batch 0 loss = 11.582565\n","epoch 48 batch 1 loss = 11.762936\n","epoch 48 batch 2 loss = 11.939315\n","epoch 49 batch 0 loss = 11.663593\n","epoch 49 batch 1 loss = 11.85049\n","epoch 49 batch 2 loss = 11.823437\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JK7ItB9AbddL"},"source":["# #20200702\n","\n","# w2sumdistance_dict = distance_experts(wstrid2wnumid, wnumid2wstrid, worker_acc_dict, t2w, t2wl, w2t, path, deepFD)\n","# t2pwld  = Top1_experts(w2sumdistance_dict, wstrid2wnumid, wnumid2wstrid, t2wl)\n","# # t2lpd = AggMethod(t2pwld,label_set)\n","# # acc, rightlist, wronglist  = getaccuracy(t2truth, t2lpd, t2w)\n","\n","# acc, rightlist, wronglist  = getaccuracy_Top1(t2truth, t2pwld, t2w)\n","\n","# # with open(path+'w2sumdistance_dict.pickle','wb') as file:\n","# #     pickle.dump(w2sumdistance_dict,file)\n","\n","# print(acc)\n","# print(len(wronglist))\n","# print(len(rightlist))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aMYOYQ-ZKflq"},"source":["# headers = ['Method','dataset','epoch_10_acc','Right','Wrong','epoch_20_acc','Right','Wrong','epoch_30_acc','Right','Wrong','epoch_40_acc','Right','Wrong','epoch_50_acc','Right','Wrong','av_acc']\n","# text = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/' + 'test.csv'\n","# with open(text,'w')as f:\n","#   f_csv = csv.writer(f)\n","#   f_csv.writerow(headers)\n","#   f_csv.writerow(headers)\n","#   f_csv.writerow(headers)\n","#   # f_csv.writerows(rows)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VdZFQJeW_E1v"},"source":["seed = 1234\n","# max_vali_f1 = 0 #暂时是没有用到,和分类相关\n","device = torch.device('cpu')\n","\n","\n","# 这几个要调整一下的;\n","b_sz = 10\n","hidden_size = 128\n","emb_size = 4\n","epochs = 10\n","\n","\n","\n","# Hyper parameters\n","alpha = 10\n","beta = 20\n","gamma = 0.01\n","lr = 0.025\n","\n","\n","\n","# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/s5_AdultContent/'\n","# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/s4_Dog data/'\n","# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/s4_Relevance/'\n","# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/s4_Face Sentiment Identification/'\n","# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/CF/'\n","# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/CF_amt/'\n","path ='/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/data_at_scale/active-crowd-toolkit/MS/'\n","datafile = path+'answer_3.csv'\n","\n","\n","#################\n","# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/truth-inference-at-scale-master/data/SpectralMethodsMeetEM/bluebird/'\n","# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/truth-inference-at-scale-master/data/SpectralMethodsMeetEM/web/'\n","# datafile = path+'label.csv'\n","\n","##### 删掉回答不足5个task的worker;\n","# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets/s4_Dog data/'\n","# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets/s4_Relevance/'\n","# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets/s5_AdultContent/'\n","\n","# datafile = path+ 'answer_with_condition.csv'\n","\n","##20200716 hyper question数据集\n","\n","# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/hyper-question-datasets/hyper-question-datasets/Science/'\n","# datafile = path+'new_answer_num.csv'\n","\n","########## pb + ab 模式的input\n","with open(path+'w2task_input.pickle','rb') as file:\n","  w2task_input = pickle.load(file)\n","\n","# with open(path+'w2w_input.pickle','rb') as file:\n","#   w2w_input = pickle.load(file) \n","#######################################################################\n","\n","# ############# LLA 模式的input\n","# with open(path+'w2input_LLA_dict.pickle','rb') as file:\n","#   w2input_LLA_dict = pickle.load(file)\n","# with open(path+'w2input_mask_dict.pickle','rb') as file:\n","#   w2input_mask_dict = pickle.load(file)\n","#############################################################################\n","\n","\n","with open(path+'wnumid2wstrid.pickle','rb') as file:\n","  wnumid2wstrid = pickle.load(file)\n","\n","with open(path+'wstrid2wnumid.pickle','rb') as file:\n","  wstrid2wnumid = pickle.load(file)\n","\n","with open(path+'t2w.pickle','rb') as file:\n","  t2w = pickle.load(file)\n","with open(path+'t2wl.pickle','rb') as file:\n","  t2wl = pickle.load(file)\n","\n","\n","with open(path+'t2truth.pickle','rb') as file:\n","  t2truth = pickle.load(file)\n","\n","\n","with open(path+'w2t.pickle','rb') as file:\n","  w2t = pickle.load(file)\n","\n","with open(path+'w2tl.pickle','rb') as file:\n","  w2tl = pickle.load(file)\n","\n","with open(path+'worker_acc_dict.pickle','rb') as file:\n","  worker_acc_dict = pickle.load(file)\n","\n","\n","\n","with open(path+'w2w.pickle','rb') as file:\n","  w2w = pickle.load(file) \n","\n","with open(path+'training_cps.pickle','rb') as file:\n","  training_cps = pickle.load(file) \n","\n","\n","with open(path+'label_set.pickle','rb') as file:\n","  label_set = pickle.load(file)\n","\n","# 20200622 worker 之间 的 similarity 和loss 的计算有关系\n","# \n","csv_data = pd.read_csv(path+'worker_simlarity.csv',low_memory = False,index_col=0)\n","worker_similarity_df = pd.DataFrame(csv_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AH-LxmROvbDm"},"source":["# 20200721 wpb input\n","# 把数据处理成tensor\n","\n","pb_feat_size = len(t2w)\n","w_pb_input = input2numpy(w2task_input,wstrid2wnumid,wnumid2wstrid,pb_feat_size,path,'w_pb_input.npy')\n","w_pb_input_tensor = torch.FloatTensor(w_pb_input).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-MozV6YOf963"},"source":["# # lla_input\n","# # 把数据处理成tensor\n","\n","# # 20200622 处理成tensor 是要注意维度 LLA 的维度是 task * choice\n","# pb_feat_size = len(t2w)*len(label_set)\n","# w_pb_input = input2numpy(w2input_LLA_dict,wstrid2wnumid,wnumid2wstrid,pb_feat_size,path,'w_pb_input.npy')\n","# w_pb_input_tensor = torch.FloatTensor(w_pb_input).to(device)\n","\n","\n","\n","# #下面的处理w_ab_input的函数要重新写,把输入换成w2wsim\n","# ab_feat_size = len(w2w_input)\n","# w_ab_input = w2wsim2numpy(w2wsim,wstrid2wnumid,wnumid2wstrid,ab_feat_size,path,'w_ab_input.npy')\n","# #features = torch.FloatTensor(w_pb_input).to(device)\n","# w_ab_input_tensor = torch.FloatTensor(w_ab_input).to(device)\n","\n","# w_pb_input = input2numpy(w2task_input,wstrid2wnumid,wnumid2wstrid,pb_feat_size,path,'w_pb_input.npy')\n","# pb_feat_size = len(t2w)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GIxFW3Bze3D1"},"source":["#### train model\n"]},{"cell_type":"code","metadata":{"id":"ZT_2Vm8T295u","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1595654996542,"user_tz":-600,"elapsed":693,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}},"outputId":"13f88ae6-7c5e-4a1e-87ba-7697d79261f7"},"source":["deepFD = DeepFD(w_pb_input_tensor, w_pb_input_tensor.size(1), emb_size)\n","\n","\n","deepFD.to(device)\n","#定义了loss\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DeepFD(\n","  (fc1): Linear(in_features=700, out_features=64, bias=True)\n","  (fc2): Linear(in_features=64, out_features=4, bias=True)\n","  (fc3): Linear(in_features=4, out_features=64, bias=True)\n","  (fc4): Linear(in_features=64, out_features=700, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"0B_wRKqK_-di"},"source":["model_loss = Loss_DeepFD(w_pb_input_tensor, worker_similarity_df, wnumid2wstrid, training_cps, device, alpha, beta, gamma)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eidpaLx9z0hY","colab":{"base_uri":"https://localhost:8080/","height":196},"executionInfo":{"status":"ok","timestamp":1595655007003,"user_tz":-600,"elapsed":993,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}},"outputId":"da9728d6-f756-4150-86a2-8a678e6d981b"},"source":["#model_loss = Loss_DeepFD(features, getattr(Dl, Dl.ds+'_simi'), args.device, args.alpha, args.beta, args.gamma)\n","\n","\n","\n","for epoch in range(epochs):\n","    #logger.info(f'----------------------EPOCH {epoch}-----------------------')\n","    \n","    deepFD = train_model(wnumid2wstrid, training_cps, deepFD, model_loss, device, epoch)\n","    #deepSW = train_model_DeepSW(wnumid2wstrid, deepSW, model_loss, device, epoch)\n","  "],"execution_count":null,"outputs":[{"output_type":"stream","text":["epoch 0 batch 0 loss = 19.600048\n","epoch 1 batch 0 loss = 18.787388\n","epoch 2 batch 0 loss = 18.038158\n","epoch 3 batch 0 loss = 17.381718\n","epoch 4 batch 0 loss = 16.841234\n","epoch 5 batch 0 loss = 16.399601\n","epoch 6 batch 0 loss = 16.011473\n","epoch 7 batch 0 loss = 15.685134\n","epoch 8 batch 0 loss = 15.418282\n","epoch 9 batch 0 loss = 15.346845\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zLehmDu87Plw"},"source":["### Top1 方法"]},{"cell_type":"code","metadata":{"id":"r0ApKAORUhFs"},"source":["#20200702\n","\n","w2sumdistance_dict = distance_experts(wstrid2wnumid, wnumid2wstrid, worker_acc_dict, t2w, t2wl, w2t, path, deepFD)\n","t2pwld  = Top1_experts(w2sumdistance_dict, wstrid2wnumid, wnumid2wstrid, t2wl)\n","# t2lpd = AggMethod(t2pwld,label_set)\n","# acc, rightlist, wronglist  = getaccuracy(t2truth, t2lpd, t2w)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3QRXgiFkdz5P","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595655483274,"user_tz":-600,"elapsed":674,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}},"outputId":"a8d31c2e-a6a2-4c3e-f15e-7991fb8c9f58"},"source":["t2pwld['109'][0][1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'7'"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"M3Tf1nHEdtxn","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1595655555657,"user_tz":-600,"elapsed":726,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}},"outputId":"473c75dc-c6d7-499c-ccff-be45bc9455b3"},"source":["acc, rightlist, wronglist  = getaccuracy_Top1(t2truth, t2pwld, t2w)\n","\n","# with open(path+'w2sumdistance_dict.pickle','wb') as file:\n","#     pickle.dump(w2sumdistance_dict,file)\n","\n","print(acc)\n","print(len(wronglist))\n","print(len(rightlist))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.51\n","343\n","357\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vlAHjcs_8El_","colab":{"base_uri":"https://localhost:8080/","height":107},"executionInfo":{"status":"ok","timestamp":1594874856752,"user_tz":-600,"elapsed":115514,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}},"outputId":"50293ca2-0048-409d-d672-60653f2cb6af"},"source":["\n","# 202007016 距离top3\n","w2sumdistance_dict = distance_experts(wstrid2wnumid, wnumid2wstrid, worker_acc_dict, t2w, t2wl, w2t, path, deepFD)\n","t2pwld  = Top1_experts(w2sumdistance_dict, wstrid2wnumid, wnumid2wstrid, t2wl)\n","t2lpd = AggMethod(t2pwld,label_set)\n","acc, rightlist, wronglist  = getaccuracy(t2truth, t2lpd, t2w)\n","\n","# acc, rightlist, wronglist  = getaccuracy_Top1(t2truth, t2pwld, t2w)\n","\n","# with open(path+'w2sumdistance_dict.pickle','wb') as file:\n","#     pickle.dump(w2sumdistance_dict,file)\n","\n","print(acc)\n","print(len(wronglist))\n","print(len(rightlist))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0.545067264573991\n","2029\n","2431\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7trRI6zo1ISi"},"source":["# 把wnum_id 转成str_id\n","def wstr_2_distance(w2sumdistance_dict,wstrid2wnumid):\n","\n","  # key : wstr_id, value: distance\n","  wstr2distance = {}\n","  for wnumid in w2sumdistance_dict:\n","    wstrid = wnumid2wstrid[wnumid]\n","    wstr2distance[wstrid] = w2sumdistance_dict[wnumid]\n","\n","  return wstr2distance\n","\n","wstr2distance = wstr_2_distance(w2sumdistance_dict,wstrid2wnumid)\n","with open(path+'wstr2distance.pickle','wb') as file:\n","    pickle.dump(wstr2distance,file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qOEG-PVA6Fe7"},"source":["### S3 方法\n","20200709"]},{"cell_type":"code","metadata":{"id":"3CiXxIPP6En6"},"source":["#20200702\n","# w2sumdistance_dict = distance_experts(wstrid2wnumid, wnumid2wstrid, worker_acc_dict, t2w, t2wl, w2t, path, deepFD)\n","t2s3wl  = S3_experts(wstrid2wnumid, wnumid2wstrid, t2w, t2wl, w2t, path, deepFD)\n","t2lpd = AggMethod(t2s3wl,label_set)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sj0HU9Ow-96f","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1594861576757,"user_tz":-600,"elapsed":718,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}},"outputId":"ffbce819-8a41-46f0-afa5-b3b4d34fee3b"},"source":["acc, rightlist, wronglist  = getaccuracy(t2truth, t2lpd, t2w)\n","\n","# with open(path+'w2sumdistance_dict.pickle','wb') as file:\n","#     pickle.dump(w2sumdistance_dict,file)\n","\n","print(acc)\n","print(len(wronglist))\n","print(len(rightlist))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.3684210526315789\n","12\n","7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tczgr8Dfh3Mq"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-CyewTh2h3kH"},"source":["## weighted_mv\n","20200720 1_weight = sum(w_ti);"]},{"cell_type":"markdown","metadata":{"id":"Wexe7oP4h7Vk"},"source":["### main 函数"]},{"cell_type":"code","metadata":{"id":"aioLVRzEh6OP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2cf7NvCzh3kQ","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1595653870142,"user_tz":-600,"elapsed":2487,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}},"outputId":"07dd449d-44cd-4cf0-f43e-eda91ee019f8"},"source":["# 20200720 加入了所有的worker answer ,看一下结果;\n","w2sumdistance_dict = distance_experts(wstrid2wnumid, wnumid2wstrid, worker_acc_dict, t2w, t2wl, w2t, path, deepFD)\n","t2pwld  = Sort_experts(w2sumdistance_dict, wstrid2wnumid, wnumid2wstrid, t2wl)\n","t2lpd = W_MV_AggMethod(t2pwld,label_set)\n","acc, rightlist, wronglist  = W_MV_getaccuracy(t2truth, t2lpd, t2w)\n","\n","# acc, rightlist, wronglist  = getaccuracy_Top1(t2truth, t2pwld, t2w)\n","\n","# with open(path+'w2sumdistance_dict.pickle','wb') as file:\n","#     pickle.dump(w2sumdistance_dict,file)\n","\n","print(acc)\n","print(len(wronglist))\n","print(len(rightlist))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.6966666666666667\n","91\n","209\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dNbUeyKImz0z"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Tq4LCCfm0Yf"},"source":["## weighted_mv 分两档\n","20200721 1_weight = sum(w_ti) 分两档或者3挡吧;"]},{"cell_type":"code","metadata":{"id":"RuJ0VXhVm0Yg"},"source":["# 20200702 选出top1的worker 作为结果;\n","def take3rd(elem):\n","    return elem[2]\n","# 20200720 对worker 按照distance 排序了 worker 作为结果;\n","def Sort_experts(w2sumdistance_dict,wstrid2wnumid, wnumid2wstrid, t2wl):\n","\n","  #t2wl = self.t2wl\n","  # worker_list = []\n","  # for i in idnumpy:\n","  #   strid = wnumid2wstrid[i]\n","  #   worker_list.append(strid)\n","  \n","  # key : task_id, value: wnumid,label,distance\n","  t2pwld = {}\n","  for t in t2wl:\n","    wl_list = t2wl[t]\n","    w1d_list = []\n","    predict = 0\n","    \n","    for wl_tuple in wl_list:\n","      worker = wl_tuple[0]\n","      label = wl_tuple[1]\n","      numid = wstrid2wnumid[worker]\n","      w_dis = w2sumdistance_dict[numid]\n","      w1d_list.append([worker,label,w_dis])\n","    # 根据 sum_distance  升序排列;\n","    w1d_list.sort(key=take3rd,reverse=False)\n","    # 取排在第一位的 wld 信息\n","    # 20200716 取几个直接进行切片就行了, 不需要进行切片了;\n","    t2pwld[t] = w1d_list\n","\n","  return t2pwld"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"47BTBAZTm0Yj"},"source":["# def my_sum(i):\n","#   if i < 0:\n","#       raise ValueError\n","#   elif i <= 1:\n","#       return i\n","#   else:\n","#       return i + my_sum(i-1)\n","\n","# 20200612 为甚要用类写, 不这么写了;\n","\n","def AggMethod(t2wl,label_set):\n","\n","  #t2wl = self.t2wl\n","  t2lpd={}\n","  for t in t2wl:\n","    t2lpd[t]={}\n","\n","    # multi label 初始化;\n","    for label in label_set:\n","      t2lpd[t][label] = 0\n","    # e2lpd[e]['0']=0\n","    # e2lpd[e]['1']=0\n","    \n","    # alls = len(t2wl[t])\n","    # for item in t2wl[t]:\n","    #   label=item[1]\n","    #   t2lpd[t][label]+= 1\n","\n","    #20200720 加上一个weight;\n","    wld_list = t2wl[t]\n","    alls = len(wld_list)\n","    # weight_sum = my_sum(alls)\n","    # 分两档的写法;20200721\n","    weight_sum = alls+ 0.5*alls\n","\n","    # 分两档的写法;\n","    for i in range(alls):\n","      if i < 0.5*alls:\n","        label = wld_list[i][1]\n","        weight = 1.0*2\n","        t2lpd[t][label] += weight\n","      else:\n","        label = wld_list[i][1]\n","        weight = 1.0*1\n","        t2lpd[t][label] += weight\n","\n","\n","    # for item in t2wl[t]:\n","    #   label=item[1]\n","    #   t2lpd[t][label]+= 1\n","    \n","    if alls!=0:\n","        # e2lpd[e]['0']=1.0*e2lpd[e]['0']/alls\n","        # e2lpd[e]['1']=1.0*e2lpd[e]['1']/alls\n","      for label in label_set:\n","          #20200720 weight_sum\n","          t2lpd[t][label] = 1.0 * t2lpd[t][label] / weight_sum\n","    else:\n","        # e2lpd[e]['0']=0.5\n","        # e2lpd[e]['1']=0.5\n","      for label in label_set:\n","          t2lpd[t][label] = 1.0 / len(label_set)\n","\n","  # return self.expand(e2lpd)\n","  return t2lpd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fQE2x3Pjm0Yl"},"source":["\n","##20200702\n","\n","def getaccuracy(e2truth, e2lpd, t2w):\n","  # 需要确保e2lpd 已经是一个完整结果了; \n","  #label 应该是字符串\n","  count = 0\n","  right_taskid_list = []\n","  wrong_taskid_list = []\n","  # truth = ''\n","  \n","  for e in t2w:\n","\n","    ## 20200627 加上这一句就可以用全部数据集了;\n","    if e not in e2truth:\n","      continue\n","    # print(e)\n","    temp = 0\n","    predict_answer = 0\n","    #找到worker的mv-answer e2lpd 就是各种方法的结果; temp 存了一个值\n","    for label in e2lpd[e]:\n","      if temp < e2lpd[e][label]:\n","        temp = e2lpd[e][label]\n","        #print(temp)\n","      \n","    count += 1 \n","    for label in e2lpd[e]:\n","      if temp == e2lpd[e][label]:\n","        predict_answer = label\n","      # print('temp',temp)\n","      # print('predict',predict_answer)\n","    \n","    if predict_answer == e2truth[e]:\n","      \n","      right_taskid_list.append(e)\n","    else:\n","      wrong_taskid_list.append(e)  \n","\n","    # 准确率, 回答正确的list, 回答错误的list\n","  return len(right_taskid_list)/count, right_taskid_list, wrong_taskid_list"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bRyemKAQm0Yn"},"source":["### main 函数"]},{"cell_type":"code","metadata":{"id":"29vuQtPwm0Yo","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1595409534340,"user_tz":-600,"elapsed":2262,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}},"outputId":"e44c1d2d-9791-4062-897b-bfec71471f4c"},"source":["# 20200720 加入了所有的worker answer ,看一下结果;\n","w2sumdistance_dict = distance_experts(wstrid2wnumid, wnumid2wstrid, worker_acc_dict, t2w, t2wl, w2t, path, deepFD)\n","t2pwld  = Sort_experts(w2sumdistance_dict, wstrid2wnumid, wnumid2wstrid, t2wl)\n","t2lpd = AggMethod(t2pwld,label_set)\n","acc, rightlist, wronglist  = getaccuracy(t2truth, t2lpd, t2w)\n","\n","# acc, rightlist, wronglist  = getaccuracy_Top1(t2truth, t2pwld, t2w)\n","\n","# with open(path+'w2sumdistance_dict.pickle','wb') as file:\n","#     pickle.dump(w2sumdistance_dict,file)\n","\n","print(acc)\n","print(len(wronglist))\n","print(len(rightlist))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.7496902106567535\n","202\n","605\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C7B6r0h8v_s3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-oMkDS64L1s3"},"source":["## DS_distance\n","20200721  \n","Class 原版\n"]},{"cell_type":"code","metadata":{"id":"i3kkFYLoL1ED"},"source":["import math\n","import csv\n","import random\n","import sys\n","\n","class EM:\n","    def __init__(self,e2wl,w2el,label_set, initquality):\n","        self.e2wl = e2wl\n","        self.w2el = w2el\n","        self.workers = self.w2el.keys()\n","        self.label_set = label_set\n","        self.initalquality = initquality\n","             \n","\n","\n","# E-step\n","    # 这个可以直接把distance当做 weight,计算e2lpd\n","    def Update_e2lpd(self):\n","        self.e2lpd = {}\n","\n","        for example, worker_label_set in e2wl.items():\n","            lpd = {}\n","            total_weight = 0\n","\n","            for tlabel, prob in self.l2pd.items():\n","                weight = prob\n","                for (w, label) in worker_label_set:\n","                    weight *= self.w2cm[w][tlabel][label]\n","                \n","                lpd[tlabel] = weight\n","                total_weight += weight\n","                \n","\n","            for tlabel in lpd:\n","                if total_weight == 0:\n","                    # uniform distribution \n","                    lpd[tlabel] = 1.0/len(self.label_set)\n","                else:\n","                    lpd[tlabel] = lpd[tlabel]*1.0/total_weight\n","            \n","            self.e2lpd[example] = lpd\n","\n","\n","\n","#M-step\n","\n","    def Update_l2pd(self):\n","        # 20200719 等于 0 是为什么?\n","        for label in self.l2pd:\n","            self.l2pd[label] = 0\n","        \n","        # 20200719 用e2lpd + 0 得到一个新的值\n","        for _, lpd in self.e2lpd.items():\n","            for label in lpd:\n","                self.l2pd[label] += lpd[label]\n","        # 20200719 这一步在干什么?\n","        for label in self.l2pd:\n","            self.l2pd[label] *= 1.0/len(self.e2lpd)\n","\n","\n","            \n","    def Update_w2cm(self):\n","\n","        # 20200719 update之前先定成0;\n","        for w in self.workers:\n","            for tlabel in self.label_set:\n","                for label in self.label_set:\n","                    self.w2cm[w][tlabel][label] = 0\n","\n","\n","        w2lweights = {}\n","        for w in self.w2el:\n","            w2lweights[w] = {}\n","            for label in self.label_set:\n","                w2lweights[w][label] = 0\n","            for example, _ in self.w2el[w]:\n","                for label in self.label_set:\n","                    w2lweights[w][label] += self.e2lpd[example][label]\n","\n","            \n","            for tlabel in self.label_set:\n","\n","                if w2lweights[w][tlabel] == 0:\n","                    for label in self.label_set:\n","                        if tlabel == label:\n","                            self.w2cm[w][tlabel][label] = self.initalquality\n","                        else:\n","                            self.w2cm[w][tlabel][label] = (1-self.initalquality)*1.0/(len(self.label_set)-1)\n","\n","                    continue\n","\n","                for example, label in self.w2el[w]:\n","                        #e2lpd[e][t] 就好比是QASCA里面 Qc\n","                        self.w2cm[w][tlabel][label] += self.e2lpd[example][tlabel]*1.0/w2lweights[w][tlabel]\n","\n","\n","\n","        return self.w2cm\n","                    \n","\n","    \n","    \n","\n","                     \n","\n","#initialization\n","    def Init_l2pd(self):\n","        #uniform probability distribution \n","        # 最一般的情况, 所有类别初始概率都是一样的;\n","        l2pd = {}\n","        for label in self.label_set:\n","            l2pd[label] = 1.0/len(self.label_set)\n","        return l2pd\n","\n","    def Init_w2cm(self):\n","        w2cm = {}\n","        for worker in self.workers:\n","            w2cm[worker] = {}\n","            for tlabel in self.label_set:\n","                w2cm[worker][tlabel] = {}\n","                for label in self.label_set:\n","                    if tlabel == label:\n","                        w2cm[worker][tlabel][label] = self.initalquality\n","                    else:\n","                        w2cm[worker][tlabel][label] = (1-self.initalquality)/(len(label_set)-1)\n","\n","        return w2cm\n","\n","    def Run(self, iterr = 20):\n","        \n","        self.l2pd = self.Init_l2pd()\n","        self.w2cm = self.Init_w2cm()\n","\n","        while iterr > 0:\n","            # E-step\n","            self.Update_e2lpd() \n","\n","            # M-step\n","            self.Update_l2pd()\n","            self.Update_w2cm()\n","\n","            # compute the likelihood\n","            # print self.computelikelihood()\n","\n","            iterr -= 1\n","        \n","        return self.e2lpd, self.w2cm\n","\n","\n","    def computelikelihood(self):\n","        \n","        lh = 0\n","\n","        for _, worker_label_set in self.e2wl.items():\n","            temp = 0\n","            for tlabel, prior in self.l2pd.items():\n","                inner = prior\n","                for worker, label in worker_label_set:\n","                    inner *= self.w2cm[worker][tlabel][label]\n","                temp += inner\n","            \n","            lh += math.log(temp)\n","        \n","        return lh\n","\n","\n","###################################\n","# The above is the EM method (a class)\n","# The following are several external functions \n","###################################\n","\n","def getaccuracy(truthfile, e2lpd, label_set):\n","  e2truth = {}\n","  f = open(truthfile, 'r')\n","  reader = csv.reader(f)\n","  next(reader)\n","\n","  right_taskid_list = []\n","  wrong_taskid_list = []\n","\n","  for line in reader:\n","      example, truth = line\n","      e2truth[example] = truth\n","\n","  tcount = 0\n","  count = 0\n","\n","  for e in e2lpd:\n","\n","    if e not in e2truth:\n","      # print(e)\n","      continue\n","\n","    temp = 0\n","    for label in e2lpd[e]:\n","        if temp < e2lpd[e][label]:\n","            temp = e2lpd[e][label]\n","    \n","    candidate = []\n","\n","    for label in e2lpd[e]:\n","        if temp == e2lpd[e][label]:\n","            candidate.append(label)\n","    \n","    truth = random.choice(candidate)\n","\n","    count += 1\n","      \n","    ###### 20200721\n","    if truth == e2truth[e]:\n","      tcount += 1\n","      right_taskid_list.append(e)\n","    else:\n","      wrong_taskid_list.append(e) \n","\n","\n","  return tcount*1.0/count, right_taskid_list, wrong_taskid_list\n","\n","\n","def gete2wlandw2el(datafile):\n","    e2wl = {}\n","    w2el = {}\n","    label_set=[]\n","    \n","    f = open(datafile, 'r')\n","    reader = csv.reader(f)\n","    next(reader)\n","\n","    for line in reader:\n","        example, worker, label = line\n","        if example not in e2wl:\n","            e2wl[example] = []\n","        e2wl[example].append([worker,label])\n","\n","        if worker not in w2el:\n","            w2el[worker] = []\n","        w2el[worker].append([example,label])\n","\n","        if label not in label_set:\n","            label_set.append(label)\n","\n","    return e2wl,w2el,label_set\n","    \n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nqv1MBF-1s-C"},"source":["import math\n","import csv\n","import random\n","import sys\n","# 20200721 \n","class EM:\n","    def __init__(self,e2wl,w2el,label_set, initquality):\n","        self.e2wl = e2wl\n","        self.w2el = w2el\n","        self.workers = self.w2el.keys()\n","        self.label_set = label_set\n","        self.initalquality = initquality\n","             \n","\n","\n","# E-step\n","    # 这个可以直接把distance当做 weight,计算e2lpd\n","    def Update_e2lpd(self):\n","        self.e2lpd = {}\n","\n","        for example, worker_label_set in e2wl.items():\n","            lpd = {}\n","            total_weight = 0\n","\n","            for tlabel, prob in self.l2pd.items():\n","                weight = prob\n","                for (w, label) in worker_label_set:\n","                    weight *= self.w2cm[w][tlabel][label]\n","                \n","                lpd[tlabel] = weight\n","                total_weight += weight\n","                \n","\n","            for tlabel in lpd:\n","                if total_weight == 0:\n","                    # uniform distribution \n","                    lpd[tlabel] = 1.0/len(self.label_set)\n","                else:\n","                    lpd[tlabel] = lpd[tlabel]*1.0/total_weight\n","            \n","            self.e2lpd[example] = lpd\n","\n","\n","\n","#M-step\n","\n","    def Update_l2pd(self):\n","        # 20200719 等于 0 是为什么?\n","        for label in self.l2pd:\n","            self.l2pd[label] = 0\n","        \n","        # 20200719 用e2lpd + 0 得到一个新的值\n","        for _, lpd in self.e2lpd.items():\n","            for label in lpd:\n","                self.l2pd[label] += lpd[label]\n","        # 20200719 这一步在干什么?\n","        for label in self.l2pd:\n","            self.l2pd[label] *= 1.0/len(self.e2lpd)\n","\n","\n","            \n","    def Update_w2cm(self):\n","\n","        # 20200719 update之前先定成0;\n","        for w in self.workers:\n","            for tlabel in self.label_set:\n","                for label in self.label_set:\n","                    self.w2cm[w][tlabel][label] = 0\n","\n","\n","        w2lweights = {}\n","        for w in self.w2el:\n","            w2lweights[w] = {}\n","            for label in self.label_set:\n","                w2lweights[w][label] = 0\n","            for example, _ in self.w2el[w]:\n","                for label in self.label_set:\n","                    w2lweights[w][label] += self.e2lpd[example][label]\n","\n","            \n","            for tlabel in self.label_set:\n","\n","                if w2lweights[w][tlabel] == 0:\n","                    for label in self.label_set:\n","                        if tlabel == label:\n","                            self.w2cm[w][tlabel][label] = self.initalquality\n","                        else:\n","                            self.w2cm[w][tlabel][label] = (1-self.initalquality)*1.0/(len(self.label_set)-1)\n","\n","                    continue\n","\n","                for example, label in self.w2el[w]:\n","                        #e2lpd[e][t] 就好比是QASCA里面 Qc\n","                        self.w2cm[w][tlabel][label] += self.e2lpd[example][tlabel]*1.0/w2lweights[w][tlabel]\n","\n","\n","\n","        return self.w2cm\n","                    \n","\n","    \n","    \n","\n","                     \n","\n","#initialization\n","    def Init_l2pd(self):\n","        #uniform probability distribution \n","        # 最一般的情况, 所有类别初始概率都是一样的;\n","        l2pd = {}\n","        for label in self.label_set:\n","            l2pd[label] = 1.0/len(self.label_set)\n","        return l2pd\n","\n","    def Init_w2cm(self):\n","        w2cm = {}\n","        for worker in self.workers:\n","            w2cm[worker] = {}\n","            for tlabel in self.label_set:\n","                w2cm[worker][tlabel] = {}\n","                for label in self.label_set:\n","                    if tlabel == label:\n","                        w2cm[worker][tlabel][label] = self.initalquality\n","                    else:\n","                        w2cm[worker][tlabel][label] = (1-self.initalquality)/(len(label_set)-1)\n","\n","        return w2cm\n","\n","    def Run(self, iterr = 20):\n","        \n","        self.l2pd = self.Init_l2pd()\n","        self.w2cm = self.Init_w2cm()\n","\n","        while iterr > 0:\n","            # E-step\n","            self.Update_e2lpd() \n","\n","            # M-step\n","            self.Update_l2pd()\n","            self.Update_w2cm()\n","\n","            # compute the likelihood\n","            # print self.computelikelihood()\n","\n","            iterr -= 1\n","        \n","        return self.e2lpd, self.w2cm\n","\n","\n","    def computelikelihood(self):\n","        \n","        lh = 0\n","\n","        for _, worker_label_set in self.e2wl.items():\n","            temp = 0\n","            for tlabel, prior in self.l2pd.items():\n","                inner = prior\n","                for worker, label in worker_label_set:\n","                    inner *= self.w2cm[worker][tlabel][label]\n","                temp += inner\n","            \n","            lh += math.log(temp)\n","        \n","        return lh\n","\n","\n","###################################\n","# The above is the EM method (a class)\n","# The following are several external functions \n","###################################\n","\n","def getaccuracy(truthfile, e2lpd, label_set):\n","  e2truth = {}\n","  f = open(truthfile, 'r')\n","  reader = csv.reader(f)\n","  next(reader)\n","\n","  right_taskid_list = []\n","  wrong_taskid_list = []\n","\n","  for line in reader:\n","      example, truth = line\n","      e2truth[example] = truth\n","\n","  tcount = 0\n","  count = 0\n","\n","  for e in e2lpd:\n","\n","    if e not in e2truth:\n","      # print(e)\n","      continue\n","\n","    temp = 0\n","    for label in e2lpd[e]:\n","        if temp < e2lpd[e][label]:\n","            temp = e2lpd[e][label]\n","    \n","    candidate = []\n","\n","    for label in e2lpd[e]:\n","        if temp == e2lpd[e][label]:\n","            candidate.append(label)\n","    \n","    truth = random.choice(candidate)\n","\n","    count += 1\n","      \n","    ###### 20200721\n","    if truth == e2truth[e]:\n","      tcount += 1\n","      right_taskid_list.append(e)\n","    else:\n","      wrong_taskid_list.append(e) \n","\n","\n","  return tcount*1.0/count, right_taskid_list, wrong_taskid_list\n","\n","\n","def gete2wlandw2el(datafile):\n","    e2wl = {}\n","    w2el = {}\n","    label_set=[]\n","    \n","    f = open(datafile, 'r')\n","    reader = csv.reader(f)\n","    next(reader)\n","\n","    for line in reader:\n","        example, worker, label = line\n","        if example not in e2wl:\n","            e2wl[example] = []\n","        e2wl[example].append([worker,label])\n","\n","        if worker not in w2el:\n","            w2el[worker] = []\n","        w2el[worker].append([example,label])\n","\n","        if label not in label_set:\n","            label_set.append(label)\n","\n","    return e2wl,w2el,label_set"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eVzROeKR1tmQ"},"source":["### main 函数"]},{"cell_type":"code","metadata":{"id":"0Dd2jI6pyUx0","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595310129010,"user_tz":-600,"elapsed":9058,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}},"outputId":"d690142b-92fd-4bba-f227-0e5e0ca5fa96"},"source":["# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/s5_AdultContent/'\n","# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/s4_Dog data/'\n","path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/s4_Relevance/'\n","# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/s4_Face Sentiment Identification/'\n","\n","datafile = path+'answer.csv'\n","\n","e2wl,w2el,label_set = gete2wlandw2el(datafile) # generate structures to pass into EM\n","iterations = 20 # EM iteration number\n","initquality = 0.7\n","e2lpd, w2cm = EM(e2wl,w2el,label_set,initquality).Run(iterations)\n","\n","# datasetname = path[78:]\n","# print(len(e2lpd))\n","# print(datasetname)\n","# print(len(w2cm))\n","# print(len(e2lpd))\n","\n","truthfile = path + 'truth.csv'\n","accuracy, rightlist_ds, wronglist_ds = getaccuracy(truthfile, e2lpd, label_set)\n","print(accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.6130044843049327\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SG_1hvK_3amV","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595309707607,"user_tz":-600,"elapsed":710,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}},"outputId":"cb93d00a-54cb-4a06-e1c2-eba85a8ec66c"},"source":["print(accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.6130044843049327\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9oBCCT9s3lnr","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595309719704,"user_tz":-600,"elapsed":669,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}},"outputId":"e5f31c12-1a48-47b8-de61-972bab7ca0cf"},"source":["len(rightlist_ds)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2734"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"code","metadata":{"id":"ll8DuSFQ2oH0"},"source":["# co_task = set(task_list_w1).union(set(task_list_w2))\n","#     #回答一致的次数\n","co_list = set(rightlist_ds).intersection(set(rightlist))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1atXFdlZ31jA"},"source":["rightlist_ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_7Rkq8wg3uhc","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1595310190314,"user_tz":-600,"elapsed":717,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}},"outputId":"02201e80-d3cc-49ab-a265-c1482ccfa7f6"},"source":["print(len(rightlist_ds))\n","print(len(rightlist))\n","print(len(co_list))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2734\n","2417\n","2035\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OSFd1wvO2Y8R"},"source":["for w_2 in w2t: \n","    task_list_w2 = w2t[w_2]\n","    task_answer_list_w2 = w2tl[w_2]\n","    co_task = set(task_list_w1).union(set(task_list_w2))\n","    #回答一致的次数\n","    co_answer = set(task_answer_list_w1).intersection(set(task_answer_list_w2))\n","    \n","    if len(co_answer) == 0:\n","      sim = 1/(task_num+len(co_task))\n","    elif len(co_answer) == len(co_task):\n","      sim = (len(co_answer)+task_num-1)/(len(co_task)+task_num)\n","    else:\n","      sim = (len(co_answer))/(len(co_task))\n","    wid=wstrid2wnumid[w_2]\n","    # worker_similarity_df.loc[w1,w2] = sim\n","    w2wsim_vector[wid] = sim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IcnQEKJqLsv2","colab":{"base_uri":"https://localhost:8080/","height":138},"executionInfo":{"status":"error","timestamp":1594697871512,"user_tz":-600,"elapsed":622,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}},"outputId":"700af946-3779-47bb-9074-61d0b47a58df"},"source":["# # path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/s4_Dog data/'\n","# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/s4_Relevance/'\n","# # path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/s5_AdultContent/'\n","# # path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/s4_Face Sentiment Identification/'\n","# datafile = path+'answer.csv'\n","###############\n","#blue birds 数据集\n","# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/truth-inference-at-scale-master/data/SpectralMethodsMeetEM/bluebird/'\n","# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/truth-inference-at-scale-master/data/SpectralMethodsMeetEM/web/'\n","#####\n","\n","# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets/s4_Dog data/'\n","# path = '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets/s4_Relevance/'\n","\n","# pathlist = ['/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets/s4_Relevance/',\n","#         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets/s4_Dog data/',\n","#         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets/s4_Face Sentiment Identification/',\n","#         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets/s5_AdultContent/'] \n","\n","# pathlist =['/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/hyper-question-datasets/hyper-question-datasets/Chinese/',\n","#         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/hyper-question-datasets/hyper-question-datasets/English/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/hyper-question-datasets/hyper-question-datasets/IT/',\n","#           '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/hyper-question-datasets/hyper-question-datasets/Medicine/',\n","#            '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/hyper-question-datasets/hyper-question-datasets/Pokemon/',\n","#            '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/hyper-question-datasets/hyper-question-datasets/Science/'\n","#  ]\n","for path in pathlist:\n","  # datafile = path+'label.csv'\n","  datafile = path+ 'answer_with_condition.csv'\n","  # datafile = sys.argv[1]\n","  e2wl,w2el,label_set = gete2wlandw2el(datafile) # generate structures to pass into EM\n","  iterations = 20 # EM iteration number\n","  initquality = 0.7\n","  e2lpd, w2cm = EM(e2wl,w2el,label_set,initquality).Run(iterations)\n","  datasetname = path[78:]\n","  print(len(e2lpd))\n","  print(datasetname)\n","  print(len(w2cm))\n","  print(len(e2lpd))\n","\n","  truthfile = path + 'truth.csv'\n","  accuracy = getaccuracy(truthfile, e2lpd, label_set)\n","  print(accuracy)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-ebeef8485c60>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    ]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"]}]},{"cell_type":"code","metadata":{"id":"64BkI631XjG3","colab":{"base_uri":"https://localhost:8080/","height":452},"executionInfo":{"status":"ok","timestamp":1594700979098,"user_tz":-600,"elapsed":1491,"user":{"displayName":"Qianli Xing","photoUrl":"","userId":"10148566665905109861"}},"outputId":"8267d1d6-e57e-4849-9ff8-8aa7711ad4a1"},"source":["# #### 对应于hyper 数据集 都很小;\n","# pathlist =['/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/hyper-question-datasets/hyper-question-datasets/Chinese/',\n","#         '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/hyper-question-datasets/hyper-question-datasets/English/',\n","#          '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/hyper-question-datasets/hyper-question-datasets/IT/',\n","#           '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/hyper-question-datasets/hyper-question-datasets/Medicine/',\n","#            '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/hyper-question-datasets/hyper-question-datasets/Pokemon/',\n","#            '/content/drive/My Drive/Colab Notebooks/Deep worker cluster/datasets/datasets_fusion_model/hyper-question-datasets/hyper-question-datasets/Science/'\n","#            ]\n","# for path in pathlist:\n","#   # datafile = path+'label.csv'\n","#   datafile = path+ 'new_answer.csv'\n","#   # datafile = sys.argv[1]\n","#   e2wl,w2el,label_set = gete2wlandw2el(datafile) # generate structures to pass into EM\n","#   e2wl_name ={}\n","   \n","#   iterations = 20 # EM iteration number\n","#   initquality = 0.7\n","#   e2lpd, w2cm = EM(e2wl,w2el,label_set,initquality).Run(iterations)\n","#   # print(len(e2lpd))\n","#   datasetname = path[139:]\n","#   print(datasetname)\n","#   print(len(w2cm))\n","#   print(len(e2lpd))\n","#   truthfile = path + 'new_truth.csv'\n","#   accuracy = getaccuracy(truthfile, e2lpd, label_set)\n","#   print(accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Chinese/\n","50\n","23\n","0.6086956521739131\n","English/\n","63\n","29\n","0.4827586206896552\n","IT/\n","36\n","24\n","0.7083333333333334\n","Medicine/\n","45\n","35\n","0.6857142857142857\n","Pokemon/\n","55\n","19\n","0.6842105263157895\n","Science/\n","111\n","19\n","0.5263157894736842\n"],"name":"stdout"}]}]}